아래 내용은 “FastAPI 핵심 정리 → 실무 팁 → LangGraph 개요 → FastAPI와 LangGraph 통합 패턴 → 실전 코드 예제 → 확장/운영 가이드” 순서로 정리했습니다. 최대한 실전에서 바로 쓰실 수 있도록 구성했습니다.

---

# FastAPI 한눈에 보기

**철학/특징**

* **ASGI 기반 비동기 프레임워크**(Starlette 위): 고성능 I/O, 웹소켓/HTTP/서버푸시(SSE) 등 현대적 기능 지원.
* **타입힌트 + Pydantic**: 요청/응답 모델 자동 검증, 직렬화/문서화. OpenAPI 스펙/Swagger UI 자동 생성.
* **간결한 DI(Dependency Injection)**: `Depends()`로 인증, DB 세션, 설정 주입을 함수 시그니처만으로 선언.

**핵심 구성요소**

* **라우팅**: `@app.get/post/…`, `APIRouter`로 모듈화, `dependencies`, `prefix`, `tags`.
* **모델**: `pydantic` `BaseModel`로 요청/응답 스키마 정의(필드 검증/기본값/alias/엄격모드 등).
* **DI**: `Depends` + `yield` 의 cleanup 패턴(예: DB 세션 open/close), `scope`(request/app) 구분.
* **수명주기**: `lifespan` 또는 `startup/shutdown` 훅으로 커넥션 풀/캐시 초기화.
* **미들웨어**: CORS, 로깅/추적, 보안 헤더 등.
* **보안**: OAuth2/JWT, 쿠키/헤더 인증, 권한부여 스코프.
* **스트리밍**: `StreamingResponse`로 토큰/이벤트 스트림, **WebSocket**으로 양방향.
* **백그라운드 작업**: `BackgroundTasks`(경량), 또는 Celery/RQ/Arq 등 외부 큐.
* **파일/대용량**: `UploadFile`(스풀링), `FileResponse`.
* **테스트**: `pytest` + `httpx.AsyncClient`(또는 `TestClient`)로 통합 테스트.
* **배포**: `uvicorn`/`hypercorn`(ASGI), 프로덕션은 보통 **리버스 프록시(Nginx/ALB)** 뒤에 멀티 워커.

**비동기/성능 주의점**

* 엔드포인트는 **가능한 `async def`**. 동기 CPU 바운드 코드는 **스레드풀**(`run_in_threadpool`) 또는 **작업 큐**로 분리.
* 외부 호출은 `httpx.AsyncClient` 등 **비동기 클라이언트** 사용(연결 풀/타임아웃 설정).
* 멀티 워커(uvicorn workers)는 CPU 수/부하에 맞게 조정. 각 워커는 별도 프로세스이므로 **전역 mutable 상태 공유 금지**.

---

# LangGraph 개요(그리고 FastAPI와의 관계)

**LangGraph란?**
LLM 워크플로를 \*\*그래프(상태 기계)\*\*로 모델링하는 파이썬 라이브러리입니다.

* **State**: 대화/도메인 상태(예: messages, user\_profile, tool\_results).
* **Nodes**: LLM 호출/툴 실행/분기/후처리 등 **하나의 처리 단위**.
* **Edges/조건 분기**: 상태에 따라 다음 노드를 결정(도구 호출/재프롬프트/에러 핸들링).
* **Checkpointer**: 멀티턴/중단 복구를 위한 상태 저장(메모리/SQLite/Redis/클라우드 등).
* **Streaming Events**: 각 노드/토큰/툴콜 진행 상황을 **이벤트로 스트리밍**.

**FastAPI와의 관계**

* LangGraph는 **두뇌(오케스트레이션/추론/툴콜)**, FastAPI는 **운반수단(HTTP/WebSocket/SSE/권한/버전/스케일링)** 역할입니다.
* 통합 시 FastAPI 엔드포인트는 보통 다음 패턴을 택합니다.

  1. **단건 호출(REST)**: `/chat` POST → 그래프 `invoke/ainvoke` → 최종 답만 반환.
  2. **스트리밍(SSE)**: `/chat/stream` GET → 그래프의 이벤트/토큰을 **실시간** 전송.
  3. **WebSocket**: `/ws` → 사용자 입력/툴콜 결과를 **양방향** 스트림으로 주고받기.
  4. **세션/스레드 관리**: `thread_id`(또는 `session_id`)를 **configurable**로 넘겨 LangGraph 체크포인터에 상태 보존.
  5. **스케일링**: FastAPI는 **stateless**로 확장, LangGraph의 **checkpointer**(DB/Redis 등)로 **상태를 외부 저장**.

---

# 통합 설계 패턴(실무 가이드)

**1) API 계층 설계**

* `POST /chat`: 요청(사용자 입력, 메타데이터, thread\_id) → LangGraph `ainvoke()` → 최종 응답 JSON.
* `GET /chat/stream?thread_id=…`: SSE로 LangGraph **이벤트/토큰 스트림** 전송.
* `WS /ws`: 프론트가 이미지/음성/텍스트를 자유롭게 보낼 수 있고, LangGraph 이벤트를 **양방향**으로 전달.

**2) 상태/세션**

* `thread_id`를 FastAPI에서 파싱 → LangGraph `config={"configurable":{"thread_id": thread_id}}`로 전달.
* Checkpointer는 개발 초기엔 `MemorySaver`, 운영에선 **SQLite/Postgres/Redis** 계열 사용 권장.

**3) 도구 실행/장시간 작업**

* LangGraph 노드에서 **블로킹/장시간** 작업 발생 시:

  * 경량이면 `run_in_threadpool` 사용.
  * 무겁거나 CPU 바운드면 \*\*작업 큐(Celery/RQ)\*\*에 위임 → 작업 ID를 LangGraph 상태에 기록 → SSE/WS로 진행률/완료 알림.

**4) 인증/권한**

* FastAPI OAuth2/JWT로 사용자 인증 후 **user\_id/권한**을 LangGraph 입력/컨텍스트에 주입.
* 요청마다 `user_id`, `locale`, `tenant` 등 **멀티테넌시 메타데이터**를 `configurable`로 넘겨 분기/템플릿에 활용.

**5) 모델 백엔드**

* **vLLM**: OpenAI 호환 모드 사용 시 HTTP로 쉽게 연동(스트리밍 지원).
* **Ollama**: 로컬 모델 HTTP 호출.
* 둘 다 **비동기 클라이언트**로 호출하고, 토큰 스트림을 LangGraph 이벤트로 래핑.

---

# 최소 예제(완전 동작형)

> LangGraph(그래프) + FastAPI(REST/SSE/WS) 통합 예시입니다.
> 의존성(예): `pip install fastapi uvicorn[standard] httpx pydantic langgraph langchain-core`

## 1) LangGraph 그래프 정의 (`graph.py`)

```python
# graph.py
from typing import Annotated, Sequence, TypedDict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# 상태 정의: 메시지 누적
class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "conversation messages"]

# 간단한 모델 호출(여기선 예시로 에코/모의 구현)
# 실제로는 langchain-openai, langchain-ollama, 또는 vLLM 호환 클라이언트 사용
async def call_llm(state: ChatState, config: RunnableConfig) -> ChatState:
    user_text = ""
    for m in state["messages"][::-1]:
        if isinstance(m, HumanMessage):
            user_text = m.content
            break
    # 실제: LLM 호출(비동기). 여기선 단순 에코.
    reply = f"안녕하세요! 당신이 말씀하신 내용: {user_text}"
    return {"messages": [AIMessage(content=reply)]}

def build_graph():
    builder = StateGraph(ChatState)
    builder.add_node("llm", call_llm)
    builder.set_entry_point("llm")
    builder.add_edge("llm", END)

    # 개발 중에는 메모리 체크포인터, 운영은 외부 저장소 권장
    checkpointer = MemorySaver()
    app = builder.compile(checkpointer=checkpointer)
    return app
```

> 실제 LLM 연동 시(예: vLLM/OpenAI 호환):
>
> * LangChain의 `ChatOpenAI`(엔드포인트/키를 vLLM 게이트웨이로) 또는 `ChatOllama`를 노드 내부에서 호출하시거나,
> * LangGraph에서 **외부 LLM 런너**를 의존성 주입으로 받아 호출하는 구조를 권장합니다.

## 2) FastAPI 앱 (`main.py`)

```python
# main.py
from fastapi import FastAPI, Depends, HTTPException, Body, Query, WebSocket
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import AsyncIterator, Dict, Any
import asyncio
import json

from graph import build_graph
from langchain_core.messages import HumanMessage
from langgraph.types import StreamEvent  # 이벤트 타입 힌트 용도

app = FastAPI(title="LangGraph + FastAPI Demo", version="1.0.0")

# 그래프를 애플리케이션 단위로 1회 빌드
graph = build_graph()

# DI: 요청마다 공통 config 생성
def graph_config(thread_id: str = Query(..., description="대화 스레드 ID")):
    # configurable에 thread_id를 넘기면 LangGraph 체크포인터가 세션을 구분/저장
    return {"configurable": {"thread_id": thread_id}}

class ChatRequest(BaseModel):
    user_input: str

@app.post("/chat")
async def chat(
    req: ChatRequest,
    config = Depends(graph_config),
):
    # 단건 호출: 최종 응답만 반환
    try:
        result = await graph.ainvoke(
            {"messages": [HumanMessage(content=req.user_input)]},
            config=config
        )
        # result는 상태 dict(예: {"messages": [AIMessage(...)]})
        ai_msg = next((m for m in result["messages"] if m.type == "ai"), None)
        return {"reply": ai_msg.content if ai_msg else ""}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/chat/stream")
async def chat_stream(
    user_input: str = Query(...),
    config = Depends(graph_config),
):
    # SSE 스트리밍: LangGraph 이벤트를 JSON 라인으로 보냄
    async def event_gen() -> AsyncIterator[bytes]:
        # LangGraph의 이벤트 스트림(a* 버전 API) 사용
        async for ev in graph.astream_events(
            {"messages": [HumanMessage(content=user_input)]},
            config=config,
            version="v1"
        ):
            # ev는 {"event": "...", "data": {...}, "metadata": {...}} 형태
            payload = {
                "event": ev["event"],
                "data": ev.get("data"),
                "metadata": ev.get("metadata"),
            }
            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n".encode("utf-8")
            # SSE는 'data:' 라인과 빈 줄로 이벤트 경계 구분

    return StreamingResponse(event_gen(), media_type="text/event-stream")

@app.websocket("/ws")
async def chat_ws(ws: WebSocket):
    await ws.accept()
    # 간단한 WS 루프: {thread_id, user_input}를 받아 LangGraph 실행 후 이벤트를 WS로 전송
    while True:
        try:
            msg_text = await ws.receive_text()
            req = json.loads(msg_text)
            thread_id = req.get("thread_id")
            user_input = req.get("user_input", "")
            if not thread_id or not user_input:
                await ws.send_text(json.dumps({"error": "thread_id and user_input required"}))
                continue

            config = {"configurable": {"thread_id": thread_id}}

            async for ev in graph.astream_events(
                {"messages": [HumanMessage(content=user_input)]},
                config=config,
                version="v1"
            ):
                await ws.send_text(json.dumps({
                    "event": ev["event"],
                    "data": ev.get("data"),
                    "metadata": ev.get("metadata"),
                }, ensure_ascii=False))
        except Exception as e:
            await ws.send_text(json.dumps({"error": str(e)}))
            await ws.close()
            break
```

**실행**

```bash
uvicorn main:app --reload  # 개발
# 프로덕션 예: uvicorn main:app --workers 4 --timeout-keep-alive 30
```

**SSE 클라이언트(브라우저 JS)**

```html
<script>
  const es = new EventSource("/chat/stream?thread_id=abc&user_input=안녕하세요");
  es.onmessage = (e) => {
    const payload = JSON.parse(e.data);
    console.log(payload.event, payload.data);
  };
  es.onerror = () => es.close();
</script>
```

**WebSocket 클라이언트(브라우저 JS)**

```js
const ws = new WebSocket("ws://localhost:8000/ws");
ws.onopen = () => {
  ws.send(JSON.stringify({ thread_id: "abc", user_input: "카메라를 켜줘" }));
};
ws.onmessage = (e) => {
  const msg = JSON.parse(e.data);
  console.log(msg.event, msg.data);
};
```

---

# 실제 LLM 연동(예시 아이디어)

아래는 **vLLM(OpenAI 호환)** 또는 **Ollama**를 LangGraph 노드에서 호출하는 전형적 패턴입니다. (핵심 아이디어만 요약)

```python
# llm_node.py
from langchain_openai import ChatOpenAI  # vLLM의 OpenAI 호환 게이트웨이에 붙일 수 있음
# from langchain_ollama import ChatOllama  # Ollama 사용 시
from langchain_core.messages import HumanMessage, AIMessage
from typing import TypedDict, Annotated, Sequence
from langgraph.graph import StateGraph, END

class ChatState(TypedDict):
    messages: Annotated[Sequence, "conversation messages"]

# 의존성 주입을 통해 모델 인스턴스를 외부에서 건네받아도 좋습니다.
llm = ChatOpenAI(
    base_url="http://<vllm-host>:<port>/v1",
    api_key="EMPTY",     # vLLM 게이트웨이 설정에 맞춤
    model="your-model",  # vLLM이 서빙중인 모델명
    streaming=True,
    temperature=0.2,
)

async def call_llm(state: ChatState, config):
    # 마지막 사용자 메시지 기준으로 호출
    user_text = ""
    for m in state["messages"][::-1]:
        if m.type == "human":
            user_text = m.content
            break

    # LangChain 표준 호출
    resp = await llm.ainvoke([HumanMessage(content=user_text)])
    return {"messages": [AIMessage(content=resp.content)]}
```

> 토큰 단위 스트리밍이 필요하면 `llm.astream()`을 사용해 **토큰을 LangGraph 이벤트로** 흘려보내는 구현을 하시면 됩니다.

---

# 운영/확장 체크리스트

**스케일링**

* FastAPI: 멀티 워커 + 오토스케일(쿠버네티스/HPA).
* LangGraph: 상태는 **외부 Checkpointer**(예: PostgreSQL/Redis)로 공유.
* 모델 서버: vLLM/Ollama는 **별도 배포**(GPU/CPU) 후 HTTP로 호출(커넥션 풀/재시도/지수 백오프).

**보안/권한**

* JWT/OAuth2로 사용자 인증 → `user_id/tenant_id/roles`를 LangGraph `configurable`에 전달.
* 각 노드/툴에서 권한 체크(예: A/S 요청, 개인정보 조회 등).

**장시간 작업**

* 영상 분석/대용량 RAG 인덱싱 등은 Celery/RQ로 분리. 작업 ID를 상태에 저장하고 SSE/WS로 진행률 알림.

**관찰성**

* 요청/응답/이벤트 로그 구조화(JSON) + 요청 상관관계 ID(correlation id).
* Prometheus/OTel로 지연/에러/모델 토큰 사용량 모니터링.

**테스트**

* 엔드투엔드: `pytest` + `httpx.AsyncClient`.
* 그래프 단위: LangGraph 노드 함수 **단위 테스트**(모의 LLM/툴).

**국가/정책/테넌트 분기**

* `configurable={"locale":"ko-KR","tenant":"acme"}` 식으로 컨텍스트를 주입, 그래프의 조건 분기/프롬프트 템플릿에 반영.

---

# 핵심 요약

* **FastAPI**는 “전달/보안/스케일링”을 책임지는 **API 레이어**,
* **LangGraph**는 “상태 관리/흐름 제어/툴콜/스트리밍 이벤트”를 책임지는 **오케스트레이션 레이어**입니다.
* 두 레이어를 느슨히 결합하면(REST/SSE/WS) **확장성·관찰성·유지보수성**이 좋아집니다.
* 운영에서는 **비동기 호출, 외부 상태 저장, 장시간 작업 분리, 관찰성**이 성패를 가릅니다.

필요하시면, 위 예제를 **vLLM 스트리밍 토큰 연동 버전** 또는 **Ollama 전환 버전**, **RAG(벡터DB) 포함 구조**, **인증(JWT) 포함 템플릿**으로 확장해 드리겠습니다.


---

>>> 비용문제로 on-premise LLM 을 사용하려고 하는데, 이때 구조 설계를 해줘

아래는 “요구/제약 → 전체 아키텍처 → 컴포넌트 역할 → 배포 패턴(소/중/대) → 모델서빙 선택(vLLM/TGI/llama.cpp) → RAG/임베딩 → 비용절감 전략 → 보안/거버넌스 → 관측/운영 → 용량 산정 방법 → 예시 구성/코드” 순서로 정리한 **온프레미스 LLM 아키텍처 설계안**입니다. (FastAPI + LangGraph 중심)

---

# 1) 목표와 제약

* **목표:** 비용 최소화, 데이터 주권(내부망), 지연시간 단축, 유지보수 용이성.
* **제약:** GPU/전력/발열, 인프라 운영 인력, 모델 라이선스/상용화 조건, 내부망 접근 통제.

---

# 2) 상위 아키텍처(개념도)

```
[클라이언트(Web/모바일)]
        │  (HTTPS/WebSocket/SSE)
        ▼
    ┌─────────────┐
    │ API Gateway │  ← 인증/레이트리밋/로깅
    └─────┬───────┘
          ▼
     ┌─────────┐
     │ FastAPI │  ← REST/SSE/WS, 멀티테넌시, 권한
     └────┬────┘
          ▼
   ┌───────────────┐
   │  LangGraph     │ ← 오케스트레이션(상태/분기/툴콜/스트리밍)
   │  + Checkpointer│
   └─────┬─────┬───┘
         │     │
         │     ├────────────┐
         │     │            │
         ▼     ▼            ▼
 [RAG/Tools] [Model Router] [Job Queue]
 (VectorDB,   (작은/중간/큰   (장시간 작업, 배치)
  검색/툴)     모델 선택)
         │        │
         ▼        ▼
   ┌───────────────────────┐
   │   LLM Inference Pool  │ ← vLLM/TGI/llama.cpp(여러 인스턴스)
   └──┬───────────┬────────┘
      │           │
      ▼           ▼
 [GPU 노드 #1] [GPU 노드 #2] ...  ← NVMe 캐시/토크나이저/모델 웨이트
```

---

# 3) 컴포넌트 역할 요약

* **API Gateway(Nginx/Envoy):** TLS 종료, JWT 검증 프리필터, 레이트리밋, 요청/응답 로그.
* **FastAPI:** 세션/테넌트/역할 기반 접근, 요청 유효성 검증, SSE/WS 스트리밍, 업/다운스트림 타임아웃 관리.
* **LangGraph:** 상태 머신(대화/프로세스), 멀티턴 저장(Checkpointer=SQLite/Redis/Postgres), 도구 호출, 오류/재시도/분기, 이벤트 스트림.
* **Model Router:** 작업 난이도/토큰 길이/보안등급에 따라 **소형/중형/대형 모델 라우팅**, 프롬프트 템플릿 스위치.
* **RAG/툴:** Qdrant/Milvus + 임베딩 서버, BM25 하이브리드 검색, 사내 API/DB, 함수도구툴.
* **LLM 서빙 풀:** vLLM(권장)/TGI/llama.cpp 등 **OpenAI 호환 엔드포인트**로 제공, 동적 배칭/KV 캐시.
* **Job Queue:** Celery/RQ/Arq(장시간/CPU 바운드 작업), 진행률 업데이트 → LangGraph 이벤트로 반영.
* **Observability:** Prometheus/Grafana + OpenTelemetry(요청-스팬 연계), 중앙 로그(ELK/Opensearch).

---

# 4) 배포 패턴(규모별)

## A. **소규모(개발/파일럿, 단일 서버)**

* 1대 서버(예: 64\~128GB RAM, NVMe, 단일 GPU(예: 4090/RTX A6000))
* Docker Compose:

  * `fastapi+langgraph` / `vllm(또는 llama.cpp)` / `qdrant` / `nginx`
* 장점: 비용 최소, 간편. 단점: 단일 장애점(SPOF), 동시성 제한.

## B. **중규모(사내 서비스)**

* 2\~4대:

  * **API 노드(2대)**: FastAPI + Gateway(무상태, 오토스케일)
  * **GPU 노드(1\~2대)**: vLLM/TGI(모델 종류별 인스턴스)
  * **데이터 노드(1대)**: Qdrant/Milvus + Postgres/Redis
* L4/L7 로드밸런서, 공용 스토리지(NFS/MinIO)로 모델/캐시 공유.

## C. **대규모(고가용성/K8s)**

* Kubernetes:

  * `Deployment`: FastAPI, Model Router, Embedding 서버, RAG 서비스
  * `StatefulSet`: VectorDB, Postgres/Redis
  * `Node Pool`: GPU 전용(vLLM/TGI), CPU 전용(API/워커)
* KServe/Seldon/Triton 선택 가능(운영 난이도↑, 다모델 대규모 환경에 유리)

---

# 5) 모델 서빙 스택 선택 가이드

| 항목 | **vLLM**(권장)                                      | **TGI**                 | **llama.cpp server**                        |
| -- | ------------------------------------------------- | ----------------------- | ------------------------------------------- |
| 장점 | OpenAI 호환, **PagedAttention/KV캐시/배칭** 강력, 대규모 동시성 | 안정적, HuggingFace 생태계 긴밀 | **CPU/경량 GPU/임베디드** 친화, **GGUF** 지원, 메모리 효율 |
| 단점 | GPU 자원 전제, 일부 모델 최적화 편차                           | GPU전제, 커스터마이징 난이도       | Throughput 낮음(대모델), 기능 제한                   |
| 용도 | 사내 메인 서빙(중\~대규모)                                  | 대안/특정 모델                | 파일럿/엣지/저예산, 보조용                             |

**권장:** 메인은 vLLM, 보조로 llama.cpp(GGUF) 소형 모델(분류/루틴태스크) 병행.

---

# 6) RAG/임베딩

* **Vector DB:** Qdrant(가볍고 온프레미스 적합) 또는 Milvus.
* **임베딩 모델(온프레미스):** `bge-m3`, `gte-large`, `e5` 계열(한글 포함 멀티링구얼 모델 고려).
* **파이프라인:**

  1. 전처리(청크·메타데이터) →
  2. 임베딩(batch 처리) →
  3. 벡터 업서트 →
  4. 쿼리 시 Hybrid(BM25+벡터) →
  5. 컨텍스트 압축(리랭커/Token-budget) →
  6. 프롬프트 합성 →
  7. LLM 호출 →
  8. 출처/근거 첨부.

---

# 7) **비용 절감 전략(핵심)**

1. **모델 라우팅**

   * 규칙/휴리스틱 + 간단 분류기로 **소형→중형→대형** 순 **그라데이션 라우팅**.
   * 예) FAQ/정형질문은 소형(GGUF 7B), 창의적/복잡 질의는 8~~14B, 매우 복잡/요약+RAG는 13~~70B.

2. **양자화/최적화**

   * GPU: AWQ/GPTQ 또는 vLLM의 정수화 지원 범위 확인.
   * CPU/경량: GGUF(Q4\_K\_M/Q5\_K\_M)로 llama.cpp 서버.
   * **KV 캐시** 재사용, **동적 배칭** 활성.

3. **컨텍스트 절약**

   * Prompt template 최소화, **RAG로 필요한 컨텍스트만** 주입(토큰 버짓링).
   * 시스템/도메인 지식은 **함수도구 호출**로 외부화.

4. **캐싱**

   * **결과 캐시(프롬프트 해시→응답)**: Redis/SQLite + TTL.
   * **프롬프트 조각 캐시**: 자주 쓰는 지시문/컨텍스트는 아이디empotent하게 재사용.
   * vLLM **PagedAttention**로 미세토막 요청 큰폭 동시 처리.

5. **배치·비동기화**

   * 임베딩/인덱싱은 배치 Job Queue로 야간 수행.
   * 스트리밍 응답으로 사용자 체감 지연 감소.

6. **지능형 폴백**

   * GPU 과부하시 **소형 모델 임시 폴백** 또는 **큐 대기 + 진행률 안내**.

---

# 8) 보안/거버넌스

* **네트워크 분리/세그먼트**, 방화벽/프록시 고정 경로.
* **암호화:** 전 구간 TLS(내부망도 가능하면 mTLS), 저장시 개인정보 암호화.
* **인증/권한:** OAuth2/JWT, 역할(RBAC), 테넌트 분리.
* **감사 로깅:** 프롬프트/응답/툴콜/결정 분기 로그(개인정보 최소화/마스킹).
* **모델/데이터 라이선스 준수:** 가중치 상업 사용 가능 여부 점검, 서드파티 데이터 사용 범위 명시.

---

# 9) 관측/운영

* **메트릭:** 요청수, 대기/토큰생성 TPS, p50/p95 지연, OOM/에러율, GPU/VRAM/RAM/디스크.
* **트레이싱:** API↔LangGraph 노드↔모델서버 단위 스팬 상관관계.
* **로그:** 구조화(JSON), 프롬프트/컨텍스트 토큰 길이, 라우팅 결정 이유.
* **알림:** 임계치 초과 시 Slack/Teams/Webhook.

---

# 10) 용량 산정 방법(실무 식)

정확한 수치는 환경에 따라 달라 **측정 기반**이 안전합니다.

1. **Workload 정의**

   * 평균/최대 프롬프트 토큰, 출력 토큰, 동시 사용자 수, QPS.
2. **시험 시나리오**

   * 대표 5\~10개 프롬프트 세트로 **부하 테스트**(Locust/k6/wrk).
3. **지표 수집**

   * GPU별 **tokens/sec**, p50/p95, OOM 여부, 배칭 효과.
4. **여유율**

   * 고점 트래픽 대비 30\~50% 여유 + 장애/업데이트 고려한 N+1.
5. **스케일 전략**

   * vLLM 인스턴스 **수평 확장** + 라우팅으로 모델 믹스를 최적화.

---

# 11) 예시 구성: Docker Compose(소규모 파일럿)

```yaml
# docker-compose.yml
version: "3.9"
services:
  gateway:
    image: nginx:stable
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports: ["443:443","80:80"]
    depends_on: [api]

  api:
    build: ./api  # FastAPI + LangGraph
    env_file: .env
    ports: ["8000:8000"]
    depends_on: [qdrant]

  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model /models/YourModel
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.9
      --max-model-len 32768
      --disable-log-requests
    environment:
      - VLLM_WORKER_USE_NVLINK=False
    volumes:
      - /mnt/models/yourmodel:/models/YourModel:ro
    ports: ["8001:8001"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - ./qdrant_storage:/qdrant/storage
    ports: ["6333:6333"]

  # (옵션) 임베딩 서버/리랭커 서버 추가 가능
```

**Nginx 샘플(요지)**

```nginx
# nginx.conf (요지)
server {
  listen 80; return 301 https://$host$request_uri;
}
server {
  listen 443 ssl;
  # ssl_certificate ...; ssl_certificate_key ...;
  location /api/ {
    proxy_pass http://api:8000/;
    proxy_buffering off; # SSE/WS
  }
}
```

**FastAPI ↔ vLLM 연동(요지)**

* OpenAI 호환 엔드포인트(`http://vllm:8001/v1`)를 LangChain `ChatOpenAI`에 연결.

```python
# api/llm_client.py
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://vllm:8001/v1",
    api_key="EMPTY",           # vLLM은 키 미검증 모드 사용 가능
    model="your-model-name",   # vLLM에 로드한 모델 이름
    streaming=True,
    temperature=0.2,
)
```

```python
# api/graph.py (요지)
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage

def build_graph(llm):
    async def call_llm(state, config):
        user = next((m for m in reversed(state["messages"]) if m.type=="human"), None)
        resp = await llm.ainvoke([HumanMessage(content=user.content)])
        return {"messages":[AIMessage(content=resp.content)]}

    g = StateGraph(dict)
    g.add_node("llm", call_llm)
    g.set_entry_point("llm")
    g.add_edge("llm", END)
    return g.compile()
```

```python
# api/main.py (요지)
from fastapi import FastAPI, Depends, Query
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage
from .llm_client import llm
from .graph import build_graph
import json

app = FastAPI(title="On-Prem LLM")
graph = build_graph(llm)

def cfg(thread_id: str = Query(...)):
    return {"configurable":{"thread_id": thread_id}}

@app.get("/api/chat/stream")
async def stream(user_input: str, config = Depends(cfg)):
    async def gen():
        async for ev in graph.astream_events({"messages":[HumanMessage(content=user_input)]},
                                             config=config, version="v1"):
            yield f"data: {json.dumps(ev, ensure_ascii=False)}\n\n"
    return StreamingResponse(gen(), media_type="text/event-stream")
```

---

# 12) 운영 체크리스트(요약)

* **모델/버전 관리:** 태깅(모델명+날짜+양자화), Canary → 점진 전환.
* **도메인별 프롬프트 템플릿:** 템플릿 레포/레지스트리화.
* **데이터 파이프라인:** 로그→(PII 마스킹)→데이터 레이크(MinIO/저장)→재학습/LoRA 파인튠(Axolotl/Unsloth).
* **장애 대응:** GPU OOM/드라이버 이슈 Runbook, 자동 재기동, 폴백 라우팅.

---

## 마무리

* **핵심:** vLLM 기반 GPU 풀 + LangGraph 오케스트레이션 + RAG 하이브리드 + 라우팅/캐시/양자화로 **비용을 크게 절감**하면서, FastAPI로 **보안/스트리밍/멀티테넌시**를 안정적으로 제공하는 구조가 실전에서 가장 효율적입니다.
* 원하시면 **귀사 환경 스펙**(보유 GPU/서버/동시성 목표/질의 유형)에 맞춰 **구체 용량 계획, 모델 후보, Compose/K8s 매니페스트**까지 커스터마이즈해서 드리겠습니다.

---

>>> LangGraph: 상태는 외부 Checkpointer(예: PostgreSQL/Redis)로 공유.
>>> 이 부분에 대해서 자세히 설명해봐

다음은 **LangGraph의 외부 Checkpointer(PostgreSQL/Redis 등)로 상태를 공유**하는 방법을 개념→데이터모델→동작원리→구현패턴→운영팁 순서로 정리한 상세 가이드입니다. (FastAPI + LangGraph 기준)

---

# 핵심 개념 정리

* **Checkpointer**는 LangGraph 실행 중간중간(노드 완료 시점 등)에 **상태(state) 스냅샷**을 저장/복구하는 플러그인입니다.
* **thread\_id**(세션 ID)로 대화를 구분하고, 필요하면 **checkpoint\_id**(버전/브랜치)을 붙여 **타임트래블/포크**가 가능합니다.
* 서버가 수평 확장되어도(여러 FastAPI 인스턴스) \*\*외부 저장소(PostgreSQL/Redis)\*\*에 상태를 공유하면 어디서든 **이어달리기**가 됩니다.

---

# 어떤 상태가 저장되나요?

* 보통 `TypedDict` 혹은 `dict` 형태의 **그래프 상태**(예: `messages`, `tool_results`, `user_profile` 등).
* **메타데이터**(입력/템플릿 버전, 분기 결정 이유), **부모 체크포인트**(브랜치 트리), **생성 시각**.
* 이벤트 단위(토큰/노드)까지 저장하고 싶다면 **이벤트 로그 테이블/스트림**을 별도로 둡니다.

---

# 데이터 모델(권장 스키마)

## PostgreSQL (JSONB 기반, 버저닝 가능)

```sql
-- 세션별 최신 스냅샷(조회 빈도 ↑)
CREATE TABLE lg_checkpoint_latest (
  thread_id TEXT PRIMARY KEY,
  checkpoint_id UUID NOT NULL,
  state JSONB NOT NULL,
  metadata JSONB,
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- 전체 이력(브랜치/타임트래블용)
CREATE TABLE lg_checkpoint_history (
  checkpoint_id UUID PRIMARY KEY,
  thread_id TEXT NOT NULL,
  parent_checkpoint_id UUID,
  state JSONB NOT NULL,
  metadata JSONB,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX ON lg_checkpoint_history(thread_id, created_at DESC);
```

## Redis (고속 캐시/세션 H/A)

* 키 설계(예시):

  * `lg:{tenant}:{thread_id}:latest` → `{"checkpoint_id": "...","state": {...},"metadata": {...}}`
  * `lg:{tenant}:{thread_id}:history` → **Sorted Set**(score=timestamp, member=checkpoint\_id)
  * `lg:checkpoint:{checkpoint_id}` → `{"thread_id":"...","parent":"...","state":{...}}`
* **원자성**: `WATCH/MULTI/EXEC` 또는 Lua 스크립트로 최신 포인터 갱신의 경쟁 상태 방지.

---

# 동작 원리(실행 시점)

1. **그래프 시작**: `config={"configurable":{"thread_id": "u123"}}`로 실행 → Checkpointer가 `thread_id`의 **마지막 스냅샷**을 읽어 초기 상태로 설정.
2. **노드 완료**: 노드 실행이 끝날 때마다(또는 프레임워크가 정한 타이밍에) **새 checkpoint**를 생성하여 DB/Redis에 기록.
3. **재시도/장애복구**: 동일 `thread_id`로 재실행하면 마지막 스냅샷부터 **이어감**.
4. **브랜치/포크**: 특정 시점의 `checkpoint_id`를 지정하면 **그 지점에서 분기 실행** 가능.

> 실사용 팁: 운영에서는 \*\*최신 포인터(빠른 조회)\*\*와 \*\*상세 이력(분기/감사)\*\*를 **분리 저장**하면, 성능과 기능을 모두 확보할 수 있습니다.

---

# 구현 패턴

## 1) 가장 쉬운 시작: 내장 Saver로 개념 검증

LangGraph에는 메모리/SQLite Saver가 포함되어 있습니다. 운영 전 **패턴 검증**에 유용합니다.

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
# 또는 from langgraph.checkpoint.sqlite import SqliteSaver

def build_graph():
    g = StateGraph(dict)
    # ... 노드/엣지 정의 ...
    saver = MemorySaver()  # 개발단계
    # saver = SqliteSaver("file:state.db")  # 파일 하나로 공유(단일 노드/소규모)
    return g.compile(checkpointer=saver)
```

## 2) PostgreSQL Checkpointer(권장: 스냅샷+이력 분리)

버전마다 인터페이스가 조금씩 다를 수 있어, **아답터 패턴**으로 구현하는 것을 권장합니다. 아래는 **레퍼런스 형태**(핵심 로직만)입니다.

```python
# postgres_checkpointer.py (요지)
import uuid, json, asyncpg
from typing import Optional, Dict, Any

class PostgresCheckpointer:
    def __init__(self, dsn: str):
        self._dsn = dsn

    async def _conn(self):
        return await asyncpg.connect(self._dsn)

    async def load_latest(self, thread_id: str) -> Optional[Dict[str, Any]]:
        async with await self._conn() as cx:
            row = await cx.fetchrow(
                "SELECT checkpoint_id, state, metadata FROM lg_checkpoint_latest WHERE thread_id=$1",
                thread_id
            )
            if not row: return None
            return dict(row)

    async def save(self, thread_id: str, state: Dict[str, Any], metadata: Optional[Dict[str, Any]]=None,
                   parent_checkpoint_id: Optional[str]=None) -> str:
        checkpoint_id = str(uuid.uuid4())
        async with await self._conn() as cx:
            async with cx.transaction():
                await cx.execute(
                    "INSERT INTO lg_checkpoint_history(checkpoint_id, thread_id, parent_checkpoint_id, state, metadata) "
                    "VALUES ($1,$2,$3,$4::jsonb,$5::jsonb)",
                    checkpoint_id, thread_id, parent_checkpoint_id, json.dumps(state), json.dumps(metadata or {})
                )
                await cx.execute(
                    "INSERT INTO lg_checkpoint_latest(thread_id, checkpoint_id, state, metadata, updated_at) "
                    "VALUES ($1,$2,$3::jsonb,$4::jsonb, now()) "
                    "ON CONFLICT(thread_id) DO UPDATE SET "
                    "checkpoint_id=EXCLUDED.checkpoint_id, state=EXCLUDED.state, metadata=EXCLUDED.metadata, updated_at=now()",
                    thread_id, checkpoint_id, json.dumps(state), json.dumps(metadata or {})
                )
        return checkpoint_id
```

LangGraph에 연결하는 방법(버전에 따라 메서드명이 다를 수 있으므로 **어댑터 함수**를 둡니다):

```python
# graph.py (요지)
from langgraph.graph import StateGraph, END

class LGCheckpointerAdapter:
    """LangGraph가 기대하는 인터페이스에 맞춰 PostgresCheckpointer를 감싸는 어댑터."""
    def __init__(self, pg):
        self.pg = pg

    # 아래 메서드명/시그니처는 사용하는 LangGraph 버전에 맞춰 연결합니다.
    async def get(self, config):
        thread_id = config["configurable"]["thread_id"]
        return await self.pg.load_latest(thread_id)

    async def put(self, config, state, metadata=None, parent=None):
        thread_id = config["configurable"]["thread_id"]
        return await self.pg.save(thread_id, state, metadata, parent)

def build_graph(pg_checkpointer):
    g = StateGraph(dict)
    # ... 노드들 추가 ...
    # 노드 구현 예: 노드 종료 시 반환 dict가 "증분"이든 "전체"든, 최종 병합 규칙은 StateGraph가 관리
    return g.compile(checkpointer=LGCheckpointerAdapter(pg_checkpointer))
```

> **주의:** 실제 LangGraph 릴리스에 따라 `checkpointer`가 기대하는 메서드/리턴 형태가 조금씩 바뀔 수 있습니다. 위처럼 **어댑터 레이어**를 만들어 연결하시면 업그레이드 대응이 쉽습니다.

## 3) Redis Checkpointer(초저지연/세션 캐시)

* **사용처**: 초저지연, 세션 캐시, 임시 상태(장기 이력은 PostgreSQL에 병행 저장 권장).
* 패턴: `SETNX`/Lua로 **최신 포인터 원자 갱신**, `ZADD`로 이력 보관(+TTL).

```python
# redis_checkpointer.py (요지)
import json, uuid, aioredis
from typing import Optional, Dict, Any

class RedisCheckpointer:
    def __init__(self, url: str, tenant: str="default"):
        self.r = aioredis.from_url(url, decode_responses=True)
        self.tenant = tenant

    def _k_latest(self, thread_id): return f"lg:{self.tenant}:{thread_id}:latest"
    def _k_hist(self, thread_id):   return f"lg:{self.tenant}:{thread_id}:history"
    def _k_ckpt(self, ckpt_id):     return f"lg:checkpoint:{ckpt_id}"

    async def load_latest(self, thread_id: str) -> Optional[Dict[str, Any]]:
        s = await self.r.get(self._k_latest(thread_id))
        return json.loads(s) if s else None

    async def save(self, thread_id: str, state: Dict[str, Any], metadata: Dict[str, Any]|None=None, parent: str|None=None):
        ckpt_id = str(uuid.uuid4())
        payload = {"checkpoint_id": ckpt_id, "thread_id": thread_id, "parent": parent, "state": state, "metadata": metadata or {}}
        p = self.r.pipeline(transaction=True)
        p.set(self._k_ckpt(ckpt_id), json.dumps(payload))
        p.set(self._k_latest(thread_id), json.dumps({"checkpoint_id": ckpt_id, "state": state, "metadata": metadata or {}}))
        p.zadd(self._k_hist(thread_id), {ckpt_id: __import__("time").time()})
        await p.execute()
        return ckpt_id
```

---

# FastAPI에서의 사용(요지)

```python
# main.py (핵심만)
from fastapi import FastAPI, Depends, Query
from langchain_core.messages import HumanMessage
from graph import build_graph
from postgres_checkpointer import PostgresCheckpointer

app = FastAPI()
pg = PostgresCheckpointer(dsn="postgresql://user:pass@host/db")
graph = build_graph(pg)

def cfg(thread_id: str = Query(...)):
    return {"configurable": {"thread_id": thread_id}}

@app.post("/chat")
async def chat(user_input: str, config = Depends(cfg)):
    result = await graph.ainvoke({"messages": [HumanMessage(content=user_input)]}, config=config)
    # result 안에는 병합된 최신 state가 들어있고,
    # checkpointer가 내부적으로 새 checkpoint를 외부 DB에 반영합니다.
    return {"reply": next((m.content for m in result["messages"] if m.type=="ai"), "")}
```

---

# 브랜치/타임트래블 패턴

* **사용 예시**: “어제 3번째 턴으로 되돌아가 같은 질문을 다른 톤으로 재실행”.
* 절차:

  1. UI에서 특정 `checkpoint_id` 선택
  2. 실행 시 `config={"configurable":{"thread_id":T, "checkpoint_id":C}}`
  3. Checkpointer가 `C`를 기준으로 초기 상태를 복원
  4. 새 분기(새 checkpoint 체인)가 생성되며 히스토리 테이블에 **부모-자식** 관계가 기록

> **권장**: `metadata`에 브랜치 이름/설명을 넣어 UI에서 히스토리를 **트리 구조**로 시각화.

---

# 성능/운영 팁

* **PostgreSQL**

  * JSONB 필드에 필요한 **GIN 인덱스**(예: `state->'messages'` 크기 필터) 선택적으로 추가.
  * 이력 테이블은 **파티셔닝**(월 단위) + **보존기간(보관/압축/삭제)** 적용.
  * 트랜잭션 묶음으로 **최신 포인터 + 이력**을 함께 기록(일관성).

* **Redis**

  * TTL을 latest 키에 주지 말고(세션 유실 위험), **히스토리만 TTL** 부여하거나 **주기적 압축** 권장.
  * 고가용성을 원하면 Redis Sentinel/Cluster 구성.

* **혼합 전략**

  * **쓰기**: PostgreSQL(영구) + Redis(캐시) **동시 기록**.
  * **읽기**: 최신은 Redis → 미스 시 Postgres → Redis 채우기.
  * 대용량/고QPS일수록 **히스토리 분리**(최근 N개는 Redis, 전체는 Postgres).

* **보안**

  * PII는 상태에 **직접 저장 금지**(토큰/키/개인정보는 외부 보안 저장소에서 필요 시 조회).
  * 저장 시 **마스킹/해시** 적용, 전송/저장 모두 **암호화**(TLS/mTLS, at-rest 암호화).

* **관측성**

  * 각 checkpoint에 **reason/meta**(예: 라우팅 기준, 사용 모델, 토큰 수) 저장 → 디버깅/비용분석에 매우 유용.
  * p95/p99 지연, OOM, 롤백/포크 빈도 모니터링.

---

# 마이그레이션 경로(현실적인 단계)

1. **MemorySaver → SqliteSaver**로 로컬 단일 인스턴스 검증
2. **PostgreSQL 도입**(위 스키마 + 아답터)로 멀티 인스턴스 공유
3. **Redis 추가**(핫 세션 캐시/저지연)
4. **브랜치 UI**(checkpoint 트리 시각화) + 보존정책 + 백업 자동화

---

필요하시면, **현재 사용 중인 LangGraph 버전**에 맞춰 위 **어댑터 메서드 시그니처**를 정확히 맞춘 코드 템플릿(테스트 포함)과, \*\*PostgreSQL/Redis 성능 튜닝 값(풀/배치/압축/TTL 정책)\*\*까지 커스터마이즈하여 드리겠습니다.

---

좋습니다. **PostgreSQL/외부 Checkpointer로 LangGraph 상태 공유**를 바로 쓰실 수 있게, FastAPI 통합 템플릿을 만들어 드렸습니다. 아래 압축파일을 받으셔서 바로 실행·확장하시면 됩니다.

[프로젝트 내려받기 (ZIP)](sandbox:/mnt/data/langgraph_checkpointer_template.zip)

---

# 무엇을 드렸나요

* **LangGraph + AsyncPostgresSaver**로 상태 영속화

  * `compile(checkpointer=saver)`로 그래프 컴파일
  * `thread_id`(필수) + `checkpoint_id`(선택) 지원
  * **최초 1회 `.setup()` 자동 실행**(테이블 생성) ([PyPI][1])
* **FastAPI** 엔드포인트

  * `POST /chat?thread_id=...` : 대화 1턴 실행(체크포인트 저장)
  * `GET /state?thread_id=...` : 최신 상태 조회
  * `GET /history?thread_id=...` : 체크포인트 히스토리 보기
* **보안 옵션**: AES 키를 넣으면 **암호화 Serializer**로 저장(선택) ([LangChain AI][2])
* **Docker Compose**: PostgreSQL 포함
* **README**: 빠른 실행 가이드 포함

> 참고: LangGraph 체크포인터는 공통적으로 **`.put / .put_writes / .get_tuple / .list`(동기/비동기 버전 포함)** 메서드를 구현·사용합니다. 그래프는 이를 통해 상태 저장/복구/히스토리를 처리합니다. ([LangChain AI][2])

---

# 빠른 실행

1. 압축을 풀고, `docker compose up -d postgres`
2. `python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`
3. `uvicorn app.main:app --reload`
4. 호출 예시

   * `POST /chat?thread_id=demo` , JSON: `{"text":"안녕하세요"}`
   * `GET /state?thread_id=demo`
   * `GET /history?thread_id=demo`

---

# 핵심 코드(템플릿에 포함)

## 체크포인터 생성 및 테이블 준비

```python
# app/checkpointer.py
from contextlib import asynccontextmanager
from typing import Optional, AsyncIterator
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

@asynccontextmanager
async def create_pg_checkpointer(db_uri: str, aes_key: Optional[str] = None) -> AsyncIterator[AsyncPostgresSaver]:
    serde = EncryptedSerializer.from_pycryptodome_aes(key=aes_key) if aes_key else JsonPlusSerializer()
    async with AsyncPostgresSaver.from_conn_string(db_uri, serde=serde) as saver:
        await saver.setup()   # 최초 1회 테이블 생성
        yield saver
```

* \*\*`.setup()`\*\*은 최초 사용 시 필수(체크포인트 테이블 생성). 수동 연결을 쓸 때는 `autocommit=True`, `row_factory=dict_row`를 쓰라는 공식 주의사항도 포함돼 있습니다. ([PyPI][1])

## 그래프 컴파일(외부 Checkpointer 주입)

```python
# app/main.py (발췌)
from app.checkpointer import create_pg_checkpointer
from app.graph import build_workflow

@app.on_event("startup")
async def on_startup():
    saver = await _stack.enter_async_context(create_pg_checkpointer(DB_URI, LANGGRAPH_AES_KEY or None))
    # 외부 Checkpointer를 연결해 컴파일
    global graph
    graph = build_workflow().compile(checkpointer=saver)
```

## 상태/히스토리 접근

```python
# 최신 상태
cfg = {"configurable": {"thread_id": thread_id}}
snap = graph.get_state(cfg)              # latest
hist = list(graph.get_state_history(cfg))# full history(신규→과거 순)
```

* 히스토리, 리플레이, 포크(특정 `checkpoint_id`에서 분기) 등도 표준 API로 지원됩니다. ([LangChain AI][2])

---

# 운영 팁(요약)

* **버전/가용성**: `langgraph-checkpoint-postgres`는 Psycopg v3 기반. Python ≥3.9, `.setup()` 필수. ([PyPI][1])
* **암호화**: `LANGGRAPH_AES_KEY` 설정 시 EncryptedSerializer 사용(플랫폼/로컬 모두 지원). ([LangChain AI][2])
* **비동기**: FastAPI와 함께 \*\*`AsyncPostgresSaver`\*\*를 권장(동시성에서 안전). 메서드의 비동기 버전(`.aput/.aget_tuple/.alist` 등)을 사용합니다. ([LangChain AI][3])

---

# 다음 단계(원하시면 추가로 드립니다)

* **Redis 읽기 캐시**(선택): `get_state` 결과 TTL 캐싱으로 히스토리 조회 부하 경감
* **K8s 배포 매니페스트**(ConfigMap/Secret로 DB URI, AES Key 관리)
* **브랜치/타임트래블 UI**: 체크포인트 트리 시각화
* **관측/거버넌스**: 요청별 `thread_id / checkpoint_id` 로깅 + 감사 추적

필요하시면 **Redis 캐시 어댑터**(PostgresSaver 위임 + `get_tuple/list` 캐시)와 **K8s 매니페스트**까지 바로 만들어 드리겠습니다.

[1]: https://pypi.org/project/langgraph-checkpoint-postgres/ "langgraph-checkpoint-postgres · PyPI"
[2]: https://langchain-ai.github.io/langgraph/concepts/persistence/ "Overview"
[3]: https://langchain-ai.github.io/langgraph/reference/checkpoints/ "Checkpointing"
