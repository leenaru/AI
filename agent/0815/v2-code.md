# Onboarding Agent â€” Monorepo Code v0.1 (OSI-only)

> ì „ì²´ ë™ì‘ íë¦„ì„ ê°–ì¶˜ **ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì†Œ ìŠ¤ì¼ˆë ˆí†¤**ì…ë‹ˆë‹¤. FastAPI + LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°(ë…¸ë“œ ìƒ˜í”Œ), GraphRAG(Parquet) + **FAISS** í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê³¼ **ê°€ì¤‘ì¹˜ rerank**, Model Adapter Layer(vLLM/Ollama), Slack ê¸°ë°˜ HITL ìŠ¹ì¸ ì›Œí¬í”Œë¡œ, Valkey(Postgres) ê¸°ë°˜ ìƒíƒœ ì €ì¥, Helm/docker-compose ë°°í¬ ìŠ¤ì¼ˆë ˆí†¤ì„ í¬í•¨í•©ë‹ˆë‹¤. **ëª¨ë“  ë¼ì´ì„ ìŠ¤ëŠ” OSI ìŠ¹ì¸**ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

## ğŸ“ Repo êµ¬ì¡°

```
onboarding-agent/
â”œâ”€ LICENSE
â”œâ”€ README.md
â”œâ”€ docker-compose.yml
â”œâ”€ configs/
â”‚  â”œâ”€ app.yaml
â”‚  â””â”€ logging.yaml
â”œâ”€ server/
â”‚  â”œâ”€ app.py
â”‚  â”œâ”€ core/
â”‚  â”‚  â”œâ”€ config.py
â”‚  â”‚  â”œâ”€ logging.py
â”‚  â”‚  â””â”€ security.py
â”‚  â”œâ”€ api/
â”‚  â”‚  â”œâ”€ routes.py
â”‚  â”‚  â””â”€ deps.py
â”‚  â”œâ”€ orchestrator/
â”‚  â”‚  â”œâ”€ graph.py
â”‚  â”‚  â”œâ”€ nodes/
â”‚  â”‚  â”‚  â”œâ”€ guard.py
â”‚  â”‚  â”‚  â”œâ”€ intent.py
â”‚  â”‚  â”‚  â”œâ”€ plan.py
â”‚  â”‚  â”‚  â”œâ”€ tools.py
â”‚  â”‚  â”‚  â”œâ”€ synthesize.py
â”‚  â”‚  â”‚  â”œâ”€ translate.py
â”‚  â”‚  â”‚  â””â”€ respond.py
â”‚  â”‚  â””â”€ checkpointer/
â”‚  â”‚     â””â”€ valkey_store.py
â”‚  â”œâ”€ model_adapter/
â”‚  â”‚  â”œâ”€ base.py
â”‚  â”‚  â”œâ”€ router.py
â”‚  â”‚  â””â”€ providers/
â”‚  â”‚     â”œâ”€ ollama.py
â”‚  â”‚     â””â”€ vllm.py
â”‚  â”œâ”€ rag/
â”‚  â”‚  â”œâ”€ pipeline.py
â”‚  â”‚  â”œâ”€ search.py
â”‚  â”‚  â”œâ”€ bm25.py
â”‚  â”‚  â”œâ”€ schemas.py
â”‚  â”‚  â””â”€ utils.py
â”‚  â”œâ”€ slack/
â”‚  â”‚  â””â”€ routes.py
â”‚  â”œâ”€ vision/
â”‚  â”‚  â”œâ”€ service.py
â”‚  â”‚  â””â”€ storage.py
â”‚  â””â”€ ops/
â”‚     â”œâ”€ metrics.py
â”‚     â””â”€ health.py
â”œâ”€ scripts/
â”‚  â”œâ”€ build_faiss.py
â”‚  â””â”€ reindex.py
â”œâ”€ helm/
â”‚  â”œâ”€ Chart.yaml
â”‚  â”œâ”€ values.yaml
â”‚  â””â”€ templates/
â”‚     â”œâ”€ deployment.yaml
â”‚     â”œâ”€ service.yaml
â”‚     â”œâ”€ ingress.yaml
â”‚     â””â”€ configmap.yaml
â””â”€ mobile/
   â”œâ”€ android/OnboardingAgentGating.kt
   â””â”€ ios/OnboardingAgentGating.swift
```

---

## ğŸ“„ LICENSE (Apache-2.0)

```text
Apache License
Version 2.0, January 2004
http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
... (í‘œì¤€ ì „ë¬¸) ...
```

---

## ğŸ§­ README.md

````md
# Onboarding Agent (OSI-only)

- Server: FastAPI + LangGraph(ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜), GraphRAG(Parquet) + FAISS, Model Adapter(vLLM/Ollama)
- Stores: Valkey(ì²´í¬í¬ì¸í„°/í/ë ˆì´íŠ¸ë¦¬ë°‹), Postgres(ì˜ì†/ì¡)
- HITL: Slack ìŠ¹ì¸ ì›Œí¬í”Œë¡œ
- ë°°í¬: docker-compose, Helm(K8s)

## Quickstart
```bash
# 1) ë¡œì»¬ ìŠ¤íƒ
cp configs/app.yaml.example configs/app.yaml
docker compose up -d --build

# 2) ì¸ë±ìŠ¤ ë¹Œë“œ (docs/ ìƒ˜í”Œ ë„£ê³  ì‹¤í–‰)
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt  # ì˜ˆ: fastapi uvicorn[standard] pydantic ... faiss-cpu
python scripts/build_faiss.py --config configs/app.yaml

# 3) ì„œë²„ ì‹¤í–‰
uvicorn server.app:app --reload
````

````

---

## âš™ï¸ configs/app.yaml (ìš”ì•½)
```yaml
server:
  host: 0.0.0.0
  port: 8000
  log_config: configs/logging.yaml

stores:
  valkey_url: "redis://valkey:6379/0"   # Valkey í˜¸í™˜
  postgres_dsn: "postgresql://postgres:postgres@postgres:5432/agent"
  s3:
    endpoint_url: "http://minio:9000"   # on-prem S3 í˜¸í™˜
    bucket: "ai-vision-temp"
    access_key: "minio"
    secret_key: "minio123"
    secure: false

models:
  router: configs/model-router.yaml

rag:
  graph_store:
    type: parquet
    path: data/parquet/
  index:
    type: faiss
    factory: hnsw
    params:
      dim: 768
      hnsw: { M: 32, efConstruction: 200 }
  ingest:
    unit_summary: true
    incremental: true
  retrieval:
    k_dense: 40
    k_sparse: 40
    k_graph: 2
  merge:
    formula: "alpha*dense + beta*bm25 + gamma*graph + delta*recency"
    weights: { alpha: 0.55, beta: 0.25, gamma: 0.15, delta: 0.05 }
    cross_encoder: { enabled: true, top_m: 10, model: bge-reranker-v2-m3 }

local_gate:
  thresholds: { low: 0.45, high: 0.75 }
  require_tools: { troubleshooting: true, device_registration: true, smalltalk: false }
  redact: { pii: ["phone", "address", "email"] }
  clip: { max_chars: 800, keep_slots: true }
````

---

## ğŸ³ docker-compose.yml

```yaml
version: "3.9"
services:
  api:
    build: .
    command: uvicorn server.app:app --host 0.0.0.0 --port 8000
    ports: ["8000:8000"]
    environment:
      - APP_CONFIG=configs/app.yaml
    depends_on: [postgres, valkey, minio]

  postgres:
    image: postgres:16
    environment:
      - POSTGRES_PASSWORD=postgres
    ports: ["5432:5432"]
    volumes: [pgdata:/var/lib/postgresql/data]

  valkey:
    image: valkey/valkey:7
    ports: ["6379:6379"]

  minio:
    image: minio/minio:RELEASE.2025-01-05T00-00-00Z
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
    ports: ["9000:9000", "9001:9001"]
    volumes: [miniodata:/data]

  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]

  vllm:
    image: vllm/vllm-openai:latest
    ports: ["8001:8000"]

volumes:
  pgdata:
  miniodata:
```

---

## ğŸ server/app.py

```python
from fastapi import FastAPI
from server.core.config import settings
from server.api.routes import router as api_router
from server.ops.health import router as health_router
from server.slack.routes import router as slack_router

app = FastAPI(title="Onboarding Agent API")
app.include_router(health_router, prefix="/")
app.include_router(api_router, prefix="/")
app.include_router(slack_router, prefix="/")
```

---

## ğŸ server/core/config.py

```python
from pydantic_settings import BaseSettings
from pydantic import Field
import yaml, os

class Settings(BaseSettings):
    app_config: str = Field(default=os.getenv("APP_CONFIG", "configs/app.yaml"))
    cfg: dict = {}

    def load(self):
        with open(self.app_config, "r", encoding="utf-8") as f:
            self.cfg = yaml.safe_load(f)
        return self

settings = Settings().load()
```

---

## ğŸ server/api/routes.py

```python
from fastapi import APIRouter, UploadFile, File, Body
from typing import Any, Dict
from server.orchestrator.graph import run_conversation
from server.vision.service import analyze_image

router = APIRouter()

@router.post("/chat")
async def chat_endpoint(payload: Dict[str, Any]):
    return await run_conversation(payload)

@router.post("/vision/analyze")
async def vision_endpoint(file: UploadFile = File(...), tasks: str = Body("ocr,detect")):
    image_bytes = await file.read()
    return analyze_image(image_bytes, tasks.split(","))

@router.post("/actions/result")
async def actions_result(payload: Dict[str, Any]):
    # í´ë¼ì´ì–¸íŠ¸ ì•¡ì…˜ ê²°ê³¼ ìˆ˜ì‹  (ì˜ˆ: ì¹´ë©”ë¼ ì´¬ì˜ ê²°ê³¼)
    return {"status": "ok"}
```

---

## ğŸ server/orchestrator/graph.py (LangGraph ìŠ¤ì¼ˆë ˆí†¤)

```python
from typing import Dict, Any
from server.orchestrator.nodes.intent import detect_intent
from server.orchestrator.nodes.plan import plan_steps
from server.orchestrator.nodes.tools import execute_tools
from server.orchestrator.nodes.synthesize import synthesize
from server.orchestrator.nodes.translate import translate
from server.orchestrator.nodes.respond import respond
from server.ops.metrics import trace

async def run_conversation(payload: Dict[str, Any]):
    state: Dict[str, Any] = {"input": payload, "kb_hits": []}
    with trace("conversation"):
        intent = await detect_intent(state)
        state["intent"] = intent
        plan = await plan_steps(state)
        state["plan"] = plan
        exec_out = await execute_tools(state)
        state.update(exec_out)
        draft = await synthesize(state)
        state["draft"] = draft
        final = await translate(state)
        state["final"] = final
        return await respond(state)
```

---

## ğŸ server/orchestrator/nodes/intent.py

```python
from typing import Dict, Any

async def detect_intent(state: Dict[str, Any]):
    text = state["input"].get("text", "")
    # ê°„ë‹¨ ê²Œì´íŠ¸: í‚¤ì›Œë“œ + ì ìˆ˜(ë°ëª¨)
    score = 0.8 if any(k in text.lower() for k in ["ë“±ë¡", "ì¶”ê°€", "ì‚¬ì§„", "ì—ëŸ¬", "ì˜¤ë¥˜"]) else 0.5
    label = "troubleshooting" if "ì˜¤ë¥˜" in text or "ì—ëŸ¬" in text else "faq"
    slots = {}
    return {"label": label, "score": score, "slots": slots}
```

---

## ğŸ server/orchestrator/nodes/plan.py

```python
from typing import Dict, Any, List

async def plan_steps(state: Dict[str, Any]) -> List[Dict[str, Any]]:
    intent = state.get("intent", {})
    label = intent.get("label")
    plan = []
    if label == "troubleshooting":
        plan.append({"tool": "rag_search", "args": {"q": state["input"].get("text", "")}})
        if any(k in state["input"].get("text", "") for k in ["ì‚¬ì§„", "ì´ë¯¸ì§€", "ì²¨ë¶€"]):
            plan.append({"tool": "request_image"})
    else:
        plan.append({"tool": "rag_search", "args": {"q": state["input"].get("text", "")}})
    return plan
```

---

## ğŸ server/orchestrator/nodes/tools.py

```python
from typing import Dict, Any
from server.rag.search import hybrid_search

async def execute_tools(state: Dict[str, Any]) -> Dict[str, Any]:
    plan = state.get("plan", [])
    kb_hits = []
    actions = []
    for step in plan:
        if step["tool"] == "rag_search":
            res = hybrid_search(step["args"]["q"])  # passages, scores, citations
            kb_hits.extend(res.get("passages", []))
        elif step["tool"] == "request_image":
            actions.append({"type": "open_camera"})
    return {"kb_hits": kb_hits, "actions": actions}
```

---

## ğŸ server/orchestrator/nodes/synthesize.py

```python
from typing import Dict, Any
from server.model_adapter.router import ModelRouter

router = ModelRouter.load_from_yaml("configs/model-router.yaml")

async def synthesize(state: Dict[str, Any]) -> str:
    prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {state['input'].get('text','')}.\nê·¼ê±°:{[p['text'] for p in state.get('kb_hits',[])][:3]}\nì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ë¼."
    return router.generate(prompt)
```

---

## ğŸ server/orchestrator/nodes/translate.py

```python
from typing import Dict, Any

async def translate(state: Dict[str, Any]) -> str:
    # ë°ëª¨: ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë‹¤êµ­ì–´ ë§¤ë‹ˆì € ì—°ë™ ì§€ì )
    return state.get("draft", "")
```

---

## ğŸ server/orchestrator/nodes/respond.py

```python
from typing import Dict, Any

async def respond(state: Dict[str, Any]):
    return {
        "messages": [state.get("final", "")],
        "actions": state.get("actions", []),
        "citations": [{"id": h.get("id"), "meta": h.get("meta")} for h in state.get("kb_hits", [])]
    }
```

---

## ğŸ server/orchestrator/checkpointer/valkey\_store.py

```python
import redis
from server.core.config import settings

_pool = redis.ConnectionPool.from_url(settings.cfg["stores"]["valkey_url"])  # Valkey í˜¸í™˜
r = redis.Redis(connection_pool=_pool)

def set_step(key: str, value: str, ttl_sec: int = 3600):
    r.set(key, value, ex=ttl_sec)

def get_step(key: str):
    v = r.get(key)
    return v.decode() if v else None
```

---

## ğŸ server/model\_adapter/base.py

```python
from abc import ABC, abstractmethod
from typing import Dict

class Provider(ABC):
    @abstractmethod
    def health(self) -> bool: ...
    @abstractmethod
    def generate(self, prompt: str, params: Dict = {}) -> str: ...
```

---

## ğŸ server/model\_adapter/providers/ollama.py

```python
import httpx
from server.model_adapter.base import Provider

class OllamaProvider(Provider):
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3"):
        self.base_url, self.model = base_url, model

    def health(self) -> bool:
        try:
            r = httpx.get(f"{self.base_url}/api/tags", timeout=2)
            return r.status_code == 200
        except Exception:
            return False

    def generate(self, prompt: str, params: dict = {}) -> str:
        payload = {"model": self.model, "prompt": prompt, **params}
        r = httpx.post(f"{self.base_url}/api/generate", json=payload, timeout=60)
        r.raise_for_status()
        # streaming ì´ë²¤íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ë§Œ í•©ì‚°
        text = "".join([line.get("response", "") for line in r.json().get("events", [])]) if r.headers.get("content-type"," ").startswith("application/json") else r.text
        return text
```

---

## ğŸ server/model\_adapter/providers/vllm.py

```python
import httpx
from server.model_adapter.base import Provider

class VLLMProvider(Provider):
    def __init__(self, base_url: str = "http://localhost:8001/v1", model: str = "qwen72b"):
        self.base_url, self.model = base_url, model

    def health(self) -> bool:
        try:
            r = httpx.get(f"{self.base_url}/models", timeout=2)
            return r.status_code == 200
        except Exception:
            return False

    def generate(self, prompt: str, params: dict = {}) -> str:
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 512,
        }
        payload.update(params)
        r = httpx.post(f"{self.base_url}/chat/completions", json=payload, timeout=60)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"]
```

---

## ğŸ server/model\_adapter/router.py

```python
import yaml
from typing import Dict
from server.model_adapter.providers.ollama import OllamaProvider
from server.model_adapter.providers.vllm import VLLMProvider

class ModelRouter:
    def __init__(self, providers: Dict[str, object], prefer: list):
        self.providers = providers
        self.prefer = prefer

    @classmethod
    def load_from_yaml(cls, path: str):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)
        providers = {
            "ollama": OllamaProvider(model=cfg["ollama"]["model"], base_url=cfg["ollama"]["base_url"]),
            "vllm": VLLMProvider(model=cfg["vllm"]["model"], base_url=cfg["vllm"]["base_url"]),
        }
        return cls(providers, cfg.get("prefer", ["ollama", "vllm"]))

    def generate(self, prompt: str) -> str:
        # í—¬ìŠ¤ì²´í¬ ê¸°ë°˜ ì„ í˜¸ë„ ìˆœì„œë¡œ ì‹œë„, ì‹¤íŒ¨ ì‹œ í˜ì¼ì˜¤ë²„
        for key in self.prefer:
            prv = self.providers[key]
            if prv.health():
                try:
                    return prv.generate(prompt)
                except Exception:
                    continue
        # ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œë¼ë„ ì‹œë„
        return list(self.providers.values())[-1].generate(prompt)
```

---

## ğŸ§¾ configs/model-router.yaml

```yaml
ollama:
  base_url: http://ollama:11434
  model: llama3
vllm:
  base_url: http://vllm:8000/v1
  model: qwen72b
prefer: ["ollama", "vllm"]
```

---

## ğŸ server/rag/schemas.py

```python
from typing import List, Dict, Any
from pydantic import BaseModel

class Passage(BaseModel):
    id: str
    text: str
    meta: Dict[str, Any] = {}
    score: float = 0.0
```

---

## ğŸ server/rag/bm25.py

```python
from rank_bm25 import BM25Okapi
from typing import List
import re

def tokenize(text: str) -> List[str]:
    return re.findall(r"\w+", text.lower())

class BM25:
    def __init__(self, corpus: List[str]):
        self.tokens = [tokenize(t) for t in corpus]
        self.model = BM25Okapi(self.tokens)

    def search(self, q: str, k: int = 10):
        scores = self.model.get_scores(tokenize(q))
        idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        return idx, [scores[i] for i in idx]
```

---

## ğŸ server/rag/pipeline.py (Graphâ†’Parquetâ†’Embedâ†’FAISS)

```python
import os, json, uuid, time
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import faiss
from sentence_transformers import SentenceTransformer

PARQUET_DIR = "data/parquet"
FAISS_DIR = "data/faiss"
EMB_DIM = 768

_model = SentenceTransformer("intfloat/multilingual-e5-small")

# ë¬¸ì„œ â†’ ìœ ë‹›/ì—£ì§€ ìƒì„± (ê°„ì´)
def graphify(docs):
    units, edges = [], []
    for doc_id, text in docs:
        chunks = [t.strip() for t in text.split("\n\n") if t.strip()]
        for c in chunks:
            uid = str(uuid.uuid4())
            units.append({"unit_id": uid, "doc_id": doc_id, "text": c, "created_at": int(time.time())})
            # ê°„ë‹¨ ì—”í‹°í‹°/ê´€ê³„ ìƒ˜í”Œ
            edges.append({"src": uid, "rel": "MENTIONS", "tgt": doc_id})
    return pd.DataFrame(units), pd.DataFrame(edges)

# ìœ ë‹› ìš”ì•½(ë°ëª¨: ì›ë¬¸ ê·¸ëŒ€ë¡œ)
def summarize_units(df_units: pd.DataFrame):
    df_units["summary"] = df_units["text"].str.slice(0, 200)
    return df_units

# ì„ë² ë”© + FAISS ì¸ë±ìŠ¤ ë¹Œë“œ
def build_faiss(df_units: pd.DataFrame):
    os.makedirs(FAISS_DIR, exist_ok=True)
    texts = df_units["summary"].tolist()
    embs = _model.encode(["query: "+t for t in texts], normalize_embeddings=True)
    index = faiss.IndexHNSWFlat(EMB_DIM, 32)
    index.hnsw.efConstruction = 200
    index.add(np.array(embs).astype("float32"))
    faiss.write_index(index, os.path.join(FAISS_DIR, "hnsw.index"))

# Parquet ì €ì¥
def save_parquet(df_units, df_edges):
    os.makedirs(PARQUET_DIR, exist_ok=True)
    pq.write_table(pa.Table.from_pandas(df_units), os.path.join(PARQUET_DIR, "units.parquet"))
    pq.write_table(pa.Table.from_pandas(df_edges), os.path.join(PARQUET_DIR, "edges.parquet"))

# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
def run_pipeline(docs):
    units, edges = graphify(docs)
    units = summarize_units(units)
    save_parquet(units, edges)
    build_faiss(units)
```

---

## ğŸ server/rag/search.py (GraphRAG+FAISS + weighted rerank)

```python
import os, numpy as np, faiss, pandas as pd, time
from typing import Dict, Any
from sentence_transformers import SentenceTransformer, CrossEncoder
from .bm25 import BM25

PARQUET_DIR = "data/parquet"
FAISS_DIR = "data/faiss"
_emb = SentenceTransformer("intfloat/multilingual-e5-small")
_ce = None

cfg = {
  "k_dense": 40, "k_sparse": 40, "k_graph": 2,
  "weights": {"alpha":0.55,"beta":0.25,"gamma":0.15,"delta":0.05},
  "cross_encoder": {"enabled": True, "top_m": 10, "model": "BAAI/bge-reranker-v2-m3"}
}

def _load_units():
    df = pd.read_parquet(os.path.join(PARQUET_DIR, "units.parquet"))
    return df

def _faiss_index():
    return faiss.read_index(os.path.join(FAISS_DIR, "hnsw.index"))

_units_cache = None
_bm25 = None


def hybrid_search(q: str) -> Dict[str, Any]:
    global _units_cache, _bm25, _ce
    if _units_cache is None:
        _units_cache = _load_units()
        _bm25 = BM25(_units_cache["summary"].tolist())
    # DENSE
    qv = _emb.encode(["query: "+q], normalize_embeddings=True).astype("float32")
    index = _faiss_index()
    D, I = index.search(qv, cfg["k_dense"])  # distances
    dense_scores = 1 - (D[0])  # cosine approx

    # SPARSE(BM25)
    idx_bm25, bm25_scores = _bm25.search(q, k=cfg["k_sparse"]) if cfg["k_sparse"]>0 else ([],[])

    # GRAPH (Parquet ìœ ë‹› ê·¼ì ‘: ë°ëª¨ë¡œ recency ì‚¬ìš©)
    now = int(time.time())
    recency = (now - _units_cache.loc[I[0], "created_at"]).clip(lower=1)
    recency_scores = 1/np.log(recency+e := 2.71828)

    # Merge candidates
    candidates = {}
    def add(idx, s_dense=0, s_bm25=0, s_graph=0, s_rec=0):
        if idx not in candidates:
            candidates[idx] = {"dense":0,"bm25":0,"graph":0,"recency":0}
        candidates[idx]["dense"] = max(candidates[idx]["dense"], s_dense)
        candidates[idx]["bm25"] = max(candidates[idx]["bm25"], s_bm25)
        candidates[idx]["graph"] = max(candidates[idx]["graph"], s_graph)
        candidates[idx]["recency"] = max(candidates[idx]["recency"], s_rec)

    for rank, idx in enumerate(I[0]):
        add(int(idx), s_dense=float(dense_scores[rank]), s_rec=float(recency_scores[rank]))
    for rank, idx in enumerate(idx_bm25):
        add(int(idx), s_bm25=float(bm25_scores[rank]))

    w = cfg["weights"]
    for k,v in candidates.items():
        v["score"] = w["alpha"]*v["dense"] + w["beta"]*v["bm25"] + w["gamma"]*v["graph"] + w["delta"]*v["recency"]

    # top-N by weighted score
    top = sorted(candidates.items(), key=lambda kv: kv[1]["score"], reverse=True)[:cfg["cross_encoder"]["top_m"]]
    rows = [(i, _units_cache.iloc[i]["summary"]) for i,_ in top]

    # OPTIONAL cross-encoder rerank
    if cfg["cross_encoder"]["enabled"]:
        if _ce is None:
            _ce = CrossEncoder(cfg["cross_encoder"]["model"])  # loads on first use
        pairs = [(q, text) for _, text in rows]
        ces = _ce.predict(pairs)
        for idx, (i, _) in enumerate(rows):
            candidates[i]["score"] = float(ces[idx])
        top = sorted(candidates.items(), key=lambda kv: kv[1]["score"], reverse=True)[:10]

    passages = []
    for i,_ in top:
        row = _units_cache.iloc[i]
        passages.append({"id": str(row["unit_id"]), "text": row["summary"], "meta": {"doc_id": row["doc_id"]}, "score": candidates[i]["score"]})

    return {"passages": passages, "citations": [p["id"] for p in passages]}
```

---

## ğŸ server/vision/service.py

```python
from typing import List, Dict
from .storage import persist_temp_artifacts

def analyze_image(image_bytes: bytes, tasks: List[str]) -> Dict:
    # ë°ëª¨: ë°”ì´íŠ¸ ê¸¸ì´ì™€ ìš”ì²­ íƒœìŠ¤í¬ ì—ì½” + (ì˜µì…˜) ì„ì‹œ ë³´ê´€
    ref = persist_temp_artifacts(image_bytes, meta={"tasks": tasks})
    return {"tasks": tasks, "bytes": len(image_bytes), "artifact_ref": ref}
```

---

## ğŸ server/vision/storage.py (S3/MinIO ì„ì‹œ ë³´ê´€ - ì„ íƒ)

```python
import boto3, os, hashlib, time
from server.core.config import settings

s3cfg = settings.cfg.get("stores", {}).get("s3", {})

s3 = boto3.client(
    "s3",
    endpoint_url=s3cfg.get("endpoint_url"),
    aws_access_key_id=s3cfg.get("access_key"),
    aws_secret_access_key=s3cfg.get("secret_key"),
    region_name="us-east-1",
    use_ssl=s3cfg.get("secure", False),
)

BUCKET = s3cfg.get("bucket", "ai-vision-temp")


def persist_temp_artifacts(image_bytes: bytes, meta: dict):
    if not image_bytes:
        return None
    h = hashlib.sha1(image_bytes).hexdigest()
    ts = int(time.time())
    key = f"temp/{ts}/{h}.bin"
    s3.put_object(Bucket=BUCKET, Key=key, Body=image_bytes)
    s3.put_object(Bucket=BUCKET, Key=key.replace(".bin", ".json"), Body=str(meta).encode())
    return {"s3": f"s3://{BUCKET}/{key}"}
```

---

## ğŸ server/slack/routes.py (HITL ìŠ¹ì¸ íë¦„)

```python
from fastapi import APIRouter, Request
from typing import Dict, Any
from server.rag.pipeline import run_pipeline

router = APIRouter()

@router.post("/kb/propose")
async def kb_propose(payload: Dict[str, Any]):
    # Slack ì¹´ë“œë¡œ ë³´ë‚¼ í˜ì´ë¡œë“œ ìƒì„± (ë°ëª¨)
    return {"ok": True, "message": "Proposed", "payload": payload}

@router.post("/kb/approve")
async def kb_approve(payload: Dict[str, Any]):
    # ìŠ¹ì¸ ì‹œ: Parquet íŒŒí‹°ì…˜ ì¦ë¶„ + FAISS add or rebuild
    docs = [(payload.get("doc_id", "ops"), payload.get("answer", ""))]
    run_pipeline(docs)  # ê°„ì´: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰(ë°ëª¨)
    return {"ok": True, "message": "Indexed"}

@router.post("/kb/reject")
async def kb_reject(payload: Dict[str, Any]):
    return {"ok": True, "message": "Rejected", "reason": payload.get("reason")}
```

---

## ğŸ server/ops/metrics.py

```python
from contextlib import contextmanager
import time

@contextmanager
def trace(name: str):
    t0 = time.time()
    try:
        yield
    finally:
        dt = (time.time() - t0) * 1000
        print(f"[trace] {name} took {dt:.1f}ms")
```

---

## ğŸ server/ops/health.py

```python
from fastapi import APIRouter

router = APIRouter()

@router.get("/healthz")
async def healthz():
    return {"status": "ok"}

@router.get("/readyz")
async def readyz():
    return {"status": "ready"}
```

---

## ğŸ scripts/build\_faiss.py

```python
import argparse, glob
from server.rag.pipeline import run_pipeline

def load_docs(path_glob: str):
    docs = []
    for p in glob.glob(path_glob):
        with open(p, "r", encoding="utf-8", errors="ignore") as f:
            docs.append((p, f.read()))
    return docs

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--docs", default="docs/*.txt")
    ap.add_argument("--config", default="configs/app.yaml")
    args = ap.parse_args()
    docs = load_docs(args.docs)
    run_pipeline(docs)
    print("FAISS index built.")
```

---

## ğŸ scripts/reindex.py

```python
# ìš´ì˜ì—ì„œ ë¶€ë¶„ ì¦ë¶„ ê°±ì‹ /ë¦¬ë¹Œë“œ ìˆ˜í–‰ìš©(ë°ëª¨ë¡œ ì „ì²´ ì¬ì‹¤í–‰)
from server.rag.pipeline import run_pipeline

if __name__ == "__main__":
    run_pipeline([("ops-update", "ì‹ ê·œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë‹µë³€ ë³¸ë¬¸...")])
    print("Reindexed.")
```

---

## â˜¸ï¸ helm/Chart.yaml

```yaml
apiVersion: v2
name: onboarding-agent
version: 0.1.0
appVersion: "0.1.0"
description: Onboarding Agent (OSI-only)
```

### â˜¸ï¸ helm/values.yaml (ìš”ì•½)

```yaml
image:
  repository: ghcr.io/example/onboarding-agent
  tag: v0.1.0
service:
  type: ClusterIP
  port: 8000
env:
  APP_CONFIG: /config/app.yaml
volumes:
  - name: config
    mountPath: /config
```

### â˜¸ï¸ helm/templates/deployment.yaml (ìš”ì•½)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: onboarding-agent }
spec:
  replicas: 2
  selector: { matchLabels: { app: onboarding-agent } }
  template:
    metadata: { labels: { app: onboarding-agent } }
    spec:
      containers:
        - name: api
          image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
          ports: [{ containerPort: 8000 }]
          env:
            - name: APP_CONFIG
              value: {{ .Values.env.APP_CONFIG }}
          volumeMounts:
            - name: config
              mountPath: /config
      volumes:
        - name: config
          configMap: { name: onboarding-agent-config }
```

---

## ğŸ¤– mobile/android/OnboardingAgentGating.kt (ê²Œì´íŒ… ìŠ¤í…)

```kotlin
package ai.onboarding

object LocalGateCfg {
    const val LOW = 0.45
    const val HIGH = 0.75
}

class OnDeviceLLM {
    fun inferIntent(text: String): Triple<String, Double, Map<String, String>> {
        val label = if (text.contains("ì˜¤ë¥˜") || text.contains("ì—ëŸ¬")) "troubleshooting" else "faq"
        val score = if (text.contains("ì‚¬ì§„") || text.contains("ì´ë¯¸ì§€")) 0.7 else 0.5
        return Triple(label, score, emptyMap())
    }
}

class Gating {
    private val llm = OnDeviceLLM()
    fun route(text: String): Action {
        val (label, score, slots) = llm.inferIntent(text)
        return when {
            score < LocalGateCfg.LOW -> Action.Clarify("ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”")
            score < LocalGateCfg.HIGH -> Action.Escalate("/chat", mapOf("text" to text, "intent" to label, "slots" to slots))
            else -> Action.LocalAnswer("ê°„ë‹¨ ë‹µë³€: $label")
        }
    }
}

sealed class Action {
    data class Clarify(val msg: String): Action()
    data class Escalate(val path: String, val payload: Map<String, Any?>): Action()
    data class LocalAnswer(val msg: String): Action()
}
```

---

## ğŸ mobile/ios/OnboardingAgentGating.swift

```swift
import Foundation

struct LocalGateCfg { static let low = 0.45; static let high = 0.75 }

class OnDeviceLLM {
  func inferIntent(_ text: String) -> (String, Double, [String:String]) {
    let label = (text.contains("ì˜¤ë¥˜") || text.contains("ì—ëŸ¬")) ? "troubleshooting" : "faq"
    let score = (text.contains("ì‚¬ì§„") || text.contains("ì´ë¯¸ì§€")) ? 0.7 : 0.5
    return (label, score, [:])
  }
}

enum Action {
  case clarify(String)
  case escalate(String, [String:Any])
  case localAnswer(String)
}

class Gating {
  let llm = OnDeviceLLM()
  func route(_ text: String) -> Action {
    let (label, score, slots) = llm.inferIntent(text)
    if score < LocalGateCfg.low { return .clarify("ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”") }
    if score < LocalGateCfg.high { return .escalate("/chat", ["text": text, "intent": label, "slots": slots]) }
    return .localAnswer("ê°„ë‹¨ ë‹µë³€: \(label)")
  }
}
```

---

### âœ… ì°¸ê³ 

* ëª¨ë“  ì˜ì¡´ì„±ì€ OSI ìŠ¹ì¸ ë¼ì´ì„ ìŠ¤(ì˜ˆ: FastAPI/MIT, Uvicorn/BSD, Pydantic/MIT, FAISS/MIT, NumPy/Pandas/BSD, PyArrow/Apache-2.0, rank\_bm25/MIT, sentence-transformers/Apache-2.0, slack\_sdk/MIT, boto3/Apache-2.0) ê¸°ë°˜ì…ë‹ˆë‹¤.
* ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„  ëª¨ë¸/í† í° í•œë„, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, ì •ì±… í•„í„°, ë²ˆì—­ ë§¤ë‹ˆì €, ë¡œê¹…/ë©”íŠ¸ë¦­ì„ ê°•í™”í•˜ì„¸ìš”.
* GraphDBëŠ” **ì˜µì…˜**ì´ë©° Parquetë¡œ ì‹œì‘ â†’ í•„ìš” ì‹œ JanusGraph/NebulaGraphë¡œ í™•ì¥.
