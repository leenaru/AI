#!/bin/bash
# setup.sh - ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ IoT AI Agent ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤..."

# ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
echo "ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘..."
mkdir -p server client logs data/models data/indices

# ê°€ìƒí™˜ê²½ ìƒì„±
echo "ğŸ Python ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
python3 -m venv venv
source venv/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ğŸ“¦ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
pip install --upgrade pip
pip install -r requirements.txt

# Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
echo "ğŸ¤– Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
if command -v ollama &> /dev/null; then
    ollama pull gemma2:2b
    ollama pull llama3.2:1b
else
    echo "âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. https://ollama.ai ì—ì„œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
fi

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±
if [ ! -f .env ]; then
    echo "âš™ï¸  í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„± ì¤‘..."
    cat > .env <<EOL
# Model Settings
MODEL_BACKEND=OLLAMA
MODEL_NAME=gemma2:2b
EMBEDDING_DIM=768

# API Keys (Optional)
OPENAI_API_KEY=
HUGGING_FACE_HUB_TOKEN=
LANGSMITH_API_KEY=

# Server Settings
LOG_LEVEL=INFO
DEBUG=False

# URLs
OLLAMA_BASE_URL=http://localhost:11434
VLLM_BASE_URL=http://localhost:8001
REDIS_URL=redis://localhost:6379
EOL
fi

echo "âœ… ì„¤ì • ì™„ë£Œ!"

---
#!/bin/bash
# start.sh - ì‹œìŠ¤í…œ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ IoT AI Agent ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤..."

# Docker Composeë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
if [ "$1" == "docker" ]; then
    echo "ğŸ³ Docker Composeë¡œ ì‹œì‘ ì¤‘..."
    docker-compose up -d
    echo "âœ… ì‹œìŠ¤í…œì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
    echo "ğŸ“ API Server: http://localhost:8000"
    echo "ğŸ“ Streamlit UI: http://localhost:8501"
    echo "ğŸ“ API Docs: http://localhost:8000/docs"
    exit 0
fi

# ë¡œì»¬ ê°œë°œ í™˜ê²½ìœ¼ë¡œ ì‹œì‘
echo "ğŸ’» ë¡œì»¬ ê°œë°œ í™˜ê²½ìœ¼ë¡œ ì‹œì‘ ì¤‘..."

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# Ollama ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
if command -v ollama &> /dev/null; then
    echo "ğŸ¤– Ollama ì„œë²„ ì‹œì‘ ì¤‘..."
    ollama serve &
    OLLAMA_PID=$!
    sleep 5
else
    echo "âš ï¸  Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”."
fi

# Redis ì‹œì‘ (Docker)
echo "ğŸ“¦ Redis ì‹œì‘ ì¤‘..."
docker run -d --name iot-redis -p 6379:6379 redis:alpine

# FastAPI ì„œë²„ ì‹œì‘
echo "ğŸ”§ API ì„œë²„ ì‹œì‘ ì¤‘..."
cd server
uvicorn main:app --reload --port 8000 &
API_PID=$!
cd ..

# Streamlit UI ì‹œì‘
echo "ğŸ¨ Streamlit UI ì‹œì‘ ì¤‘..."
cd client
streamlit run streamlit_app.py --server.port 8501 &
UI_PID=$!
cd ..

echo "âœ… ì‹œìŠ¤í…œì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "ğŸ“ API Server: http://localhost:8000"
echo "ğŸ“ Streamlit UI: http://localhost:8501"
echo "ğŸ“ API Docs: http://localhost:8000/docs"
echo ""
echo "ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”."

# ì¢…ë£Œ ì²˜ë¦¬
trap "kill $OLLAMA_PID $API_PID $UI_PID; docker stop iot-redis; docker rm iot-redis" EXIT

# í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸°
wait

---
#!/bin/bash
# test.sh - í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."

# API í—¬ìŠ¤ì²´í¬
echo "1ï¸âƒ£ API ì„œë²„ ìƒíƒœ í™•ì¸..."
curl -s http://localhost:8000/health | python3 -m json.tool

echo ""
echo "2ï¸âƒ£ ìƒ˜í”Œ ì±„íŒ… í…ŒìŠ¤íŠ¸..."
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test_user",
    "message": "ìŠ¤ë§ˆíŠ¸ í—ˆë¸Œë¥¼ ì„¤ì •í•˜ê³  ì‹¶ì–´ìš”",
    "mode": "on_demand"
  }' | python3 -m json.tool

echo ""
echo "3ï¸âƒ£ Ollama ëª¨ë¸ í™•ì¸..."
curl http://localhost:11434/api/tags | python3 -m json.tool

echo ""
echo "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"

---
# README.md

# ğŸ¤– IoT AI Agent System

IoT ê¸°ê¸°ì˜ ì„¤ì¹˜ë¶€í„° ì¥ì• ê¹Œì§€, ì‚¬ìš©ìê°€ ì¹´ë©”ë¼ë¥¼ ë¹„ì¶”ê³  ë§í•˜ë©´ ë°”ë¡œ í•´ê²°í•˜ëŠ” ëŒ€í™”í˜• AI Agent ì‹œìŠ¤í…œ

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **ê¸°ê¸° ì˜¨ë³´ë”©**: ìŠ¤ë§ˆíŠ¸ ê¸°ê¸° ìë™ ê°ì§€ ë° ë“±ë¡
- **ë¬¸ì œ í•´ê²°**: ì‹¤ì‹œê°„ Troubleshooting
- **VOC ì ‘ìˆ˜**: ê³ ê° ë¶ˆë§Œ ë° ìš”ì²­ ìë™ ë¶„ë¥˜
- **ì—ëŸ¬ ì½”ë“œ í•´ê²°**: ì¦‰ê°ì ì¸ ì—ëŸ¬ í•´ê²° ê°€ì´ë“œ
- **ë§¤ë‰´ì–¼ ì œê³µ**: ëŒ€í™”í˜• ì‚¬ìš© ì„¤ëª…ì„œ
- **êµ¬ë§¤ ê°€ì´ë“œ**: ë§ì¶¤í˜• ì œí’ˆ ì¶”ì²œ

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### Backend
- **FastAPI**: ê³ ì„±ëŠ¥ ë¹„ë™ê¸° API ì„œë²„
- **LangGraph**: ë³µì¡í•œ ëŒ€í™” í”Œë¡œìš° ê´€ë¦¬
- **vLLM/Ollama**: ì˜¨í”„ë ˆë¯¸ìŠ¤ LLM ì„œë¹™
- **GraphRAG + FAISS + BM25**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- **Redis**: ìºì‹± ë° ì„¸ì…˜ ê´€ë¦¬

### Frontend  
- **Streamlit**: ì›¹ ê¸°ë°˜ ëŒ€í™”í˜• UI
- **WebSocket**: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­
- Python 3.10+
- Docker & Docker Compose
- 8GB+ RAM (LLM ì‹¤í–‰ìš©)
- (ì„ íƒ) NVIDIA GPU

### ì„¤ì¹˜ ë° ì‹¤í–‰

1. **ì €ì¥ì†Œ í´ë¡ **
```bash
git clone https://github.com/your-org/iot-ai-agent.git
cd iot-ai-agent
```

2. **ì´ˆê¸° ì„¤ì •**
```bash
chmod +x setup.sh
./setup.sh
```

3. **ì‹œìŠ¤í…œ ì‹œì‘**

Docker Compose ì‚¬ìš© (ê¶Œì¥):
```bash
./start.sh docker
```

ë¡œì»¬ ê°œë°œ í™˜ê²½:
```bash
./start.sh
```

4. **ì ‘ì†**
- ì›¹ UI: http://localhost:8501
- API ë¬¸ì„œ: http://localhost:8000/docs
- í—¬ìŠ¤ì²´í¬: http://localhost:8000/health

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
iot-ai-agent/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ main.py              # FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ orchestrator.py      # LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”œâ”€â”€ model_adapter.py     # LLM ì–´ëŒ‘í„° (vLLM/Ollama)
â”‚   â”œâ”€â”€ kce.py              # Knowledge Context Engine
â”‚   â””â”€â”€ config.py           # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ client/
â”‚   â””â”€â”€ streamlit_app.py    # Streamlit UI
â”œâ”€â”€ docker-compose.yml       # Docker êµ¬ì„±
â”œâ”€â”€ requirements.txt         # Python íŒ¨í‚¤ì§€
â”œâ”€â”€ setup.sh                # ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ start.sh                # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md

```

## ğŸ”§ ì„¤ì •

`.env` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```env
# ëª¨ë¸ ì„¤ì •
MODEL_BACKEND=OLLAMA        # OLLAMA, VLLM, OPENAI
MODEL_NAME=gemma2:2b        # ì‚¬ìš©í•  ëª¨ë¸ëª…

# ë¡œê¹…
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
ENABLE_LANGSMITH=false      # LangSmith ëª¨ë‹ˆí„°ë§
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

- **API ìƒíƒœ**: http://localhost:8000/health
- **ë¡œê·¸**: `logs/agent.log`
- **LangSmith**: í™˜ê²½ë³€ìˆ˜ì—ì„œ í™œì„±í™” ì‹œ ìë™ ì—°ë™

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
./test.sh
```

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! PRì„ ì œì¶œí•´ì£¼ì„¸ìš”.

## ğŸ“œ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### Ollama ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
ollama pull gemma2:2b
ollama serve
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
`.env`ì—ì„œ ë” ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½:
```env
MODEL_NAME=llama3.2:1b
```

### í¬íŠ¸ ì¶©ëŒ
Docker Compose íŒŒì¼ì—ì„œ í¬íŠ¸ ë²ˆí˜¸ ë³€ê²½

## ğŸ“ ì§€ì›

- Issues: GitHub Issuesì— ë“±ë¡
- Email: support@example.com