#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlit VOC Topic Explorer (JSON input)
----------------------------------------

ê¸°ëŠ¥ ìš”ì•½
- JSON/NDJSON VOC íŒŒì¼ ì—…ë¡œë“œ â†’ ì„ë² ë”© â†’ (ì„ íƒ) UMAP â†’ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ â†’ c-TF-IDF í‚¤ì›Œë“œ â†’ ìë™ ë¼ë²¨ â†’ ëŒ€ì‹œë³´ë“œ ì‹œê°í™”
- í˜¹ì€, ì´ì „ íŒŒì´í”„ë¼ì¸ ì‚°ì¶œë¬¼(ë¬¸ì„œë³„ docs.jsonl + summary.json) ì—…ë¡œë“œí•˜ì—¬ ì¦‰ì‹œ ì‹œê°í™”
- UMAP 2D ì¸í„°ë™í‹°ë¸Œ ì‚°ì ë„(Plotly), í´ëŸ¬ìŠ¤í„° í¬ê¸° ë§‰ëŒ€ê·¸ë˜í”„, êµ°ì§‘ë³„ Top í‚¤ì›Œë“œ, ìƒ˜í”Œ ë¬¸ì¥, (ì„ íƒ) ì‹œê³„ì—´ ì¶”ì´
- ê²°ê³¼(ë¬¸ì„œë³„, ìš”ì•½)ë¥¼ JSON/CSVë¡œ ë‹¤ìš´ë¡œë“œ

ì‹¤í–‰ ë°©ë²•
1) ì˜ì¡´ì„± ì„¤ì¹˜:
   pip install -U streamlit pandas numpy scikit-learn sentence-transformers hdbscan umap-learn plotly
   # (ì˜µì…˜) BERTopic ì‚¬ìš©ì‹œ
   pip install -U bertopic

2) ì•± ì‹¤í–‰:
   streamlit run app.py

ì…ë ¥ í˜•ì‹
- ì›ë³¸ JSON: ë°°ì—´ JSON([{}, {}, ...]) ë˜ëŠ” NDJSON(ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON)
- í…ìŠ¤íŠ¸ í•„ë“œê°€ ì—¬ëŸ¬ ê³³ì— ìˆìœ¼ë©´ ì í‘œê¸° ê²½ë¡œë¡œ ë‹¤ì¤‘ ì„ íƒ(ì˜ˆ: title, body, user.comment)
- ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ ì‚¬ìš©: voc_topics_docs.jsonl + voc_topics_summary.json ì—…ë¡œë“œ

ë©”ëª¨
- ëŒ€ìš©ëŸ‰ì—ì„œ UMAP/ì„ë² ë”©ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒ˜í”Œë§ ì˜µì…˜ì„ ì œê³µí•˜ë©°, ìµœì†Œ íŒŒë¼ë¯¸í„°ë¡œ ë¨¼ì € ëŒë ¤ë³´ì„¸ìš”.
- ìƒ‰ìƒì€ ê¸°ë³¸ íŒ”ë ˆíŠ¸ ì‚¬ìš©(ì§€ì •í•˜ì§€ ì•ŠìŒ), í•œ ì°¨íŠ¸ì— í•œ Figure ì›ì¹™ì€ matplotlibì—ì„œì˜ ì œì•½ì´ì§€ë§Œ ì—¬ê¸°ì„  Plotlyë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import io
import json
import os
import re
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

# ì„ë² ë”©/ì°¨ì›ì¶•ì†Œ/í´ëŸ¬ìŠ¤í„°ë§
from sentence_transformers import SentenceTransformer

try:
    import umap  # type: ignore
    _HAS_UMAP = True
except Exception:
    _HAS_UMAP = False

import hdbscan  # type: ignore
from sklearn.feature_extraction.text import TfidfVectorizer

try:
    from bertopic import BERTopic  # type: ignore
    _HAS_BERTOPIC = True
except Exception:
    _HAS_BERTOPIC = False

# ---------------------------
# ìœ í‹¸: JSON ì½ê¸°/í”Œë˜íŠ¼/í…ìŠ¤íŠ¸ ê²°í•©
# ---------------------------
_URL_RE = re.compile(r"http[s]?://\S+")
_EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
_PHONE_RE = re.compile(r"\b\d{2,4}[-\s]?\d{2,4}[-\s]?\d{2,4}\b")


def clean_text(s: str) -> str:
    s = str(s)
    s = _URL_RE.sub("<URL>", s)
    s = _EMAIL_RE.sub("<EMAIL>", s)
    s = _PHONE_RE.sub("<PHONE>", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()


def _read_json_or_ndjson(file: io.BytesIO) -> List[Dict[str, Any]]:
    raw = file.getvalue().decode("utf-8", errors="ignore").strip()
    if not raw:
        return []
    if raw.startswith("[") and raw.endswith("]"):
        data = json.loads(raw)
        if not isinstance(data, list):
            raise ValueError("Top-level JSON must be a list.")
        return data
    # NDJSON
    rows = []
    for ln, line in enumerate(raw.splitlines(), 1):
        line = line.strip()
        if not line:
            continue
        try:
            obj = json.loads(line)
        except Exception as e:
            raise ValueError(f"Invalid NDJSON at line {ln}: {e}")
        rows.append(obj)
    return rows


def _flatten(d: Dict[str, Any], parent_key: str = "", sep: str = ".") -> Dict[str, Any]:
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else str(k)
        if isinstance(v, dict):
            items.extend(_flatten(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


def discover_keys(records: List[Dict[str, Any]], sample_n: int = 200) -> List[str]:
    keys = set()
    for rec in records[:sample_n]:
        if isinstance(rec, dict):
            flat = _flatten(rec)
            for k in flat.keys():
                keys.add(k)
    return sorted(keys)


def join_fields(flat: Dict[str, Any], fields: List[str]) -> str:
    parts: List[str] = []
    for f in fields:
        val = flat.get(f)
        if val is None:
            continue
        if isinstance(val, (list, tuple)):
            parts.append(" ".join([str(x) for x in val if x is not None]))
        else:
            parts.append(str(val))
    return " ".join([p for p in parts if p]).strip()


# ---------------------------
# ìºì‹œ: ì„ë² ë”© ëª¨ë¸/ê³„ì‚°
# ---------------------------
@st.cache_resource(show_spinner=False)
def load_model(name: str) -> SentenceTransformer:
    return SentenceTransformer(name)


@st.cache_data(show_spinner=True)
def compute_embeddings(texts: List[str], model_name: str) -> np.ndarray:
    model = load_model(model_name)
    emb = model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)
    return np.asarray(emb, dtype=np.float32)


@st.cache_data(show_spinner=True)
def run_umap(emb: np.ndarray, n_components: int = 2, random_state: int = 42) -> np.ndarray:
    if not _HAS_UMAP:
        raise RuntimeError("umap-learn ë¯¸ì„¤ì¹˜: pip install umap-learn")
    reducer = umap.UMAP(n_neighbors=15, n_components=n_components, min_dist=0.0, metric="cosine", random_state=random_state)
    coords = reducer.fit_transform(emb)
    return coords


@st.cache_data(show_spinner=True)
def run_hdbscan(X: np.ndarray, min_cluster_size: int = 20) -> np.ndarray:
    cl = hdbscan.HDBSCAN(min_cluster_size=max(5, int(min_cluster_size)), metric="euclidean", cluster_selection_method="eom", prediction_data=True)
    return cl.fit_predict(X)


# ---------------------------
# í‚¤ì›Œë“œ/ë¼ë²¨
# ---------------------------
@st.cache_data(show_spinner=False)
def c_tf_idf_keywords(big_docs: List[str], topn: int = 10) -> Tuple[List[List[str]], List[int]]:
    vec = TfidfVectorizer(min_df=2, ngram_range=(1, 2), analyzer="word", lowercase=True)
    X = vec.fit_transform(big_docs)
    terms = np.array(vec.get_feature_names_out())
    top_idx_mat = np.argsort(X.toarray(), axis=1)[:, ::-1][:, :topn]
    top_terms = [[terms[j] for j in row] for row in top_idx_mat]
    vocab_size = int(len(terms))
    return top_terms, [vocab_size] * len(big_docs)


_KO_STOP = set([
    "ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ë˜ëŠ”","ë˜","ë°","ê·¸ë˜ì„œ","ì •ë„","ë¶€ë¶„","ì‚¬ìš©","ë¬¸ì œ","í˜„ìƒ",
    "ë¬¸ì˜","ìš”ì²­","ê´€ë ¨","ë°œìƒ","ì´ìŠˆ","í•´ê²°","ê°€ëŠ¥","í™•ì¸","ì œëŒ€ë¡œ","ì „ì²´","ì¼ë¶€","ì‚¬ìš©ì",
    "ë°œì†¡","ì§„í–‰","ìƒíƒœ","ì´í›„","í˜„ì¬","í•„ìš”","ì ìš©","ì—…ë°ì´íŠ¸","ë²„ì „","ì •ë³´","ë°ì´í„°"
])


def label_from_keywords(kws: List[str], max_tokens: int = 3) -> str:
    filtered: List[str] = []
    for w in kws:
        w2 = w.replace("_", " ").replace("-", " ").strip()
        if not w2 or len(w2) <= 1 or w2 in _KO_STOP:
            continue
        filtered.append(w2)
    if not filtered:
        filtered = kws[:max_tokens]
    return " / ".join(filtered[:max_tokens])


# ---------------------------
# SEED ë¡œë”©/ë§¤ì¹­ + í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ìœ í‹¸
# ---------------------------
def load_seeds_from_inputs(file, text_input: str):
    """CSV(category,type,value) ë˜ëŠ” JSON ë°°ì—´ì„ ë°›ì•„ í‘œì¤€ dictë¡œ ì •ê·œí™”
    return {category: {"keyword": [...], "regex": [...], "example": [...]}}
    """
    import io as _io
    seeds = {}
    # íŒŒì¼ ìš°ì„ 
    if file is not None:
        content = file.getvalue().decode("utf-8", errors="ignore").strip()
        if content:
            if file.name.lower().endswith(".json"):
                try:
                    arr = json.loads(content)
                    assert isinstance(arr, list)
                    for row in arr:
                        cat = str(row.get("category",""))
                        typ = str(row.get("type","keyword")).lower()
                        val = row.get("value", "")
                        if not cat or not val:
                            continue
                        seeds.setdefault(cat, {"keyword":[],"regex":[],"example":[]})
                        if isinstance(val, list):
                            seeds[cat][typ].extend([str(v) for v in val])
                        else:
                            seeds[cat][typ].append(str(val))
                except Exception:
                    st.warning("Seed JSON íŒŒì‹± ì‹¤íŒ¨: CSVë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
            else:
                try:
                    df = pd.read_csv(_io.StringIO(content))
                    for _, r in df.iterrows():
                        cat = str(r.get("category",""))
                        typ = str(r.get("type","keyword")).lower()
                        val = str(r.get("value",""))
                        if not cat or not val:
                            continue
                        seeds.setdefault(cat, {"keyword":[],"regex":[],"example":[]})
                        seeds[cat][typ].append(val)
                except Exception as e:
                    st.error(f"Seed CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
    # í…ìŠ¤íŠ¸ ì…ë ¥(ì„ íƒ)
    if text_input and text_input.strip():
        try:
            arr = json.loads(text_input)
            assert isinstance(arr, list)
            for row in arr:
                cat = str(row.get("category",""))
                typ = str(row.get("type","keyword")).lower()
                val = row.get("value", "")
                if not cat or not val:
                    continue
                seeds.setdefault(cat, {"keyword":[],"regex":[],"example":[]})
                if isinstance(val, list):
                    seeds[cat][typ].extend([str(v) for v in val])
                else:
                    seeds[cat][typ].append(str(val))
        except Exception as e:
            st.error(f"Seed JSON í…ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
    return seeds


def seed_assignments(df: pd.DataFrame, seeds: dict, model_name: str,
                     use_kw_regex: bool = True, use_examples: bool = True, example_thresh: float = 0.55,
                     doc_emb: Optional[np.ndarray] = None):
    """ì‚¬ì „ ì¹´í…Œê³ ë¦¬ í• ë‹¹. ë°˜í™˜: (df[seed_* ì»¬ëŸ¼ ì±„ì›€], ì„ë² ë”©)"""
    df = df.copy()
    df["seed_category"] = None
    df["seed_method"] = None
    df["seed_score"] = 0.0
    if not seeds:
        return df, doc_emb

    # 1) í‚¤ì›Œë“œ/ì •ê·œì‹ ë§¤ì¹­
    if use_kw_regex:
        texts_lower = df["text_clean"].astype(str).str.lower().tolist()
        compiled_regex = {cat: [re.compile(p, flags=re.IGNORECASE) for p in spec.get("regex", []) if str(p).strip()]
                          for cat, spec in seeds.items()}
        for i, t in enumerate(texts_lower):
            best_cat, best_score = None, 0
            for cat, spec in seeds.items():
                score = 0
                for kw in spec.get("keyword", []):
                    k = str(kw).lower().strip()
                    if not k:
                        continue
                    score += t.count(k)
                for rgx in compiled_regex.get(cat, []):
                    score += len(rgx.findall(t))
                if score > best_score:
                    best_cat, best_score = cat, score
            if best_cat and best_score > 0:
                df.loc[df.index[i], ["seed_category","seed_method","seed_score"]] = [best_cat, "kw/regex", float(best_score)]

    # 2) ì˜ˆì‹œë¬¸ì¥ ì„ë² ë”© ë§¤ì¹­
    if use_examples:
        if doc_emb is None:
            doc_emb = compute_embeddings(df["text_clean"].astype(str).tolist(), model_name)
        cat_proto = {}
        for cat, spec in seeds.items():
            exs = [clean_text(x) for x in spec.get("example", []) if str(x).strip()]
            if not exs:
                continue
            vecs = compute_embeddings(exs, model_name)
            proto = vecs.mean(axis=0)
            proto = proto / (np.linalg.norm(proto) + 1e-12)
            cat_proto[cat] = proto
        if cat_proto:
            protos = np.stack(list(cat_proto.values()))
            cats = list(cat_proto.keys())
            sims = doc_emb @ protos.T
            mask_un = df["seed_category"].isna().values
            for i, un in enumerate(mask_un):
                if not un:
                    continue
                row = sims[i]
                j = int(np.argmax(row))
                s = float(row[j])
                if s >= example_thresh:
                    df.loc[df.index[i], ["seed_category","seed_method","seed_score"]] = [cats[j], "example", s]
    return df, doc_emb


def pipeline_from_raw_hybrid(records: List[Dict[str, Any]], text_fields: List[str], id_field: Optional[str], model_name: str,
                             min_topic_size: int, umap_components: int, engine: str,
                             seeds: Optional[dict] = None,
                             use_kw_regex: bool = True, use_examples: bool = True, example_thresh: float = 0.55,
                             exclude_seeded_from_clustering: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:
    # ì›ë³¸ â†’ DF
    rows = []
    for i, rec in enumerate(records):
        if not isinstance(rec, dict):
            continue
        flat = _flatten(rec)
        doc_id = flat.get(id_field) if id_field else None
        if doc_id is None:
            doc_id = f"doc_{i+1}"
        text = join_fields(flat, text_fields)
        rows.append({"id": doc_id, "text": text})
    df = pd.DataFrame(rows)
    df["text_clean"] = df["text"].map(clean_text)
    df = df[df["text_clean"].str.len() > 5].drop_duplicates(subset=["text_clean"]).reset_index(drop=True)
    if len(df) == 0:
        raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # SEED ì‚¬ì „í• ë‹¹
    emb_for_examples = None
    if seeds:
        df, emb_for_examples = seed_assignments(df, seeds, model_name, use_kw_regex, use_examples, example_thresh, None)
    else:
        df["seed_category"], df["seed_method"], df["seed_score"] = None, None, 0.0

    # í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ
    if exclude_seeded_from_clustering and seeds:
        df_cl = df[df["seed_category"].isna()].copy()
    else:
        df_cl = df.copy()

    # ì„ë² ë”© ë° í´ëŸ¬ìŠ¤í„°ë§
    emb = compute_embeddings(df_cl["text_clean"].tolist(), model_name)
    if engine == "bertopic":
        if not _HAS_BERTOPIC:
            raise RuntimeError("BERTopic ë¯¸ì„¤ì¹˜: pip install bertopic")
        topic_model = BERTopic(language="multilingual", n_gram_range=(1,2), min_topic_size=max(5, int(min_topic_size)), calculate_probabilities=True, verbose=False)
        topics, _ = topic_model.fit_transform(df_cl["text_clean"].tolist(), embeddings=emb)
        df_cl["cluster"] = topics
        info = topic_model.get_topic_info()
        summary_records = []
        for _, row in info.iterrows():
            cid = int(row["Topic"])
            if cid == -1:
                continue
            topic_terms = topic_model.get_topic(cid) or []
            top_terms = [t for t, _ in topic_terms[:10]]
            auto_label = label_from_keywords(top_terms, max_tokens=3)
            idxs = df_cl.index[df_cl["cluster"] == cid].tolist()
            sample_texts = [df_cl.loc[k, "text_clean"] for k in idxs[: min(3, len(idxs))]]
            summary_records.append({"cluster": cid, "size": int(row["Count"]), "auto_label": auto_label, "keywords": top_terms, "sample_texts": sample_texts})
        sum_df = pd.DataFrame(summary_records).sort_values("size", ascending=False).reset_index(drop=True)
    else:
        X = emb
        if umap_components > 0:
            X = run_umap(emb, n_components=umap_components)
        labels = run_hdbscan(X, min_cluster_size=min_topic_size)
        df_cl["cluster"] = labels
        by_cluster = {}
        for i, c in enumerate(labels):
            if c == -1:
                continue
            by_cluster.setdefault(int(c), []).append(df_cl.index[i])
        summary_records, cid_to_terms, cid_to_label = [], {}, {}
        if by_cluster:
            big_docs = [" ".join(df_cl.loc[idxs, "text_clean"]) for _, idxs in sorted(by_cluster.items())]
            clusters_sorted = [cid for cid, _ in sorted(by_cluster.items())]
            top_terms_list, _ = c_tf_idf_keywords(big_docs, topn=10)
            for cid, terms in zip(clusters_sorted, top_terms_list):
                auto_label = label_from_keywords(terms, max_tokens=3)
                cid_to_terms[cid] = terms
                cid_to_label[cid] = auto_label
                idxs = by_cluster[cid]
                sample_texts = [df_cl.loc[k, "text_clean"] for k in idxs[: min(3, len(idxs))]]
                summary_records.append({"cluster": cid, "size": len(idxs), "auto_label": auto_label, "keywords": terms, "sample_texts": sample_texts})
        sum_df = pd.DataFrame(summary_records).sort_values("size", ascending=False).reset_index(drop=True)

    # ì›ë³¸ dfì— ë³‘í•© ë° ìµœì¢… ë¼ë²¨
    if "cluster" in df.columns:
        df = df.drop(columns=["cluster"])
    df = df.merge(df_cl[["cluster"]], left_index=True, right_index=True, how="left")
    df["cluster"] = df["cluster"].fillna(-2).astype(int)  # -2: SEEDë¡œë§Œ í• ë‹¹ëœ ë¬¸ì„œ

    cid_to_label = dict(zip(sum_df["cluster"], sum_df["auto_label"])) if not sum_df.empty else {}
    df["auto_label"] = df["cluster"].map(lambda c: cid_to_label.get(c, "ê¸°íƒ€/ë…¸ì´ì¦ˆ") if c != -2 else None)

    df["final_label"] = df.apply(lambda r: r["seed_category"] if pd.notna(r["seed_category"]) and r["seed_category"] else (r["auto_label"] if r["auto_label"] else "ê¸°íƒ€/ë…¸ì´ì¦ˆ"), axis=1)
    df["label_source"] = df.apply(lambda r: "SEED" if pd.notna(r["seed_category"]) and r["seed_category"] else ("AUTO" if r["auto_label"] else "NOISE"), axis=1)

    return df, sum_df

# ---------------------------
# íŒŒì´í”„ë¼ì¸: ì›ë³¸ JSON â†’ í† í”½ ìƒì„±
# ---------------------------
@st.cache_data(show_spinner=True)
def pipeline_from_raw(records: List[Dict[str, Any]], text_fields: List[str], id_field: Optional[str], model_name: str,
                      min_topic_size: int, umap_components: int, engine: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    rows = []
    for i, rec in enumerate(records):
        if not isinstance(rec, dict):
            continue
        flat = _flatten(rec)
        doc_id = flat.get(id_field) if id_field else None
        if doc_id is None:
            doc_id = f"doc_{i+1}"
        text = join_fields(flat, text_fields)
        rows.append({"id": doc_id, "text": text})

    df = pd.DataFrame(rows)
    df["text_clean"] = df["text"].map(clean_text)
    df = df[df["text_clean"].str.len() > 5].drop_duplicates(subset=["text_clean"]).reset_index(drop=True)
    if len(df) == 0:
        raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    emb = compute_embeddings(df["text_clean"].tolist(), model_name)

    if engine == "bertopic":
        if not _HAS_BERTOPIC:
            raise RuntimeError("BERTopic ë¯¸ì„¤ì¹˜: pip install bertopic")
        topic_model = BERTopic(language="multilingual", n_gram_range=(1,2), min_topic_size=max(5, int(min_topic_size)), calculate_probabilities=True, verbose=False)
        topics, probs = topic_model.fit_transform(df["text_clean"].tolist(), embeddings=emb)
        df["cluster"] = topics
        info = topic_model.get_topic_info()
        summary_records = []
        for _, row in info.iterrows():
            cid = int(row["Topic"])
            if cid == -1:
                continue
            topic_terms = topic_model.get_topic(cid) or []
            top_terms = [t for t, _ in topic_terms[:10]]
            auto_label = label_from_keywords(top_terms, max_tokens=3)
            idxs = df.index[df["cluster"] == cid].tolist()
            sample_texts = [df.loc[k, "text_clean"] for k in idxs[: min(3, len(idxs))]]
            summary_records.append({"cluster": cid, "size": int(row["Count"]), "auto_label": auto_label, "keywords": top_terms, "sample_texts": sample_texts})
        cid_to_label = {r["cluster"]: r["auto_label"] for r in summary_records}
        cid_to_terms = {r["cluster"]: r["keywords"] for r in summary_records}
        df["auto_label"] = df["cluster"].map(lambda c: cid_to_label.get(c, "ê¸°íƒ€/ë…¸ì´ì¦ˆ"))
        df["top_keywords"] = df["cluster"].map(lambda c: cid_to_terms.get(c, []))
        sum_df = pd.DataFrame(summary_records).sort_values("size", ascending=False).reset_index(drop=True)
    else:
        # custom: HDBSCAN + c-TF-IDF
        X = emb
        if umap_components > 0:
            X = run_umap(emb, n_components=umap_components)
        labels = run_hdbscan(X, min_cluster_size=min_topic_size)
        df["cluster"] = labels
        # êµ°ì§‘ big-doc
        by_cluster: Dict[int, List[int]] = {}
        for i, c in enumerate(labels):
            if c == -1:
                continue
            by_cluster.setdefault(int(c), []).append(i)
        summary_records = []
        cid_to_terms: Dict[int, List[str]] = {}
        cid_to_label: Dict[int, str] = {}
        if by_cluster:
            big_docs = [" ".join(df.loc[idxs, "text_clean"]) for _, idxs in sorted(by_cluster.items())]
            clusters_sorted = [cid for cid, _ in sorted(by_cluster.items())]
            top_terms_list, _ = c_tf_idf_keywords(big_docs, topn=10)
            for cid, terms in zip(clusters_sorted, top_terms_list):
                auto_label = label_from_keywords(terms, max_tokens=3)
                cid_to_terms[cid] = terms
                cid_to_label[cid] = auto_label
                idxs = by_cluster[cid]
                sample_texts = [df.loc[k, "text_clean"] for k in idxs[: min(3, len(idxs))]]
                summary_records.append({"cluster": cid, "size": len(idxs), "auto_label": auto_label, "keywords": terms, "sample_texts": sample_texts})
        df["auto_label"] = df["cluster"].map(lambda c: cid_to_label.get(int(c), "ê¸°íƒ€/ë…¸ì´ì¦ˆ") if c != -1 else "ê¸°íƒ€/ë…¸ì´ì¦ˆ")
        df["top_keywords"] = df["cluster"].map(lambda c: cid_to_terms.get(int(c), []) if c != -1 else [])
        sum_df = pd.DataFrame(summary_records).sort_values("size", ascending=False).reset_index(drop=True)

    return df, sum_df


# ---------------------------
# ì‹œê°í™” ìœ í‹¸
# ---------------------------
def plot_cluster_sizes(sum_df: pd.DataFrame, topn: int = 15):
    if sum_df.empty:
        st.info("ìš”ì•½ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    top = sum_df.sort_values("size", ascending=False).head(topn)
    fig = px.bar(top, x="size", y=top.apply(lambda r: f"#{int(r['cluster'])}  {r['auto_label']}", axis=1), orientation='h', title="Cluster Sizes (Top-N)")
    st.plotly_chart(fig, use_container_width=True)


def plot_umap_scatter(docs_df: pd.DataFrame, model_name: str, max_points: int = 5000, n_components: int = 2):
    if len(docs_df) == 0:
        st.warning("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    texts = docs_df.get("text", docs_df.get("text_clean")).astype(str).tolist()
    # ìƒ˜í”Œë§(ì„±ëŠ¥ ë³´í˜¸)
    if len(texts) > max_points:
        docs_df = docs_df.sample(max_points, random_state=42).reset_index(drop=True)
        texts = docs_df.get("text", docs_df.get("text_clean")).astype(str).tolist()
        st.caption(f"UMAP ì‚°ì ë„ëŠ” ì„±ëŠ¥ì„ ìœ„í•´ {max_points}ê±´ ìƒ˜í”Œë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
    emb = compute_embeddings(texts, model_name)
    if not _HAS_UMAP:
        st.error("umap-learn ë¯¸ì„¤ì¹˜: pip install umap-learn")
        return
    coords = run_umap(emb, n_components=n_components)
    docs_df = docs_df.copy()
    docs_df["x"] = coords[:, 0]
    docs_df["y"] = coords[:, 1]
    fig = px.scatter(
        docs_df,
        x="x", y="y",
        color=docs_df["cluster"].astype(str),
        hover_data={"id": True, "auto_label": True, "text_clean": True, "x": False, "y": False},
        title="UMAP 2D Scatter by Cluster",
    )
    st.plotly_chart(fig, use_container_width=True)


def plot_cluster_keywords(sum_df: pd.DataFrame, cluster_id: int, topn: int = 10):
    row = sum_df[sum_df["cluster"] == cluster_id]
    if row.empty:
        st.info("í•´ë‹¹ êµ°ì§‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    kws = (row.iloc[0]["keywords"] or [])[:topn]
    if not kws:
        st.info("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    dfk = pd.DataFrame({"keyword": kws, "rank": list(range(len(kws), 0, -1))})
    fig = px.bar(dfk, x="rank", y="keyword", orientation='h', title=f"Top Keywords of Cluster #{cluster_id}")
    st.plotly_chart(fig, use_container_width=True)


def plot_timeline(docs_df: pd.DataFrame, time_field: str, top_clusters: List[int], freq: str = "W-MON"):
    if time_field not in docs_df.columns:
        st.info(f"ë¬¸ì„œì— '{time_field}' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    ts = pd.to_datetime(docs_df[time_field], errors="coerce")
    if ts.notna().sum() == 0:
        st.info("ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    tmp = docs_df.copy()
    tmp["_week"] = ts.dt.to_period(freq).astype(str)
    tmp = tmp[tmp["cluster"].isin(top_clusters)]
    pv = tmp.pivot_table(index="_week", columns="cluster", values="id", aggfunc="count").fillna(0).sort_index()
    pv = pv.reset_index().melt(id_vars="_week", var_name="cluster", value_name="count")
    pv["cluster"] = pv["cluster"].astype(int)
    fig = px.line(pv, x="_week", y="count", color="cluster", markers=True, title="Weekly Trends by Cluster (Top-N)")
    fig.update_layout(xaxis_title="Week", yaxis_title="Count")
    st.plotly_chart(fig, use_container_width=True)


# ---------------------------
# Streamlit UI
# ---------------------------
st.set_page_config(page_title="VOC Topic Explorer", layout="wide")
st.title("ğŸ“Š VOC Topic Explorer (JSON)")
st.caption("ì¹´í…Œê³ ë¦¬ ë¯¸ì§€ì • ë¹„ì§€ë„ í† í”½í™” + ëŒ€ì‹œë³´ë“œ ì‹œê°í™”")

with st.sidebar:
    st.header("ëª¨ë“œ ì„ íƒ")
    mode = st.radio("ì…ë ¥ ëª¨ë“œ", ["ì›ë³¸ JSONì—ì„œ ìƒì„±", "ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ ì‹œê°í™”"], index=0)
    st.divider()
    st.header("ê³µí†µ ì„¤ì •")
    model_name = st.text_input("Sentence-Transformer ëª¨ë¸", value="paraphrase-multilingual-MiniLM-L12-v2")
    topn_clusters = st.slider("Cluster Top-N (ì°¨íŠ¸)", 5, 50, 15)
    topn_keywords = st.slider("Keywords Top-K", 3, 20, 10)
    st.divider()
    st.markdown("**ë‹¤ìš´ë¡œë“œ ì˜µì…˜**")
    want_docs_csv = st.checkbox("ë¬¸ì„œë³„ ê²°ê³¼ CSV ë²„íŠ¼ í‘œì‹œ", True)
    want_sum_csv = st.checkbox("ìš”ì•½ CSV ë²„íŠ¼ í‘œì‹œ", True)
    
    # í•˜ì´ë¸Œë¦¬ë“œ: Seed ì˜µì…˜
    st.divider()
    st.header("ì‚¬ì „ ì •ì˜ ì¹´í…Œê³ ë¦¬(ì„ íƒ)")
    seed_file = st.file_uploader("Seed íŒŒì¼ (CSV ë˜ëŠ” JSON)", type=["csv","json"], accept_multiple_files=False)
    seed_text = st.text_area("ë˜ëŠ” JSON ë°°ì—´ ì§ì ‘ ì…ë ¥", height=140, help='ì˜ˆ: [{"category":"ê²°ì œ","type":"keyword","value":"ì´ì¤‘ ê²°ì œ"}]')
    use_kw = st.checkbox("í‚¤ì›Œë“œ/ì •ê·œì‹ ë§¤ì¹­ ì‚¬ìš©", True)
    use_ex = st.checkbox("ì˜ˆì‹œë¬¸ì¥ ì„ë² ë”© ë§¤ì¹­ ì‚¬ìš©", True)
    ex_thresh = st.slider("ì˜ˆì‹œë¬¸ì¥ ì„ê³„ (cos sim)", 0.30, 0.90, 0.55, 0.01)
    exclude_seeded = st.checkbox("SEEDë¡œ í• ë‹¹ëœ ë¬¸ì„œëŠ” í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ì œì™¸", True)


# ìƒíƒœ ë³´ê´€
DOCS_DF: Optional[pd.DataFrame] = None
SUM_DF: Optional[pd.DataFrame] = None

if mode == "ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ ì‹œê°í™”":
    st.subheader("ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ ì—…ë¡œë“œ")
    c1, c2 = st.columns(2)
    with c1:
        docs_file = st.file_uploader("ë¬¸ì„œë³„ JSONL (voc_topics_docs.jsonl)", type=["jsonl","txt"], accept_multiple_files=False)
    with c2:
        sum_file = st.file_uploader("ìš”ì•½ JSON (voc_topics_summary.json)", type=["json","txt"], accept_multiple_files=False)

    if docs_file and sum_file:
        # ë¡œë“œ
        docs_rows = []
        for line in io.StringIO(docs_file.getvalue().decode("utf-8", errors="ignore")).read().splitlines():
            line = line.strip()
            if not line:
                continue
            docs_rows.append(json.loads(line))
        DOCS_DF = pd.DataFrame(docs_rows)
        SUM_DF = pd.DataFrame(json.loads(sum_file.getvalue().decode("utf-8", errors="ignore")))

elif mode == "ì›ë³¸ JSONì—ì„œ ìƒì„±":
    st.subheader("ì›ë³¸ JSON/NDJSON ì—…ë¡œë“œ â†’ ìë™ í† í”½ ìƒì„±")
    raw_file = st.file_uploader("JSON/NDJSON íŒŒì¼", type=["json","ndjson","jsonl","txt"], accept_multiple_files=False)
    engine = st.selectbox("ì—”ì§„", ["custom(HDBSCAN)", "bertopic"], index=0)
    engine_key = "bertopic" if engine == "bertopic" else "custom"
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.metric("ë¬¸ì„œ ìˆ˜", len(DOCS_DF))
    with c2:
        st.metric("êµ°ì§‘ ìˆ˜(ë…¸ì´ì¦ˆ ì œì™¸)", int((SUM_DF['cluster'] != -1).sum()))
    with c3:
        st.metric("ìµœëŒ€ êµ°ì§‘ í¬ê¸°", int(SUM_DF['size'].max() if not SUM_DF.empty else 0))
    with c4:
        seed_cov = float((DOCS_DF['label_source'] == 'SEED').mean()*100.0) if 'label_source' in DOCS_DF.columns else 0.0
        st.metric("SEED ì ìš© ë¹„ìœ¨", f"{seed_cov:.1f}%")")

    text_fields: List[str] = []
    if raw_file is not None:
        try:
            records = _read_json_or_ndjson(raw_file)
            st.caption(f"ë ˆì½”ë“œ ìˆ˜: {len(records)}")
            if len(records) == 0:
                st.stop()
            keys = discover_keys(records)
            st.markdown("**í…ìŠ¤íŠ¸ í•„ë“œ ì„ íƒ(ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)**")
            text_fields = st.multiselect("ì í‘œê¸° í‚¤ë¡œ ì„ íƒ", options=keys, default=[k for k in keys if k.lower() in ("title","body","message","msg.title","msg.detail")])
            # Seed êµ¬ì„±
            seeds = load_seeds_from_inputs(seed_file, seed_text)
            if seeds:
                st.success(f"Seed ì¹´í…Œê³ ë¦¬ {len(seeds)}ê°œ ë¡œë“œ ì™„ë£Œ")
            run_btn = st.button("í† í”½ ìƒì„± ì‹¤í–‰", type="primary")
            if run_btn:
                with st.spinner("ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘..."):
                    DOCS_DF, SUM_DF = pipeline_from_raw_hybrid(
                        records, text_fields, id_field_in or None, model_name,
                        int(min_topic_size), int(umap_components), engine_key,
                        seeds=seeds if seeds else None,
                        use_kw_regex=use_kw,
                        use_examples=use_ex,
                        example_thresh=float(ex_thresh),
                        exclude_seeded_from_clustering=bool(exclude_seeded),
                    )
        except Exception as e:
            st.error(f"ë¡œë”©/ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ---------------------------
# ì‹œê°í™”/íƒìƒ‰
# ---------------------------
if DOCS_DF is not None and SUM_DF is not None:
    st.success(f"ë¬¸ì„œ {len(DOCS_DF)}ê±´, êµ°ì§‘ {SUM_DF['cluster'].nunique()}ê°œ")

    # ìƒë‹¨ KPI
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("ë¬¸ì„œ ìˆ˜", len(DOCS_DF))
    with c2:
        st.metric("êµ°ì§‘ ìˆ˜(ë…¸ì´ì¦ˆ ì œì™¸)", int((SUM_DF['cluster'] != -1).sum()))
    with c3:
        st.metric("ìµœëŒ€ êµ°ì§‘ í¬ê¸°", int(SUM_DF['size'].max() if not SUM_DF.empty else 0))

    st.subheader("í´ëŸ¬ìŠ¤í„° í¬ê¸° (Top-N)")
    plot_cluster_sizes(SUM_DF, topn=topn_clusters)

    st.subheader("UMAP 2D ì‚°ì ë„")
    with st.expander("ë³´ê¸°/ì„¤ì •", expanded=True):
        max_points = st.slider("ì‚°ì ë„ ìƒ˜í”Œ ìµœëŒ€ ìˆ˜", 1000, 20000, 5000, 1000)
        umap_dim = st.radio("UMAP ì°¨ì›", [2, 3], index=0)
        plot_btn = st.button("UMAP ì¬ê³„ì‚°", type="secondary")
        if plot_btn:
            plot_umap_scatter(DOCS_DF, model_name=model_name, max_points=max_points, n_components=int(umap_dim))
        else:
            # ìµœì´ˆ 1íšŒ ìë™ í‘œì‹œ
            plot_umap_scatter(DOCS_DF, model_name=model_name, max_points=max_points, n_components=int(umap_dim))

    st.subheader("êµ°ì§‘ ìƒì„¸")
    # êµ°ì§‘ ì„ íƒ
    default_cid = int(SUM_DF.sort_values("size", ascending=False).iloc[0]["cluster"]) if not SUM_DF.empty else -1
    cid = st.number_input("êµ°ì§‘ ID", value=default_cid, step=1)
    plot_cluster_keywords(SUM_DF, int(cid), topn=topn_keywords)

    # ìƒ˜í”Œ ë¬¸ì„œ í‘œì‹œ
    st.markdown("**ìƒ˜í”Œ ë¬¸ì¥ (ìµœëŒ€ 20ê±´)**")
    samples = DOCS_DF[DOCS_DF["cluster"] == int(cid)].head(20)
    if samples.empty:
        st.info("í•´ë‹¹ êµ°ì§‘ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        show_cols = [c for c in ["id","final_label","label_source","seed_method","seed_score","cluster","auto_label","top_keywords","text_clean"] if c in samples.columns]
        st.dataframe(samples[show_cols], use_container_width=True, hide_index=True)

    # ì‹œê³„ì—´(ì˜µì…˜)
    with st.expander("ì‹œê³„ì—´ ì¶”ì´ (ì˜µì…˜)"):
        time_field = st.text_input("ë‚ ì§œ/ì‹œê°„ ì¹¼ëŸ¼ëª…(ì˜ˆ: created_at)")
        topk = st.slider("Top-N êµ°ì§‘(ì‹œê³„ì—´)", 3, 20, 6)
        if time_field:
            top_clusters = SUM_DF.sort_values("size", ascending=False).head(topk)["cluster"].astype(int).tolist()
            plot_timeline(DOCS_DF, time_field, top_clusters)

    # ë‹¤ìš´ë¡œë“œ
    st.subheader("ë‹¤ìš´ë¡œë“œ")
    if want_docs_csv:
        csv = DOCS_DF.to_csv(index=False).encode("utf-8")
        st.download_button("ë¬¸ì„œë³„ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", csv, file_name="voc_docs.csv", mime="text/csv")
    if want_sum_csv:
        csv2 = SUM_DF.to_csv(index=False).encode("utf-8")
        st.download_button("êµ°ì§‘ ìš”ì•½ CSV ë‹¤ìš´ë¡œë“œ", csv2, file_name="voc_summary.csv", mime="text/csv")
else:
    st.info("ì¢Œì¸¡ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê±°ë‚˜, ë¯¸ë¦¬ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
