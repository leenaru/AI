ì•„ë˜ëŠ” **MobileCLIPì— LoRAë¥¼ ì ìš©í•˜ê³  Contrastive Lossë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ì „ì²´ ì½”ë“œ**ì…ë‹ˆë‹¤.  
ë˜í•œ, **MobileCLIPì˜ Text EncoderëŠ” ë¯¸ë¦¬ ì „ì²´ í´ë˜ìŠ¤ë¥¼ ë°›ì•„ ì„ë² ë”©ì„ í•´ì•¼ í•˜ë¯€ë¡œ**, ì´ë¥¼ ë°˜ì˜í•˜ì—¬ **Text Encoderë¥¼ ì‚¬ì „ì— ì„ë² ë”©í•˜ê³  ì¬ì‚¬ìš©í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

---

### **ğŸ”¥ MobileCLIP + LoRA + Contrastive Loss ì ìš© ì½”ë“œ (Text Encoder ì‚¬ì „ ì„ë² ë”© í¬í•¨)**
```python
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import CLIPProcessor, CLIPModel
from peft import get_peft_model, LoraConfig, TaskType

# 1. MobileCLIP ëª¨ë¸ ë¡œë“œ
model_name = "apple/ml-mobileclip"  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ MobileCLIP ëª¨ë¸ë¡œ ë³€ê²½ í•„ìš”
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 2. LoRA ì„¤ì • ë° ì ìš©
lora_config = LoraConfig(
    task_type=TaskType.FEATURE_EXTRACTION,
    r=8,  # LoRA ë­í¬
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,
    target_modules=["text_model.encoder.layers", "vision_model.encoder.layers"],  # ì ìš© ëŒ€ìƒ
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 3. ì „ì²´ í´ë˜ìŠ¤ í…ìŠ¤íŠ¸ ì„ë² ë”© (Text Encoder ì‚¬ì „ ê³„ì‚°)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

class_labels = ["a cat", "a dog", "a car", "a tree"]  # ì‹¤ì œ ì‚¬ìš©í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸
text_inputs = processor(text=class_labels, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    text_outputs = model.get_text_features(**text_inputs)  # text_encoderë¥¼ ì‚¬ì „ì— ì²˜ë¦¬
    text_features = F.normalize(text_outputs, dim=-1)  # ì •ê·œí™”

print("Text features shape:", text_features.shape)  # (num_classes, embedding_dim)

# 4. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì •ì˜ (ì‚¬ìš©ì ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •)
class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, processor, num_samples=100):
        self.processor = processor
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        image = torch.randn(3, 224, 224)  # ê°€ì§œ ì´ë¯¸ì§€ ë°ì´í„° (ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´ í•„ìš”)
        label = torch.randint(0, len(class_labels), (1,)).item()  # ëœë¤ ë ˆì´ë¸” í• ë‹¹
        return {"image": image, "label": label}

train_dataset = DummyDataset(processor)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# 5. ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµ ì„¤ì •
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# 6. Contrastive Lossë¥¼ ìˆ˜ë™ ê³„ì‚°í•˜ë©° ëª¨ë¸ í•™ìŠµ
for epoch in range(3):  # 3 Epoch ì˜ˆì œ
    for batch in train_dataloader:
        images = batch["image"].to(device)
        labels = batch["label"].to(device)

        # Processorë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì…ë ¥ ì¤€ë¹„
        image_inputs = processor(images=images, return_tensors="pt").to(device)

        # Forward Pass (ì´ë¯¸ì§€ ì„ë² ë”©ë§Œ ì¶”ì¶œ)
        outputs = model.get_image_features(**image_inputs)
        image_features = F.normalize(outputs, dim=-1)  # ì´ë¯¸ì§€ ì„ë² ë”© ì •ê·œí™”

        # Similarity matrix ê³„ì‚°
        logit_scale = model.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.T  # (batch_size, num_classes)

        # Cross-Entropy Loss ê³„ì‚°
        loss_img = F.cross_entropy(logits_per_image, labels.long())  # labelsê°€ Long íƒ€ì…ì´ì–´ì•¼ í•¨

        # Backward & Optimization
        optimizer.zero_grad()
        loss_img.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss_img.item():.4f}")

# 7. í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("mobileclip_lora")
processor.save_pretrained("mobileclip_lora")

# 8. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸
model = CLIPModel.from_pretrained("mobileclip_lora")
processor = CLIPProcessor.from_pretrained("mobileclip_lora")

test_image = torch.randn(3, 224, 224)  # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€
image_inputs = processor(images=[test_image], return_tensors="pt").to(device)

with torch.no_grad():
    image_features = model.get_image_features(**image_inputs)
    image_features = F.normalize(image_features, dim=-1)
    similarity_scores = image_features @ text_features.T  # ìœ ì‚¬ë„ ê³„ì‚°

print("Similarity scores:", similarity_scores)
```

---

## **ğŸ›  ì£¼ìš” ê°œì„  ì‚¬í•­**
### âœ… **1. MobileCLIPì˜ Text Encoderë¥¼ ë¯¸ë¦¬ ê³„ì‚°**
- `model.get_text_features(**text_inputs)`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ **í•œ ë²ˆë§Œ ê³„ì‚° í›„ ì €ì¥**í•©ë‹ˆë‹¤.
- í•™ìŠµ ë£¨í”„ì—ì„œëŠ” **ë¯¸ë¦¬ ê³„ì‚°ëœ `text_features`ë¥¼ ì¬ì‚¬ìš©**í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

### âœ… **2. Contrastive Lossë¥¼ ìˆ˜ë™ ê³„ì‚°**
- `logits_per_image = logit_scale * image_features @ text_features.T` ë°©ì‹ìœ¼ë¡œ `logits_per_image`ë¥¼ ê³„ì‚°
- `F.cross_entropy(logits_per_image, labels.long())`ë¥¼ ì‚¬ìš©í•˜ì—¬ cross-entropy loss ê³„ì‚°

### âœ… **3. `F.normalize()` ì ìš©í•˜ì—¬ ì„ë² ë”© ì •ê·œí™”**
- `image_features`ì™€ `text_features`ë¥¼ `F.normalize(..., dim=-1)`ë¡œ ì •ê·œí™”í•˜ì—¬ CLIP ìŠ¤íƒ€ì¼ì˜ similarity ê³„ì‚°ì„ ìœ ì§€

### âœ… **4. `labels`ë¥¼ `LongTensor`ë¡œ ë³€í™˜**
- `F.cross_entropy()`ì—ì„œ `labels`ëŠ” `LongTensor`ì—¬ì•¼ í•˜ë¯€ë¡œ `labels.long()`ì„ ì ìš©í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€

---

## **ğŸš€ ê¸°ëŒ€ íš¨ê³¼**
âœ… **Text EncoderëŠ” ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì¬ì‚¬ìš©í•˜ë¯€ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ**  
âœ… **Contrastive Loss ìˆ˜ë™ ê³„ì‚°ì„ ì ìš©í•˜ì—¬ `loss.backward()` ì˜¤ë¥˜ ì—†ì´ í•™ìŠµ ê°€ëŠ¥**  
âœ… **MobileCLIPì— LoRA ì ìš©í•˜ì—¬ ê°€ë³ê²Œ í›ˆë ¨ ê°€ëŠ¥**  
âœ… **í•™ìŠµ í›„ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ì¶”ë¡  ê°€ëŠ¥**

ì´ì œ ì´ ì½”ë“œë¡œ MobileCLIPì„ LoRAì™€ í•¨ê»˜ í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”! ğŸš€ğŸ˜Š

---
---
---
ì•„ë˜ëŠ” `tqdm`ì„ ì‚¬ìš©í•œ **MobileCLIP + LoRA + Contrastive Loss í•™ìŠµ ê³¼ì •**ì„ **ì‹œê°í™”**í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  

### âœ… **ì¶”ê°€ëœ ì‹œê°í™” ê¸°ëŠ¥**
- `matplotlib`ì„ ì‚¬ìš©í•˜ì—¬ **Loss ë³€í™” ê·¸ë˜í”„ ì¶œë ¥**
- `seaborn`ì„ í™œìš©í•´ **í•™ìŠµ ì•ˆì •ì„±ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„**
- `similarity_scores` íˆíŠ¸ë§µ ì‹œê°í™”  

---

## **ğŸ”¥ MobileCLIP + LoRA + Contrastive Loss í•™ìŠµ + ì‹œê°í™” ì½”ë“œ**
```python
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import CLIPProcessor, CLIPModel
from peft import get_peft_model, LoraConfig, TaskType
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# MobileCLIP ëª¨ë¸ ë¡œë“œ
model_name = "apple/ml-mobileclip"  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ MobileCLIP ëª¨ë¸ë¡œ ë³€ê²½ í•„ìš”
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# LoRA ì„¤ì • ë° ì ìš©
lora_config = LoraConfig(
    task_type=TaskType.FEATURE_EXTRACTION,
    r=8,  
    lora_alpha=32,  
    lora_dropout=0.1,
    target_modules=["text_model.encoder.layers", "vision_model.encoder.layers"],  
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
model.train()

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# í…ìŠ¤íŠ¸ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
class_labels = ["a cat", "a dog", "a car", "a tree"]
text_inputs = processor(text=class_labels, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    text_outputs = model.get_text_features(**text_inputs)  
    text_features = F.normalize(text_outputs, dim=-1)  

text_features = text_features.clone().detach().requires_grad_(True)

# ë°ì´í„°ì…‹ & ë°ì´í„°ë¡œë” ì •ì˜
class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, processor, num_samples=100):
        self.processor = processor
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        image = torch.randn(3, 224, 224)  
        label = torch.randint(0, len(class_labels), (1,)).item()
        return {"image": image, "label": label}

train_dataset = DummyDataset(processor)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# Loss ê¸°ë¡ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
loss_history = []

# í•™ìŠµ ë£¨í”„
num_epochs = 3
for epoch in range(num_epochs):
    epoch_loss = 0  
    progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}", leave=True)

    for batch in progress_bar:
        images = batch["image"].to(device)
        labels = batch["label"].to(device)

        # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        image_inputs = processor(images=images, return_tensors="pt").to(device)
        image_features = model.get_image_features(**image_inputs)

        # Normalize features
        image_features = F.normalize(image_features, dim=-1)
        image_features.requires_grad_(True)

        # Similarity matrix ê³„ì‚°
        logit_scale = model.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.T

        # labels í¬ê¸° ì¡°ì •
        if labels.dim() > 1:
            labels = labels.squeeze(1)
        if labels.dim() > 1:
            labels = labels.argmax(dim=-1)  

        # Cross-entropy loss ê³„ì‚°
        loss_img = F.cross_entropy(logits_per_image, labels.long())
        epoch_loss += loss_img.item()

        # Loss ì €ì¥ (ì‹œê°í™”ë¥¼ ìœ„í•´)
        loss_history.append(loss_img.item())

        # Backward & Optimization
        optimizer.zero_grad()
        loss_img.backward()
        optimizer.step()

        # tqdmì— í˜„ì¬ Loss í‘œì‹œ
        progress_bar.set_postfix(loss=f"{loss_img.item():.4f}")

    print(f"Epoch {epoch+1} completed. Avg Loss: {epoch_loss / len(train_dataloader):.4f}")

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("mobileclip_lora")
processor.save_pretrained("mobileclip_lora")

# ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
model = CLIPModel.from_pretrained("mobileclip_lora")
processor = CLIPProcessor.from_pretrained("mobileclip_lora")

test_image = torch.randn(3, 224, 224)
image_inputs = processor(images=[test_image], return_tensors="pt").to(device)

with torch.no_grad():
    image_features = model.get_image_features(**image_inputs)
    image_features = F.normalize(image_features, dim=-1)
    similarity_scores = image_features @ text_features.T  

print("Similarity scores:", similarity_scores)

# ğŸ“Š Loss ë³€í™” ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(loss_history, label="Training Loss", color='b')
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title("Training Loss Over Time")
plt.legend()
plt.show()

# ğŸ”¥ Similarity Score íˆíŠ¸ë§µ ì‹œê°í™”
plt.figure(figsize=(6, 5))
sns.heatmap(similarity_scores.cpu().numpy(), annot=True, fmt=".2f", cmap="coolwarm", xticklabels=class_labels, yticklabels=["Test Image"])
plt.title("Image-Text Similarity Scores")
plt.xlabel("Text Labels")
plt.ylabel("Image")
plt.show()
```

---

## **ğŸš€ ì¶”ê°€ëœ ì‹œê°í™”**
### **ğŸ“Š 1. í•™ìŠµ Loss ë³€í™” ê·¸ë˜í”„**
```python
plt.figure(figsize=(10, 5))
plt.plot(loss_history, label="Training Loss", color='b')
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title("Training Loss Over Time")
plt.legend()
plt.show()
```
- **Lossì˜ ê°ì†Œ ì—¬ë¶€ë¥¼ í™•ì¸**í•˜ì—¬ ëª¨ë¸ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì²´í¬ ê°€ëŠ¥  
- `loss_history` ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  Iteration ë™ì•ˆ ê¸°ë¡ëœ `loss`ë¥¼ ì‹œê°í™”  

---

### **ğŸ”¥ 2. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ íˆíŠ¸ë§µ**
```python
plt.figure(figsize=(6, 5))
sns.heatmap(similarity_scores.cpu().numpy(), annot=True, fmt=".2f", cmap="coolwarm", xticklabels=class_labels, yticklabels=["Test Image"])
plt.title("Image-Text Similarity Scores")
plt.xlabel("Text Labels")
plt.ylabel("Image")
plt.show()
```
- í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ **íˆíŠ¸ë§µ(Heatmap)ìœ¼ë¡œ ì‹œê°í™”**  
- `similarity_scores`ë¥¼ `seaborn.heatmap()`ì„ ì‚¬ìš©í•´ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„  
- ë°ì€ ìƒ‰ìƒì€ ë†’ì€ ìœ ì‚¬ë„ë¥¼, ì–´ë‘ìš´ ìƒ‰ìƒì€ ë‚®ì€ ìœ ì‚¬ë„ë¥¼ ë‚˜íƒ€ëƒ„  

---

## **ğŸš€ ê¸°ëŒ€ íš¨ê³¼**
âœ… `tqdm`ì„ í™œìš©í•´ í•™ìŠµ ì§„í–‰ë¥ ì„ ì§ê´€ì ìœ¼ë¡œ í™•ì¸  
âœ… `matplotlib`ì„ ì´ìš©í•œ **Loss ë³€í™” ì‹œê°í™”**ë¡œ í•™ìŠµ í’ˆì§ˆ í™•ì¸  
âœ… `seaborn`ì„ í™œìš©í•œ **ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ íˆíŠ¸ë§µ**ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€  
âœ… **MobileCLIP + LoRA ëª¨ë¸ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ì‹œê°í™”í•˜ê³  ê²€ì¦ ê°€ëŠ¥**  

ì´ì œ í•™ìŠµ ê³¼ì •ì„ **ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§**í•˜ê³ , í•™ìŠµ ê²°ê³¼ë¥¼ **ì‹œê°ì ìœ¼ë¡œ ë¶„ì„**í•  ìˆ˜ ìˆì–´ìš”! ğŸš€ğŸ”¥
