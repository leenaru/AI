ì•„ë˜ëŠ” **MobileCLIPì— LoRAë¥¼ ì ìš©í•˜ê³  Contrastive Lossë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ì „ì²´ ì½”ë“œ**ì…ë‹ˆë‹¤.  
ë˜í•œ, **MobileCLIPì˜ Text EncoderëŠ” ë¯¸ë¦¬ ì „ì²´ í´ë˜ìŠ¤ë¥¼ ë°›ì•„ ì„ë² ë”©ì„ í•´ì•¼ í•˜ë¯€ë¡œ**, ì´ë¥¼ ë°˜ì˜í•˜ì—¬ **Text Encoderë¥¼ ì‚¬ì „ì— ì„ë² ë”©í•˜ê³  ì¬ì‚¬ìš©í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

---

### **ğŸ”¥ MobileCLIP + LoRA + Contrastive Loss ì ìš© ì½”ë“œ (Text Encoder ì‚¬ì „ ì„ë² ë”© í¬í•¨)**
```python
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import CLIPProcessor, CLIPModel
from peft import get_peft_model, LoraConfig, TaskType

# 1. MobileCLIP ëª¨ë¸ ë¡œë“œ
model_name = "apple/ml-mobileclip"  # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ MobileCLIP ëª¨ë¸ë¡œ ë³€ê²½ í•„ìš”
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 2. LoRA ì„¤ì • ë° ì ìš©
lora_config = LoraConfig(
    task_type=TaskType.FEATURE_EXTRACTION,
    r=8,  # LoRA ë­í¬
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,
    target_modules=["text_model.encoder.layers", "vision_model.encoder.layers"],  # ì ìš© ëŒ€ìƒ
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 3. ì „ì²´ í´ë˜ìŠ¤ í…ìŠ¤íŠ¸ ì„ë² ë”© (Text Encoder ì‚¬ì „ ê³„ì‚°)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

class_labels = ["a cat", "a dog", "a car", "a tree"]  # ì‹¤ì œ ì‚¬ìš©í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸
text_inputs = processor(text=class_labels, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    text_outputs = model.get_text_features(**text_inputs)  # text_encoderë¥¼ ì‚¬ì „ì— ì²˜ë¦¬
    text_features = F.normalize(text_outputs, dim=-1)  # ì •ê·œí™”

print("Text features shape:", text_features.shape)  # (num_classes, embedding_dim)

# 4. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì •ì˜ (ì‚¬ìš©ì ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •)
class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, processor, num_samples=100):
        self.processor = processor
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        image = torch.randn(3, 224, 224)  # ê°€ì§œ ì´ë¯¸ì§€ ë°ì´í„° (ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´ í•„ìš”)
        label = torch.randint(0, len(class_labels), (1,)).item()  # ëœë¤ ë ˆì´ë¸” í• ë‹¹
        return {"image": image, "label": label}

train_dataset = DummyDataset(processor)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# 5. ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµ ì„¤ì •
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# 6. Contrastive Lossë¥¼ ìˆ˜ë™ ê³„ì‚°í•˜ë©° ëª¨ë¸ í•™ìŠµ
for epoch in range(3):  # 3 Epoch ì˜ˆì œ
    for batch in train_dataloader:
        images = batch["image"].to(device)
        labels = batch["label"].to(device)

        # Processorë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì…ë ¥ ì¤€ë¹„
        image_inputs = processor(images=images, return_tensors="pt").to(device)

        # Forward Pass (ì´ë¯¸ì§€ ì„ë² ë”©ë§Œ ì¶”ì¶œ)
        outputs = model.get_image_features(**image_inputs)
        image_features = F.normalize(outputs, dim=-1)  # ì´ë¯¸ì§€ ì„ë² ë”© ì •ê·œí™”

        # Similarity matrix ê³„ì‚°
        logit_scale = model.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.T  # (batch_size, num_classes)

        # Cross-Entropy Loss ê³„ì‚°
        loss_img = F.cross_entropy(logits_per_image, labels.long())  # labelsê°€ Long íƒ€ì…ì´ì–´ì•¼ í•¨

        # Backward & Optimization
        optimizer.zero_grad()
        loss_img.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss_img.item():.4f}")

# 7. í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("mobileclip_lora")
processor.save_pretrained("mobileclip_lora")

# 8. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸
model = CLIPModel.from_pretrained("mobileclip_lora")
processor = CLIPProcessor.from_pretrained("mobileclip_lora")

test_image = torch.randn(3, 224, 224)  # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€
image_inputs = processor(images=[test_image], return_tensors="pt").to(device)

with torch.no_grad():
    image_features = model.get_image_features(**image_inputs)
    image_features = F.normalize(image_features, dim=-1)
    similarity_scores = image_features @ text_features.T  # ìœ ì‚¬ë„ ê³„ì‚°

print("Similarity scores:", similarity_scores)
```

---

## **ğŸ›  ì£¼ìš” ê°œì„  ì‚¬í•­**
### âœ… **1. MobileCLIPì˜ Text Encoderë¥¼ ë¯¸ë¦¬ ê³„ì‚°**
- `model.get_text_features(**text_inputs)`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ **í•œ ë²ˆë§Œ ê³„ì‚° í›„ ì €ì¥**í•©ë‹ˆë‹¤.
- í•™ìŠµ ë£¨í”„ì—ì„œëŠ” **ë¯¸ë¦¬ ê³„ì‚°ëœ `text_features`ë¥¼ ì¬ì‚¬ìš©**í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

### âœ… **2. Contrastive Lossë¥¼ ìˆ˜ë™ ê³„ì‚°**
- `logits_per_image = logit_scale * image_features @ text_features.T` ë°©ì‹ìœ¼ë¡œ `logits_per_image`ë¥¼ ê³„ì‚°
- `F.cross_entropy(logits_per_image, labels.long())`ë¥¼ ì‚¬ìš©í•˜ì—¬ cross-entropy loss ê³„ì‚°

### âœ… **3. `F.normalize()` ì ìš©í•˜ì—¬ ì„ë² ë”© ì •ê·œí™”**
- `image_features`ì™€ `text_features`ë¥¼ `F.normalize(..., dim=-1)`ë¡œ ì •ê·œí™”í•˜ì—¬ CLIP ìŠ¤íƒ€ì¼ì˜ similarity ê³„ì‚°ì„ ìœ ì§€

### âœ… **4. `labels`ë¥¼ `LongTensor`ë¡œ ë³€í™˜**
- `F.cross_entropy()`ì—ì„œ `labels`ëŠ” `LongTensor`ì—¬ì•¼ í•˜ë¯€ë¡œ `labels.long()`ì„ ì ìš©í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€

---

## **ğŸš€ ê¸°ëŒ€ íš¨ê³¼**
âœ… **Text EncoderëŠ” ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ì¬ì‚¬ìš©í•˜ë¯€ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ**  
âœ… **Contrastive Loss ìˆ˜ë™ ê³„ì‚°ì„ ì ìš©í•˜ì—¬ `loss.backward()` ì˜¤ë¥˜ ì—†ì´ í•™ìŠµ ê°€ëŠ¥**  
âœ… **MobileCLIPì— LoRA ì ìš©í•˜ì—¬ ê°€ë³ê²Œ í›ˆë ¨ ê°€ëŠ¥**  
âœ… **í•™ìŠµ í›„ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ì¶”ë¡  ê°€ëŠ¥**

ì´ì œ ì´ ì½”ë“œë¡œ MobileCLIPì„ LoRAì™€ í•¨ê»˜ í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”! ğŸš€ğŸ˜Š
