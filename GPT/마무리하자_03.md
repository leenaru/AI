MobileCLIP 모델의 이미지-텍스트 멀티모달 입력을 고려한 전체 변환 코드입니다:

```python
import torch
import json
from pathlib import Path
import coremltools as ct
from mobileclip.modules.common.mobileone import reparameterize_model
from peft import PeftModel, get_peft_model, LoraConfig
from mobileclip import create_model

class ModelWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, image, text_tokens):
        # MobileCLIP의 forward 패턴에 맞춤
        outputs = self.model(image, text_tokens)
        if isinstance(outputs, dict):
            return outputs
        elif isinstance(outputs, (tuple, list)):
            return {
                "image_embeddings": outputs[0],
                "text_embeddings": outputs[1]
            }
        else:
            return {"embeddings": outputs}

def save_model_files(model, save_dir, config=None):
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. 모델 가중치 저장
    torch.save(model.state_dict(), save_dir / "model_weights.pth")
    
    # 2. 모델 설정 저장
    if config:
        with open(save_dir / "config.json", "w") as f:
            json.dump(config, f, indent=4)
    
    # 3. 모델 아키텍처 저장 (TorchScript)
    model.eval()
    
    # 예제 입력 생성
    example_image = torch.randn(1, 3, 256, 256)
    example_text = torch.randint(0, 49408, (1, 77))  # CLIP 토크나이저 기준
    
    # 모델 래핑 및 트레이싱
    wrapped_model = ModelWrapper(model)
    wrapped_model.eval()
    
    with torch.no_grad():
        traced_model = torch.jit.trace(
            wrapped_model,
            (example_image, example_text),
            strict=False
        )
    
    torch.jit.save(traced_model, save_dir / "model_traced.pt")
    return str(save_dir)

def convert_to_coreml(model, save_path):
    model.eval()
    
    # 이미지 인코더와 텍스트 인코더 분리
    image_encoder = reparameterize_model(model.image_encoder)
    text_encoder = model.text_encoder
    
    # 1. 이미지 인코더 변환
    image_input = ct.ImageType(
        name="input_image",
        shape=(1, 3, 256, 256),
        scale=1/255.0,
        bias=[0, 0, 0],
        color_layout=ct.colorlayout.RGB,
    )
    
    image_model = ct.convert(
        image_encoder,
        inputs=[image_input],
        outputs=[
            ct.TensorType(name="image_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 2. 텍스트 인코더 변환
    text_input = ct.TensorType(
        name="input_text",
        shape=(1, 77),
        dtype=np.int32
    )
    
    text_model = ct.convert(
        text_encoder,
        inputs=[text_input],
        outputs=[
            ct.TensorType(name="text_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 메타데이터 추가
    for model in [image_model, text_model]:
        model.author = "MobileCLIP Converter"
        model.license = "MIT"
        model.version = "1.0"
        model.short_description = "Fine-tuned MobileCLIP-S2"
    
    # 모델 저장
    image_model.save(f"{save_path}/MobileCLIP_S2_Image.mlpackage")
    text_model.save(f"{save_path}/MobileCLIP_S2_Text.mlpackage")

def save_finetuned_model(base_model, lora_weights, save_path):
    # LoRA 가중치 저장
    lora_weights.save_pretrained(save_path)
    
    # 전체 모델 저장
    merged_model = base_model.merge_and_unload()
    torch.save(merged_model.state_dict(), f"{save_path}/merged_model.pth")

# 사용 예시
def main():
    # 1. 기본 모델 생성
    base_model = create_model('mobileclip_s2', pretrained=True)
    
    # 2. LoRA 설정
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1,
        bias="none"
    )
    
    # 3. LoRA 모델 생성
    lora_model = get_peft_model(base_model, lora_config)
    
    # 4. 모델 저장
    save_path = "./mobileclip_lora_finetuned"
    save_finetuned_model(
        base_model=base_model,
        lora_weights=lora_model,
        save_path=save_path
    )
    
    # 5. CoreML 변환
    config = {
        "model_type": "mobileclip_s2",
        "input_size": 256,
        "embedding_dim": 512
    }
    
    save_dir = save_model_files(lora_model, "./exported_model", config)
    convert_to_coreml(lora_model, save_dir)
    print(f"Model converted and saved to: {save_dir}")

if __name__ == "__main__":
    main()
```

주요 변경사항:
1. `ModelWrapper` 클래스 추가로 이미지와 텍스트 입력 처리
2. 이미지/텍스트 인코더 분리하여 각각 CoreML 변환
3. 토큰화된 텍스트 입력 (shape: 1x77) 추가
4. 두 개의 별도 mlpackage 파일 생성

이 코드는 파인튜닝된 MobileCLIP 모델을 iOS 앱에서 사용할 수 있도록 완전히 변환합니다. 변환된 두 개의 mlpackage 파일은 각각 이미지와 텍스트 처리를 담당하며, iOS 앱에서는 이 두 모델을 조합하여 사용하면 됩니다.

---
---
---
이 에러는 모델을 CoreML로 변환하기 전에 TorchScript 형식으로 변환하지 않아서 발생합니다. 수정된 `convert_to_coreml` 함수는 다음과 같습니다:

```python
def convert_to_coreml(model, save_path):
    model.eval()
    
    # 이미지 인코더와 텍스트 인코더 분리
    image_encoder = reparameterize_model(model.image_encoder)
    text_encoder = model.text_encoder
    
    # TorchScript로 변환
    example_image = torch.randn(1, 3, 256, 256)
    example_text = torch.randint(0, 49408, (1, 77))
    
    with torch.no_grad():
        traced_image_encoder = torch.jit.trace(image_encoder, example_image)
        traced_text_encoder = torch.jit.trace(text_encoder, example_text)
    
    # 이미지 인코더 변환
    image_input = ct.ImageType(
        name="input_image",
        shape=(1, 3, 256, 256),
        scale=1/255.0,
        bias=[0, 0, 0],
        color_layout=ct.colorlayout.RGB,
    )
    
    image_model = ct.convert(
        traced_image_encoder,  # TorchScript 모델 사용
        inputs=[image_input],
        outputs=[
            ct.TensorType(name="image_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 텍스트 인코더 변환
    text_model = ct.convert(
        traced_text_encoder,  # TorchScript 모델 사용
        inputs=[
            ct.TensorType(name="input_text", shape=(1, 77), dtype=np.int32)
        ],
        outputs=[
            ct.TensorType(name="text_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 모델 저장
    image_model.save(f"{save_path}/MobileCLIP_S2_Image.mlpackage")
    text_model.save(f"{save_path}/MobileCLIP_S2_Text.mlpackage")
```

주요 변경사항:
1. `torch.jit.trace()`를 사용하여 모델을 TorchScript 형식으로 변환[1]
2. 이미지와 텍스트 인코더를 각각 따로 트레이싱[2]
3. 변환된 TorchScript 모델을 CoreML 변환에 사용[1][6]

Citations:
[1] https://discuss.huggingface.co/t/converting-clip-to-coreml/31345
[2] https://huggingface.co/fguzman82/MobileCLIP
[3] https://stackoverflow.com/questions/44697524/core-ml-model-conversion-fails-with-unable-to-infer-input-name-and-dimensions
[4] https://stackoverflow.com/questions/72499842/how-would-i-convert-this-tensorflow-image-classification-model-to-core-ml
[5] https://huggingface.co/blog/fguzman82/frompytorch-to-coreml
[6] https://tutorials.pytorch.kr/prototype/ios_coreml_workflow.html
[7] https://github.com/apple/coremltools/issues/1418
[8] https://huggingface.co/apple/coreml-mobileclip/discussions/1

---
