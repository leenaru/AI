파인튜닝된 MobileCLIP-S2 모델의 저장, 로드, CoreML 변환을 위한 전체 코드를 제시합니다.

## 1. LoRA 가중치 저장 및 로드
```python
import torch
from peft import PeftModel, PeftConfig
from mobileclip import create_model

# 1-1. 모델 저장
def save_finetuned_model(base_model, lora_weights, save_path):
    # LoRA 가중치 저장
    lora_weights.save_pretrained(save_path)
    
    # 전체 모델 저장 (선택사항)
    merged_model = base_model.merge_and_unload()
    torch.save(merged_model.state_dict(), f"{save_path}/merged_model.pth")

# 1-2. 모델 로드
def load_finetuned_model(base_model_name="mobileclip_s2", lora_path="./lora_weights"):
    # 베이스 모델 로드
    base_model = create_model(base_model_name, pretrained=True)
    
    # LoRA 설정 로드
    config = PeftConfig.from_pretrained(lora_path)
    
    # LoRA 가중치 적용
    model = PeftModel.from_pretrained(base_model, lora_path)
    return model
```

## 2. 파일 저장을 위한 전체 코드
```python
import torch
import json
from pathlib import Path

def save_model_files(model, save_dir, config=None):
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # 2-1. 모델 가중치 저장
    torch.save(model.state_dict(), save_dir / "model_weights.pth")
    
    # 2-2. 모델 설정 저장
    if config:
        with open(save_dir / "config.json", "w") as f:
            json.dump(config, f, indent=4)
    
    # 2-3. 모델 아키텍처 저장 (TorchScript)
    model.eval()
    example_input = torch.randn(1, 3, 256, 256)
    traced_model = torch.jit.trace(model, example_input)
    torch.jit.save(traced_model, save_dir / "model_traced.pt")
    
    return str(save_dir)
```

## 3. CoreML 변환 전체 코드
```python
import coremltools as ct
import torch
from mobileclip.modules.common.mobileone import reparameterize_model

def convert_to_coreml(model, save_path):
    # 3-1. 모델 리파라미터라이제이션
    model.eval()
    reparam_model = reparameterize_model(model.image_encoder)
    
    # 3-2. TorchScript 변환
    example_input = torch.randn(1, 3, 256, 256)
    with torch.no_grad():
        traced_model = torch.jit.trace(reparam_model, example_input)
    
    # 3-3. 이미지 입력 설정 (MobileCLIP 사양)
    image_input = ct.ImageType(
        name="input_image",
        shape=(1, 3, 256, 256),
        scale=1/255.0,
        bias=[0, 0, 0],
        color_layout=ct.colorlayout.RGB,
    )
    
    # 3-4. CoreML 변환
    mlmodel = ct.convert(
        traced_model,
        inputs=[image_input],
        outputs=[
            ct.TensorType(name="image_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 3-5. 메타데이터 추가
    mlmodel.author = "MobileCLIP Converter"
    mlmodel.license = "MIT"
    mlmodel.version = "1.0"
    mlmodel.short_description = "Fine-tuned MobileCLIP-S2 Image Encoder"
    
    # 3-6. 모델 저장
    mlmodel.save(f"{save_path}/MobileCLIP_S2_Image.mlpackage")

# 전체 변환 프로세스 실행
def convert_finetuned_model(model_path, save_dir):
    # 모델 로드
    model = load_finetuned_model(lora_path=model_path)
    
    # 파일 저장
    save_path = save_model_files(
        model, 
        save_dir,
        config={
            "model_type": "mobileclip_s2",
            "input_size": 256,
            "embedding_dim": 512
        }
    )
    
    # CoreML 변환
    convert_to_coreml(model, save_path)
    
    return save_path
```

## 사용 예시
```python
# 모델 변환 실행
model_path = "./lora_weights"
save_dir = "./exported_model"
converted_path = convert_finetuned_model(model_path, save_dir)
print(f"Model converted and saved to: {converted_path}")
```

이 코드는 다음 특징을 가집니다:
- LoRA 가중치와 베이스 모델을 분리하여 관리[1]
- 4비트 양자화된 가중치를 CoreML 포맷으로 변환[2]
- Neural Engine 가속을 위한 최적화 설정 포함[4]
- MobileCLIP-S2 아키텍처에 맞는 입력 형상 및 전처리 파이프라인 구성[2]

실행 시 최소 iOS 16 이상의 기기에서 테스트해야 하며, 변환된 모델은 Vision 프레임워크와 함께 사용할 수 있습니다[4].

Citations:
[1] https://blog.sionic.ai/finetuning_llama
[2] https://huggingface.co/fguzman82/MobileCLIP
[3] https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog
[4] https://developer.apple.com/kr/machine-learning/core-ml/
[5] https://ariz1623.tistory.com/348
[6] https://slow-run.tistory.com/29
[7] https://blog.kbanknow.com/82
[8] https://developer.apple.com/kr/videos/play/tech-talks/10154/
[9] https://www.youtube.com/watch?v=aUXwVp4eUH4
[10] https://huggingface.co/apple/coreml-mobileclip/commit/55c8098c12ba8f928e37e26b2e1dd097777c3f98

---
---
---
파인튜닝된 MobileCLIP-S2 모델의 저장, 로드, CoreML 변환을 위한 전체 코드를 제시합니다.

## 1. LoRA 가중치 저장 및 로드
```python
import torch
from peft import PeftModel, PeftConfig
from mobileclip import create_model

# 1-1. 모델 저장
def save_finetuned_model(base_model, lora_weights, save_path):
    # LoRA 가중치 저장
    lora_weights.save_pretrained(save_path)
    
    # 전체 모델 저장 (선택사항)
    merged_model = base_model.merge_and_unload()
    torch.save(merged_model.state_dict(), f"{save_path}/merged_model.pth")

# 1-2. 모델 로드
def load_finetuned_model(base_model_name="mobileclip_s2", lora_path="./lora_weights"):
    # 베이스 모델 로드
    base_model = create_model(base_model_name, pretrained=True)
    
    # LoRA 설정 로드
    config = PeftConfig.from_pretrained(lora_path)
    
    # LoRA 가중치 적용
    model = PeftModel.from_pretrained(base_model, lora_path)
    return model
```

## 2. 파일 저장을 위한 전체 코드
```python
import torch
import json
from pathlib import Path

def save_model_files(model, save_dir, config=None):
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # 2-1. 모델 가중치 저장
    torch.save(model.state_dict(), save_dir / "model_weights.pth")
    
    # 2-2. 모델 설정 저장
    if config:
        with open(save_dir / "config.json", "w") as f:
            json.dump(config, f, indent=4)
    
    # 2-3. 모델 아키텍처 저장 (TorchScript)
    model.eval()
    example_input = torch.randn(1, 3, 256, 256)
    traced_model = torch.jit.trace(model, example_input)
    torch.jit.save(traced_model, save_dir / "model_traced.pt")
    
    return str(save_dir)
```

## 3. CoreML 변환 전체 코드
```python
import coremltools as ct
import torch
from mobileclip.modules.common.mobileone import reparameterize_model

def convert_to_coreml(model, save_path):
    # 3-1. 모델 리파라미터라이제이션
    model.eval()
    reparam_model = reparameterize_model(model.image_encoder)
    
    # 3-2. TorchScript 변환
    example_input = torch.randn(1, 3, 256, 256)
    with torch.no_grad():
        traced_model = torch.jit.trace(reparam_model, example_input)
    
    # 3-3. 이미지 입력 설정 (MobileCLIP 사양)
    image_input = ct.ImageType(
        name="input_image",
        shape=(1, 3, 256, 256),
        scale=1/255.0,
        bias=[0, 0, 0],
        color_layout=ct.colorlayout.RGB,
    )
    
    # 3-4. CoreML 변환
    mlmodel = ct.convert(
        traced_model,
        inputs=[image_input],
        outputs=[
            ct.TensorType(name="image_embeddings", shape=(1, 512))
        ],
        convert_to="mlprogram",
        compute_units=ct.ComputeUnit.CPU_AND_NEURAL_ENGINE,
        compute_precision=ct.precision.FLOAT16,
        minimum_deployment_target=ct.target.iOS16
    )
    
    # 3-5. 메타데이터 추가
    mlmodel.author = "MobileCLIP Converter"
    mlmodel.license = "MIT"
    mlmodel.version = "1.0"
    mlmodel.short_description = "Fine-tuned MobileCLIP-S2 Image Encoder"
    
    # 3-6. 모델 저장
    mlmodel.save(f"{save_path}/MobileCLIP_S2_Image.mlpackage")

# 전체 변환 프로세스 실행
def convert_finetuned_model(model_path, save_dir):
    # 모델 로드
    model = load_finetuned_model(lora_path=model_path)
    
    # 파일 저장
    save_path = save_model_files(
        model, 
        save_dir,
        config={
            "model_type": "mobileclip_s2",
            "input_size": 256,
            "embedding_dim": 512
        }
    )
    
    # CoreML 변환
    convert_to_coreml(model, save_path)
    
    return save_path
```

## 사용 예시
```python
# 모델 변환 실행
model_path = "./lora_weights"
save_dir = "./exported_model"
converted_path = convert_finetuned_model(model_path, save_dir)
print(f"Model converted and saved to: {converted_path}")
```

이 코드는 다음 특징을 가집니다:
- LoRA 가중치와 베이스 모델을 분리하여 관리[1]
- 4비트 양자화된 가중치를 CoreML 포맷으로 변환[2]
- Neural Engine 가속을 위한 최적화 설정 포함[4]
- MobileCLIP-S2 아키텍처에 맞는 입력 형상 및 전처리 파이프라인 구성[2]

실행 시 최소 iOS 16 이상의 기기에서 테스트해야 하며, 변환된 모델은 Vision 프레임워크와 함께 사용할 수 있습니다[4].

Citations:
[1] https://blog.sionic.ai/finetuning_llama
[2] https://huggingface.co/fguzman82/MobileCLIP
[3] https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog
[4] https://developer.apple.com/kr/machine-learning/core-ml/
[5] https://ariz1623.tistory.com/348
[6] https://slow-run.tistory.com/29
[7] https://blog.kbanknow.com/82
[8] https://developer.apple.com/kr/videos/play/tech-talks/10154/
[9] https://www.youtube.com/watch?v=aUXwVp4eUH4
[10] https://huggingface.co/apple/coreml-mobileclip/commit/55c8098c12ba8f928e37e26b2e1dd097777c3f98

---
