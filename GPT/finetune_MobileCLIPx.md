## ğŸ”¹ **MobileCLIPì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œí•˜ëŠ” ë°©ë²•** (ğŸ”¥ `torch.hub.load` ì‚¬ìš© ì•ˆ í•¨)

> **âœ… ì£¼ìš” ëª©í‘œ:**  
> - `torch.hub.load()` ì—†ì´ MobileCLIPì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì„¤ëª…  
> - `state_dict` ê¸°ë°˜ìœ¼ë¡œ PyTorchì—ì„œ ëª¨ë¸ì„ ë¡œë“œ  
> - **ì§ì ‘ ëª¨ë¸ ì •ì˜ + ë¡œì»¬ì—ì„œ MobileCLIP ë¶ˆëŸ¬ì˜¤ê¸°**

---

## ğŸ“Œ **1. MobileCLIP ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**  
MobileCLIP ëª¨ë¸ì„ Apple Researchì—ì„œ ì œê³µí•˜ëŠ” `.pth` íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.  
> **ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ ë§í¬ (ì§ì ‘ í™•ì¸ í•„ìš”)**  
```
https://example.com/apple/mobileclip_weights.pth
```
**ğŸ”¥ í„°ë¯¸ë„ì—ì„œ ë‹¤ìš´ë¡œë“œ**
```bash
mkdir -p weights
wget -O weights/mobileclip.pth https://example.com/apple/mobileclip_weights.pth
```
---

## ğŸ“Œ **2. MobileCLIP ëª¨ë¸ ì§ì ‘ êµ¬í˜„í•˜ê¸°**  
Appleì´ ì œê³µí•˜ëŠ” `torch.hub.load("apple/mobileclip", "image_encoder")` ëŒ€ì‹   
âœ… **MobileCLIPì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì§ì ‘ ì •ì˜í•˜ê³  `state_dict`ë¡œ ë¡œë“œ**

ğŸ“ **MobileCLIP ê¸°ë³¸ ëª¨ë¸ ì •ì˜**
```python
import torch
import torch.nn as nn
import torchvision.models as models

class MobileCLIP(nn.Module):
    def __init__(self):
        super().__init__()
        
        # âœ… ì´ë¯¸ì§€ ì¸ì½”ë” (EfficientNet ê¸°ë°˜)
        self.image_encoder = models.efficientnet_b0(pretrained=False)
        self.image_encoder.classifier = nn.Identity()  # ë¶„ë¥˜ê¸° ì œê±°í•˜ì—¬ feature extractorë¡œ ì‚¬ìš©
        
        # âœ… í…ìŠ¤íŠ¸ ì¸ì½”ë” (Transformer ê¸°ë°˜)
        self.text_encoder = nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6)
        
        # âœ… í•™ìŠµ ê°€ëŠ¥í•œ ë¡œê·¸ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° (CLIPê³¼ ë™ì¼)
        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1 / 0.07)))

    def forward(self, images, texts):
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(texts["input_ids"], texts["attention_mask"])
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * torch.matmul(image_features, text_features.t())
        logits_per_text = logits_per_image.t()
        return logits_per_image, logits_per_text
```

---

## ğŸ“Œ **3. ë‹¤ìš´ë¡œë“œí•œ MobileCLIP ê°€ì¤‘ì¹˜ ë¡œë“œ**
ìœ„ì—ì„œ ì •ì˜í•œ `MobileCLIP` ëª¨ë¸ì— ì§ì ‘ `.pth` íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

ğŸ“ **ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ**
```python
# âœ… MobileCLIP ëª¨ë¸ ìƒì„±
model = MobileCLIP()

# âœ… ë‹¤ìš´ë¡œë“œí•œ ê°€ì¤‘ì¹˜ ë¡œë“œ
model_path = "./weights/mobileclip.pth"
state_dict = torch.load(model_path, map_location="cpu")
model.load_state_dict(state_dict)

# âœ… GPUê°€ ìˆë‹¤ë©´ ëª¨ë¸ì„ GPUë¡œ ì´ë™
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print("âœ… MobileCLIP ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
```
âœ… **ì´ì œ `torch.hub.load` ì—†ì´ë„ MobileCLIP ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ!**

---

## ğŸ“Œ **4. MobileCLIP í…ŒìŠ¤íŠ¸ (Inference)**
âœ… MobileCLIP ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸  
âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸

ğŸ“ **ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**
```python
from PIL import Image
import torchvision.transforms as T
from transformers import AutoTokenizer

# âœ… ì´ë¯¸ì§€ ë³€í™˜ (MobileCLIPì— ë§ê²Œ ì •ê·œí™”)
image_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
])

def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    return image_transform(image).unsqueeze(0).to(device)

# âœ… í…ìŠ¤íŠ¸ í† í°í™” (Hugging Face BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_text(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    return {key: val.to(device) for key, val in tokens.items()}
```

ğŸ“ **ëª¨ë¸ ì¶”ë¡  (Inference)**
```python
# âœ… ìƒ˜í”Œ ì…ë ¥ ë°ì´í„°
image_path = "./test_images/cat.jpg"
text_input = "A cat sitting on a chair."

# âœ… ë°ì´í„° ë³€í™˜
image = preprocess_image(image_path)
text = preprocess_text(text_input)

# âœ… ëª¨ë¸ ì¶”ë¡ 
with torch.no_grad():
    logits_per_image, logits_per_text = model(image, text)

print(f"ğŸ”¹ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜: {logits_per_image.item():.4f}")
```

âœ… **ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë©´ MobileCLIPì„ ì‚¬ìš©í•  ì¤€ë¹„ ì™„ë£Œ!**

---

## ğŸ“Œ **5. MobileCLIP ëª¨ë¸ ì €ì¥ (ì¶”í›„ ì‚¬ìš©)**
í•™ìŠµ í›„ ëª¨ë¸ì„ ë‹¤ì‹œ ì €ì¥í•  ìˆ˜ë„ ìˆìŒ.
```python
save_path = "./weights/mobileclip_finetuned.pth"
torch.save(model.state_dict(), save_path)
print(f"âœ… í•™ìŠµëœ MobileCLIP ëª¨ë¸ì´ ì €ì¥ë¨: {save_path}")
```

---

# ğŸ¯ **ê²°ë¡ **
| ë‹¨ê³„ | ì„¤ëª… |
|------|------|
| ğŸ“¥ 1. MobileCLIP ëª¨ë¸ ë‹¤ìš´ë¡œë“œ | `.pth` íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œ |
| ğŸ”§ 2. ëª¨ë¸ ì •ì˜ | MobileCLIPì„ ì§ì ‘ PyTorchë¡œ êµ¬í˜„ |
| ğŸ— 3. ëª¨ë¸ ë¡œë“œ | `state_dict()`ë¥¼ ì‚¬ìš©í•´ ë¡œë“œ |
| ğŸ¯ 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸ | ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ê²°ê³¼ í™•ì¸ |
| ğŸ’¾ 5. ëª¨ë¸ ì €ì¥ | í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš© |

ğŸ‘‰ **ì´ì œ `torch.hub.load` ì—†ì´ MobileCLIPì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸš€  
ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš” ğŸ˜Š
