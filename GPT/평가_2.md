아래는 JSON 파일로 구성된 데이터셋을 이용하여 MobileCLIP 모델을 평가하고, 추가로 F1, Precision, Recall, Confusion Matrix 등 다양한 평가 지표를 계산 및 시각화하는 예시 코드입니다.  
예제는 두 가지 경우에 대해 진행합니다.

1. **원본 MobileCLIP 모델 평가 (JSON 데이터셋 사용, 추가 평가 지표 포함)**  
2. **LoRA를 적용하여 파인튜닝한 MobileCLIP 모델 평가 (JSON 데이터셋 사용, 추가 평가 지표 포함)**

---

> **참고:**  
> - JSON 파일은 각 샘플이 딕셔너리 형태로 구성되며, 예를 들어 아래와 같이 구성되어 있다고 가정합니다.  
>   ```json
>   [
>     {"image_path": "path/to/image1.jpg", "label": "cat"},
>     {"image_path": "path/to/image2.jpg", "label": "dog"},
>     ...
>   ]
>   ```  
> - 아래 코드는 MobileCLIP 모델과 Hugging Face의 CLIPProcessor, PEFT 라이브러리(LoRA 적용)를 사용합니다. 실제 모델 이름이나 데이터 전처리 방식은 여러분의 환경에 맞게 수정하시기 바랍니다.

---

## 1. 원본 MobileCLIP 모델 평가 (JSON 데이터셋 사용, 추가 평가 지표 포함)

### 1-1. 평가를 위한 JSON 데이터셋 로드 및 전처리

```python
import json
import pandas as pd

# JSON 파일 로드 (각 항목은 {"image_path": ..., "label": ...} 형태)
with open("dataset.json", "r") as f:
    data = json.load(f)
    
# pandas DataFrame으로 변환 (편리한 인덱싱을 위해)
df = pd.DataFrame(data)
```

### 1-2. 모델 및 프로세서 로드, 후보 텍스트 임베딩 계산

```python
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

# 모델 이름을 여러분이 사용하는 MobileCLIP 모델의 Hugging Face Hub ID로 변경하세요.
model_name = "mobileclip/mobileclip-model"  # 예시 모델 이름
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)
model.eval()  # 평가 모드로 전환

# 데이터셋에 있는 유니크 레이블을 후보 텍스트 목록으로 생성
candidate_labels = df['label'].unique().tolist()

# 후보 텍스트에 대해 미리 임베딩 계산 (정규화 포함)
with torch.no_grad():
    text_inputs = processor(text=candidate_labels, return_tensors="pt", padding=True)
    text_features = model.get_text_features(**text_inputs)
    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)
```

### 1-3. 평가 진행 및 다양한 평가 지표 계산

```python
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

all_preds = []
all_gts = []

# 각 샘플에 대해 이미지 임베딩을 구하고, 코사인 유사도 기반 예측 수행
for _, row in tqdm(df.iterrows(), total=len(df)):
    # 이미지 로드 (PIL 이용)
    image = Image.open(row['image_path']).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
        image_features /= image_features.norm(p=2, dim=-1, keepdim=True)
    
    # 코사인 유사도 계산: image_features와 후보 text_features의 내적
    similarities = (image_features @ text_features.T).squeeze(0)
    pred_idx = torch.argmax(similarities).item()
    pred_label = candidate_labels[pred_idx]
    
    all_preds.append(pred_label)
    all_gts.append(row['label'])

# 다양한 평가 지표 계산 (클래스가 여러 개인 경우 weighted 평균 사용)
accuracy = accuracy_score(all_gts, all_preds)
precision = precision_score(all_gts, all_preds, average='weighted')
recall = recall_score(all_gts, all_preds, average='weighted')
f1 = f1_score(all_gts, all_preds, average='weighted')
cm = confusion_matrix(all_gts, all_preds)

print("=== 원본 MobileCLIP 평가 결과 ===")
print("Accuracy :", accuracy)
print("Precision:", precision)
print("Recall   :", recall)
print("F1 Score :", f1)

# Confusion Matrix 시각화
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=candidate_labels, yticklabels=candidate_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix (Original MobileCLIP)")
plt.show()
```

---

## 2. LoRA를 적용한 MobileCLIP 파인튜닝 및 평가 (JSON 데이터셋 사용, 추가 평가 지표 포함)

### 2-1. 파인튜닝용 JSON 데이터셋 클래스 정의

파인튜닝 시 JSON 파일을 이용하여 데이터를 로드하는 Dataset 클래스를 정의합니다.

```python
from torch.utils.data import Dataset

class CLIPJsonDataset(Dataset):
    def __init__(self, json_file, processor):
        with open(json_file, "r") as f:
            self.data = json.load(f)
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        # 이미지와 텍스트(레이블)를 로드
        image = Image.open(item['image_path']).convert("RGB")
        text = item['label']  # 만약 별도의 설명(text) 컬럼이 있다면 변경 가능
        inputs = self.processor(text=[text], images=[image],
                                return_tensors="pt", padding="max_length", truncation=True)
        # 배치 차원 제거
        inputs = {k: v.squeeze(0) for k, v in inputs.items()}
        return inputs
```

### 2-2. LoRA 적용 및 파인튜닝 준비

아래 예시는 Hugging Face의 PEFT 라이브러리를 이용해 CLIP 모델의 텍스트 인코더에 LoRA를 적용한 후 Trainer를 사용하여 파인튜닝하는 코드입니다.

```python
from transformers import Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType

# 기존 MobileCLIP 모델과 프로세서 로드
model_name = "mobileclip/mobileclip-model"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# LoRA 설정 (텍스트 인코더의 일부 모듈에 적용; 모델 구조에 맞게 target_modules를 수정하세요)
lora_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,  # CLIP의 텍스트 인코더는 Transformer 구조를 가짐
    r=8,                            # 저랭크 행렬의 차원
    lora_alpha=32,                  # 스케일링 파라미터
    target_modules=["q_proj", "k_proj", "v_proj"]  # fine-tuning할 모듈 (모델에 따라 달라질 수 있음)
)
# 텍스트 인코더에 LoRA 적용
model.text_model = get_peft_model(model.text_model, lora_config)

# 파인튜닝용 데이터셋 (예: train_dataset.json)
train_dataset = CLIPJsonDataset("train_dataset.json", processor)

# 데이터 콜레이터 (배치 내 각 딕셔너리의 텐서를 스택)
def collate_fn(batch):
    collated = {}
    for key in batch[0]:
        collated[key] = torch.stack([item[key] for item in batch])
    return collated

# TrainingArguments 설정 (여러분의 환경에 맞게 수정)
training_args = TrainingArguments(
    output_dir="./lora_mobileclip_output",
    per_device_train_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-5,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no",  # 여기서는 파인튜닝만 진행
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn,
)

# 파인튜닝 수행
trainer.train()
```

### 2-3. 파인튜닝된 모델 평가 (JSON 데이터셋, 추가 평가 지표 포함)

파인튜닝이 완료된 후, 앞서와 동일한 평가 코드를 사용하여 LoRA 적용 후 모델을 평가합니다.

```python
# 평가 모드로 전환
model.eval()

# (원본 평가와 동일하게) 후보 텍스트 임베딩 계산
with torch.no_grad():
    text_inputs = processor(text=candidate_labels, return_tensors="pt", padding=True)
    text_features = model.get_text_features(**text_inputs)
    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)

all_preds_lora = []
all_gts = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    image = Image.open(row['image_path']).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
        image_features /= image_features.norm(p=2, dim=-1, keepdim=True)
    similarities = (image_features @ text_features.T).squeeze(0)
    pred_idx = torch.argmax(similarities).item()
    pred_label = candidate_labels[pred_idx]
    
    all_preds_lora.append(pred_label)
    all_gts.append(row['label'])

# 평가 지표 계산
accuracy_lora = accuracy_score(all_gts, all_preds_lora)
precision_lora = precision_score(all_gts, all_preds_lora, average='weighted')
recall_lora = recall_score(all_gts, all_preds_lora, average='weighted')
f1_lora = f1_score(all_gts, all_preds_lora, average='weighted')
cm_lora = confusion_matrix(all_gts, all_preds_lora)

print("=== LoRA 적용 후 MobileCLIP 평가 결과 ===")
print("Accuracy :", accuracy_lora)
print("Precision:", precision_lora)
print("Recall   :", recall_lora)
print("F1 Score :", f1_lora)

# Confusion Matrix 시각화
plt.figure(figsize=(8, 6))
sns.heatmap(cm_lora, annot=True, fmt="d", cmap="Oranges",
            xticklabels=candidate_labels, yticklabels=candidate_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix (LoRA Finetuned MobileCLIP)")
plt.show()
```

---

## 3. 최종 정리

- **데이터셋 구성:**  
  - JSON 파일로 이미지 경로와 레이블 정보를 저장합니다.
  - 평가 및 파인튜닝 데이터셋 모두 JSON 파일을 활용하며, 필요에 따라 별도의 Dataset 클래스를 정의합니다.

- **모델 평가:**  
  - MobileCLIP 모델을 로드한 후, 이미지와 텍스트 임베딩을 계산하고 코사인 유사도를 사용하여 예측합니다.
  - 평가 시 accuracy 외에도 precision, recall, F1 score, confusion matrix 등을 계산하여 보다 다양한 관점에서 모델 성능을 분석합니다.
  - confusion matrix는 seaborn의 heatmap을 이용해 시각화할 수 있습니다.

- **LoRA 파인튜닝:**  
  - PEFT 라이브러리를 사용하여 텍스트 인코더에 LoRA를 적용한 후, JSON 파일 기반의 데이터셋으로 파인튜닝합니다.
  - 파인튜닝 후 동일한 평가 지표와 방법을 사용하여 원본 모델과의 성능 변화를 비교합니다.

이와 같이 단계별 코드를 참고하여 JSON 데이터셋을 활용한 MobileCLIP 평가 및 LoRA 파인튜닝 후 성능 비교 작업을 진행해 보세요.
