MobileCLIPì— LoRA(Low-Rank Adaptation)ë¥¼ ì ìš©í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¥¼ ë”°ë¥´ë©´ ë©ë‹ˆë‹¤.

### 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
ë¨¼ì €, í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
```bash
pip install torch torchvision transformers peft
```
PEFT(ğŸ¤— Hugging Faceì˜ `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬)ë¥¼ ì‚¬ìš©í•˜ë©´ LoRAë¥¼ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2. MobileCLIP ëª¨ë¸ ë¡œë“œ
MobileCLIP ëª¨ë¸ì€ Appleì—ì„œ ê³µê°œí•œ ëª¨ë¸ë¡œ, ViT ê¸°ë°˜ì˜ ê²½ëŸ‰ CLIP ëª¨ë¸ì…ë‹ˆë‹¤. ë¨¼ì €, MobileCLIPì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

```python
import torch
from transformers import CLIPProcessor, CLIPModel

# MobileCLIP ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
model_name = "apple/ml-mobileclip"  # ì‹¤ì œ MobileCLIP ëª¨ë¸ì´ ê³µê°œë˜ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)
```

### 3. LoRA ì„¤ì • ë° ì ìš©
LoRAë¥¼ ì ìš©í•˜ë ¤ë©´ `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

```python
from peft import get_peft_model, LoraConfig, TaskType

# LoRA ì„¤ì •
lora_config = LoraConfig(
    task_type=TaskType.FEATURE_EXTRACTION,  # CLIPì€ íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸
    r=8,  # ë­í¬ ê°’ (ì„±ëŠ¥ê³¼ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
    lora_alpha=32,  # Scaling Factor
    lora_dropout=0.1,
    target_modules=["text_model.encoder.layers", "vision_model.encoder.layers"],  # ì ìš©í•  ë ˆì´ì–´
)

# MobileCLIP ëª¨ë¸ì— LoRA ì ìš©
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # í™•ì¸
```

### 4. LoRAë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í›ˆë ¨
LoRAê°€ ì ìš©ëœ MobileCLIPì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ê³ , ëª¨ë¸ì„ í›ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from torch.utils.data import DataLoader
from transformers import CLIPTokenizer, CLIPTextModel, CLIPVisionModel
import torch.optim as optim

# ë°ì´í„°ì…‹ ë¡œë“œ (ì‚¬ìš©ì ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì •)
train_dataloader = DataLoader(..., batch_size=32, shuffle=True)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# í›ˆë ¨ ë£¨í”„
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.train()

for epoch in range(3):  # 3 Epoch ì˜ˆì‹œ
    for batch in train_dataloader:
        images, texts = batch["image"].to(device), batch["text"]
        inputs = processor(text=texts, images=images, return_tensors="pt", padding=True).to(device)

        outputs = model(**inputs)
        loss = outputs.loss  # CLIPì€ ê¸°ë³¸ì ìœ¼ë¡œ Contrastive Lossë¥¼ ì‚¬ìš©

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")
```

### 5. LoRA ì ìš© ëª¨ë¸ ì €ì¥ ë° í™œìš©
í›ˆë ¨ì´ ëë‚œ í›„ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ëª¨ë¸ ì €ì¥
model.save_pretrained("mobileclip_lora")
processor.save_pretrained("mobileclip_lora")

# ëª¨ë¸ ë¡œë“œ í›„ ì¶”ë¡  ì˜ˆì‹œ
model = CLIPModel.from_pretrained("mobileclip_lora")
processor = CLIPProcessor.from_pretrained("mobileclip_lora")

# ì¶”ë¡  ìˆ˜í–‰
text_inputs = ["a cat", "a dog"]
image_inputs = ...  # ì…ë ¥ ì´ë¯¸ì§€
inputs = processor(text=text_inputs, images=image_inputs, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜

print(logits_per_image)
```

ì´ë ‡ê²Œ í•˜ë©´ MobileCLIPì— LoRAë¥¼ ì ìš©í•˜ì—¬ ê²½ëŸ‰í™”ëœ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì„ ìˆ˜í–‰í•˜ê³ , ë‹¤ì–‘í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ì—ì„œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
