JSON í˜•ì‹ì˜ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ëŠ” PyTorch `Dataset` í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ê³ , `DataLoader`ë¥¼ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

---

## 1. `Dataset` í´ë˜ìŠ¤ êµ¬í˜„
JSON íŒŒì¼ì„ ì½ê³ , ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë¡œë“œí•˜ëŠ” `MobileCLIPDataset` í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import json
import os
from transformers import CLIPTokenizer

class MobileCLIPDataset(Dataset):
    def __init__(self, json_file, image_dir, tokenizer, transform=None):
        """
        MobileCLIPì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤

        :param json_file: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì´ ì €ì¥ëœ JSON íŒŒì¼ ê²½ë¡œ
        :param image_dir: ì´ë¯¸ì§€ íŒŒì¼ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
        :param tokenizer: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í°í™”í•  CLIP í† í¬ë‚˜ì´ì €
        :param transform: ì´ë¯¸ì§€ì— ì ìš©í•  ë³€í™˜(transform)
        """
        # JSON íŒŒì¼ ë¡œë“œ
        with open(json_file, "r", encoding="utf-8") as f:
            self.data = json.load(f)
        
        self.image_dir = image_dir
        self.tokenizer = tokenizer
        self.transform = transform if transform else transforms.Compose([
            transforms.Resize((224, 224)),  # MobileCLIPì€ 224x224 í•´ìƒë„ë¥¼ ì‚¬ìš©
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
        img_path = os.path.join(self.image_dir, self.data[idx]["image"])
        image = Image.open(img_path).convert("RGB")
        image = self.transform(image)

        # í…ìŠ¤íŠ¸ ë¡œë“œ ë° í† í°í™”
        text = self.data[idx]["text"]
        text_tokens = self.tokenizer(text, padding='max_length', truncation=True, return_tensors="pt")

        return image, text_tokens['input_ids'].squeeze(0), text_tokens['attention_mask'].squeeze(0)
```

---

## 2. `DataLoader` ìƒì„±
ìœ„ì—ì„œ ì •ì˜í•œ `MobileCLIPDataset`ì„ ì´ìš©í•˜ì—¬ `DataLoader`ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
# CLIPìš© í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# ë°ì´í„°ì…‹ ìƒì„±
dataset = MobileCLIPDataset(json_file="dataset.json", image_dir="images", tokenizer=tokenizer)

# ë°ì´í„° ë¡œë” ìƒì„±
data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

# ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸
for batch in data_loader:
    images, text_ids, attention_masks = batch
    print("Image batch shape:", images.shape)
    print("Text token batch shape:", text_ids.shape)
    print("Attention mask shape:", attention_masks.shape)
    break  # í•œ ë°°ì¹˜ë§Œ í™•ì¸
```

---

## 3. ì£¼ìš” ì„¤ëª…
1. **JSON ë°ì´í„° ë¡œë“œ**
   - `json.load(f)`ë¥¼ ì´ìš©í•´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì˜ JSONì„ ì½ìŒ
   - ê° í•­ëª©ì—ì„œ `"image"`ì™€ `"text"` í•„ë“œë¥¼ ì¶”ì¶œ

2. **ì´ë¯¸ì§€ ì „ì²˜ë¦¬**
   - ì´ë¯¸ì§€ëŠ” `224x224` í¬ê¸°ë¡œ ë³€í™˜
   - ì •ê·œí™” `(0.5, 0.5, 0.5)`ë¡œ ì„¤ì •í•˜ì—¬ MobileCLIP í•™ìŠµ í™˜ê²½ê³¼ ë§ì¶¤

3. **í…ìŠ¤íŠ¸ í† í°í™”**
   - `CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")`ë¥¼ ì´ìš©í•´ í† í°í™”
   - `padding='max_length'`, `truncation=True` ì„¤ì •

4. **`DataLoader` ì„¤ì •**
   - `batch_size=32`: í•œ ë²ˆì— 32ê°œ ìƒ˜í”Œ ë¡œë“œ
   - `shuffle=True`: ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ìŒ
   - `num_workers=4`: ë°ì´í„° ë¡œë”© ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ 4ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©

---

## 4. ì‹¤í–‰ ì˜ˆì‹œ ì¶œë ¥
```
Image batch shape: torch.Size([32, 3, 224, 224])
Text token batch shape: torch.Size([32, 77])
Attention mask shape: torch.Size([32, 77])
```

ì´ì œ MobileCLIPì„ í•™ìŠµí•  ë•Œ ì´ `DataLoader`ë¥¼ í™œìš©í•˜ë©´ ë©ë‹ˆë‹¤. ğŸš€
