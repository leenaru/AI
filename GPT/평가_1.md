아래는 MobileCLIP 모델을 여러분이 보유한 데이터셋에 대해 평가하고, LoRA를 적용하여 파인튜닝한 후 그 결과를 비교 및 시각화하는 워크플로우의 예시 코드입니다.  
  
**주의:** 아래 코드는 MobileCLIP 모델과 데이터셋의 형태(예, 이미지 경로와 텍스트/레이블 정보 등)에 따라 수정이 필요할 수 있으며, 여기서는 CLIP 모델과 Hugging Face의 [PEFT](https://github.com/huggingface/peft) 라이브러리(LoRA 적용용)를 활용하는 예시로 작성되었습니다.

---

## 1. 내가 만든 데이터셋으로 MobileCLIP 모델 평가

예시에서는 CSV 파일(`dataset.csv`)에 각 행마다 이미지 경로(`image_path`)와 텍스트 레이블(`label`)이 있다고 가정합니다.  
평가는 “zero-shot classification” 방식—이미지와 후보 텍스트 간의 코사인 유사도를 계산하여 가장 높은 점수를 가진 레이블을 예측—로 진행합니다.

```python
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import accuracy_score

# 1. 데이터셋 로드 (CSV 파일에 image_path, label 컬럼이 있다고 가정)
df = pd.read_csv("dataset.csv")

# 2. MobileCLIP (또는 CLIP 기반) 모델과 프로세서 로드  
# (모델 이름은 여러분이 사용하시는 MobileCLIP 모델의 Hugging Face Hub ID로 변경하세요)
model_name = "mobileclip/mobileclip-model"  # 예시 모델 이름
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)
model.eval()  # 평가 모드

# 3. 후보 텍스트 목록 생성 (데이터셋에 존재하는 유니크 레이블)
candidate_labels = df['label'].unique().tolist()

# 4. 후보 텍스트에 대해 미리 텍스트 임베딩 계산 (정규화 포함)
with torch.no_grad():
    text_inputs = processor(text=candidate_labels, return_tensors="pt", padding=True)
    text_features = model.get_text_features(**text_inputs)
    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)

# 5. 데이터셋의 각 이미지에 대해 예측 수행
all_preds = []
all_gts = []

for idx, row in tqdm(df.iterrows(), total=len(df)):
    # 이미지 로드 (PIL 사용)
    image = Image.open(row['image_path']).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
        image_features /= image_features.norm(p=2, dim=-1, keepdim=True)
    # 코사인 유사도를 통해 가장 유사한 텍스트(레이블) 선택
    similarities = (image_features @ text_features.T).squeeze(0)
    pred_idx = torch.argmax(similarities).item()
    pred_label = candidate_labels[pred_idx]
    
    all_preds.append(pred_label)
    all_gts.append(row['label'])

# 6. 평가 지표 (정확도) 계산
accuracy = accuracy_score(all_gts, all_preds)
print("LoRA 적용 전 MobileCLIP 정확도:", accuracy)
```

---

## 2. MobileCLIP에 LoRA 적용하여 파인튜닝한 후 다시 평가하기

여기서는 Hugging Face의 [PEFT 라이브러리](https://github.com/huggingface/peft)를 사용해 CLIP 모델의 텍스트 인코더(Transformer 부분)에 LoRA를 적용하는 예시를 보여드립니다.  
※ **주의:**  
- 실제로 CLIP 모델의 이미지와 텍스트 모달리티 모두에 LoRA를 적용할 수도 있지만, 예시에서는 텍스트 인코더에 적용합니다.  
- 여러분의 데이터셋이 학습(파인튜닝)용 데이터셋(`train_dataset.csv`)과 평가용 데이터셋(`dataset.csv`)으로 나뉘어 있다고 가정합니다.

### 2-1. 파인튜닝 데이터셋 클래스 정의

```python
from torch.utils.data import Dataset

class CLIPDataset(Dataset):
    def __init__(self, csv_file, processor):
        self.df = pd.read_csv(csv_file)
        self.processor = processor

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        # 이미지와 텍스트(레이블 또는 설명)를 로드합니다.
        image = Image.open(row['image_path']).convert("RGB")
        text = row['label']  # 혹은 row['text'] 등 적절한 컬럼 사용
        inputs = self.processor(text=[text], images=[image],
                                return_tensors="pt", padding="max_length", truncation=True)
        # 배치 차원을 제거
        inputs = {k: v.squeeze(0) for k, v in inputs.items()}
        return inputs
```

### 2-2. LoRA 적용 및 파인튜닝 준비

```python
from transformers import CLIPModel, CLIPProcessor, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType

# 1. 기존 MobileCLIP 모델과 프로세서 로드
model_name = "mobileclip/mobileclip-model"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# 2. LoRA 설정 (여기서는 텍스트 인코더의 특정 모듈에 적용)
lora_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,  # 해당 모듈에 맞게 변경 (CLIP의 텍스트 인코더는 Transformer 구조입니다)
    r=8,                            # 저랭크 행렬의 rank
    lora_alpha=32,                  # 스케일링 파라미터
    target_modules=["q_proj", "k_proj", "v_proj"]  # fine-tuning 할 모듈 이름 (모델에 따라 다름)
)

# 3. 텍스트 인코더에 LoRA 적용 (CLIPModel은 .text_model 속성에 텍스트 인코더를 포함)
model.text_model = get_peft_model(model.text_model, lora_config)

# 4. 파인튜닝용 데이터셋 로드 (예: train_dataset.csv)
train_dataset = CLIPDataset("train_dataset.csv", processor)

# 5. Trainer용 데이터 콜레이터 정의 (배치 내 dict 항목을 스택)
def collate_fn(batch):
    collated = {}
    for key in batch[0].keys():
        collated[key] = torch.stack([item[key] for item in batch])
    return collated

# 6. TrainingArguments 설정 (여러분의 환경에 맞게 수정)
training_args = TrainingArguments(
    output_dir="./lora_mobileclip_output",
    per_device_train_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-5,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no",  # 여기서는 단순히 파인튜닝만 진행합니다.
    remove_unused_columns=False,
)

# 7. Trainer 생성 및 파인튜닝 수행
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn,
)

trainer.train()
```

### 2-3. 파인튜닝된 모델로 평가 수행

파인튜닝이 완료된 모델을 이용하여 1번과 동일한 방식으로 평가를 진행합니다.  
여기서는 이전에 사용한 `df`(평가용 CSV 데이터셋)를 그대로 사용합니다.

```python
# 평가 전에 모델을 평가 모드로 전환
model.eval()

# (1번과 동일한 후보 텍스트 임베딩 계산)
with torch.no_grad():
    text_inputs = processor(text=candidate_labels, return_tensors="pt", padding=True)
    text_features = model.get_text_features(**text_inputs)
    text_features /= text_features.norm(p=2, dim=-1, keepdim=True)

all_preds_lora = []
# 이미 앞서 all_gts에는 정답 레이블이 저장되어 있음

for idx, row in tqdm(df.iterrows(), total=len(df)):
    image = Image.open(row['image_path']).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
        image_features /= image_features.norm(p=2, dim=-1, keepdim=True)
    similarities = (image_features @ text_features.T).squeeze(0)
    pred_idx = torch.argmax(similarities).item()
    pred_label = candidate_labels[pred_idx]
    
    all_preds_lora.append(pred_label)

# 평가 지표 계산
accuracy_lora = accuracy_score(all_gts, all_preds_lora)
print("LoRA 적용 후 MobileCLIP 정확도:", accuracy_lora)
```

---

## 3. 평가 결과 및 성능 비교 시각화

두 가지 경우(LoRA 미적용 vs. LoRA 적용)의 정확도(또는 다른 지표)를 바 차트 등으로 비교할 수 있습니다.

```python
import matplotlib.pyplot as plt

# 평가 결과 (예: 정확도)를 리스트로 정리
methods = ["Original", "LoRA Finetuned"]
accuracies = [accuracy, accuracy_lora]

plt.figure(figsize=(6, 4))
plt.bar(methods, accuracies, color=["skyblue", "salmon"])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("MobileCLIP 평가 결과 비교")
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()
```

또한, 평가 과정에서 다른 메트릭(예: F1-score, confusion matrix 등)도 활용하여 비교 및 시각화할 수 있습니다.

---

## 최종 정리

1. **평가 전 단계:**  
   - 데이터셋(CSV)에서 이미지와 텍스트(레이블)를 로드  
   - MobileCLIP 모델과 프로세서를 사용해 이미지와 텍스트 임베딩 계산  
   - 코사인 유사도 기반으로 예측하고 정확도를 계산

2. **파인튜닝 단계:**  
   - PEFT의 LoRA를 적용하여 텍스트 인코더 일부를 미세 조정  
   - 파인튜닝용 데이터셋을 구성하고 Hugging Face Trainer를 통해 학습 수행  
   - 파인튜닝 후 동일한 방식으로 평가

3. **결과 비교 및 시각화:**  
   - 두 평가 결과(정확도 등)를 matplotlib 등을 사용해 시각화

이와 같이 단계별로 코드를 작성하여 MobileCLIP 모델에 대한 평가와 LoRA 적용 후 성능 변화를 확인할 수 있습니다. 여러분의 실제 데이터셋 및 모델 구조에 맞게 코드를 수정하여 활용하시기 바랍니다.
